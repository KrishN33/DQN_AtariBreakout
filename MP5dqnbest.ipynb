{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "8BnrI1Uu6b0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a11f18f-b259-492e-a48a-4fb1f4d4f7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the path in your Google Drive in the string below. Note: do not escape slashes or spaces\n",
        "import os\n",
        "datadir = \"/content/drive/MyDrive/assignment5/\"\n",
        "if not os.path.exists(datadir):\n",
        "  !ln -s \"/content/drive/MyDrive/assignment5/\" $datadir\n",
        "os.chdir(datadir)\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2jKFiRc6cHC",
        "outputId": "93c305fd-362e-4c0c-e7fc-534001b3c65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/assignment5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6roKqDbdgPtu"
      },
      "source": [
        "# Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6-868UGjAjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIZAbeZhgPtx"
      },
      "source": [
        "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoDHm2FwgPty",
        "outputId": "7267fdaa-4d35-4fda-9b30-6c9551c0cce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-opengl\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5MvY_W_gPt0",
        "outputId": "42b8143f-4dfa-422e-8580-808ded6f92d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /root/.local/lib/python3.10/site-packages (69.5.1)\n",
            "Requirement already satisfied: ez_setup in /usr/local/lib/python3.10/dist-packages (0.9)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.4.0)\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.66.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install --upgrade setuptools --user\n",
        "!pip3 install ez_setup\n",
        "!pip3 install gym[atari]\n",
        "!pip3 install gym[accept-rom-license]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcryLzDlgPt0"
      },
      "source": [
        "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for DQN agent, memory and training\n",
        "EPISODES = 3500\n",
        "HEIGHT = 84\n",
        "WIDTH = 84\n",
        "HISTORY_SIZE = 4\n",
        "learning_rate = 0.0001\n",
        "evaluation_reward_length = 100\n",
        "Memory_capacity = 1000000\n",
        "train_frame = 100000 # You can set it to a lower value while testing your code so you don't have to wait longer to see if the training code does not have any syntax errors\n",
        "batch_size = 128\n",
        "scheduler_gamma = 0.4\n",
        "scheduler_step_size = 100000\n",
        "\n",
        "# Hyperparameters for Double DQN agent\n",
        "update_target_network_frequency = 1000\n",
        "\n",
        "# Hyperparameters for DQN LSTM agent\n",
        "lstm_seq_length = 5"
      ],
      "metadata": {
        "id": "z-HPdxamna45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "# from config import *\n",
        "\n",
        "def find_max_lives(env):\n",
        "    env.reset()\n",
        "    _, _, _, info = env.step(0)\n",
        "    return info['lives']\n",
        "\n",
        "def check_live(life, cur_life):\n",
        "    if life > cur_life:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def get_frame(X):\n",
        "    x = np.uint8(resize(rgb2gray(X), (HEIGHT, WIDTH), mode='reflect') * 255)\n",
        "    return x\n",
        "\n",
        "def get_init_state(history, s, history_size):\n",
        "    for i in range(history_size):\n",
        "        history[i, :, :] = get_frame(s)"
      ],
      "metadata": {
        "id": "B7MERgYZnUia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from config import HEIGHT, WIDTH, lstm_seq_length\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.fc = nn.Linear(3136, 512)\n",
        "        self.head = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "# class DQN_LSTM(nn.Module):\n",
        "#     def __init__(self, action_size):\n",
        "#         super(DQN_LSTM, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
        "#         self.bn1 = nn.BatchNorm2d(32)\n",
        "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "#         self.bn2 = nn.BatchNorm2d(64)\n",
        "#         self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(64)\n",
        "#         self.fc = nn.Linear(3136, 512)\n",
        "#         self.head = nn.Linear(256, action_size)\n",
        "#         self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True)\n",
        "\n",
        "#     def forward(self, x, hidden=None, train=True):\n",
        "#         if train:  # If training, we will merge all the visual states into one.\n",
        "#             x = x.view(-1, 1, HEIGHT, WIDTH)\n",
        "#         x = F.relu(self.bn1(self.conv1(x)))\n",
        "#         x = F.relu(self.bn2(self.conv2(x)))\n",
        "#         x = F.relu(self.bn3(self.conv3(x)))\n",
        "#         x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
        "#         if train:  # We will reshape the output to match the original shape.\n",
        "#             x = x.view(-1, lstm_seq_length, 512)\n",
        "#             lstm_output, hidden = self.lstm(x, hidden)\n",
        "#             x = lstm_output[:, -1, :]\n",
        "#         return self.head(x), hidden"
      ],
      "metadata": {
        "id": "t1A0Sg_Vnmmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from config import *\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque(maxlen=Memory_capacity)\n",
        "\n",
        "    def push(self, history, action, reward, done):\n",
        "        self.memory.append((history, action, reward, done))\n",
        "\n",
        "    def sample_mini_batch(self, frame):\n",
        "        mini_batch = []\n",
        "        if frame >= Memory_capacity:\n",
        "            sample_range = Memory_capacity\n",
        "        else:\n",
        "            sample_range = frame\n",
        "\n",
        "        # history size\n",
        "        sample_range -= (HISTORY_SIZE + 1)\n",
        "\n",
        "        idx_sample = random.sample(range(sample_range), batch_size)\n",
        "        for i in idx_sample:\n",
        "            sample = []\n",
        "            for j in range(HISTORY_SIZE + 1):\n",
        "                sample.append(self.memory[i + j])\n",
        "\n",
        "            sample = np.array(sample, dtype=object)\n",
        "            mini_batch.append((np.stack(sample[:, 0], axis=0), sample[3, 1], sample[3, 2], sample[3, 3]))\n",
        "\n",
        "        return mini_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class ReplayMemoryLSTM(ReplayMemory):\n",
        "    \"\"\"\n",
        "    This is a version of Replay Memory modified for LSTMs.\n",
        "    Replay memory generally stores (state, action, reward, next state).\n",
        "    But LSTMs need sequential data.\n",
        "    So we store (state, action, reward, next state) for previous few states, constituting a trajectory.\n",
        "    During training, the previous states will be used to generate the current state of LSTM.\n",
        "    Note that samples from previous episode might get included in the trajectory.\n",
        "    Inspite of not being fully correct, this simple Replay Buffer performs well.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def sample_mini_batch(self, frame):\n",
        "        mini_batch = []\n",
        "        if frame >= Memory_capacity:\n",
        "            sample_range = Memory_capacity\n",
        "        else:\n",
        "            sample_range = frame\n",
        "\n",
        "        sample_range -= (lstm_seq_length + 1)\n",
        "\n",
        "        idx_sample = random.sample(range(sample_range - lstm_seq_length), batch_size)\n",
        "        for i in idx_sample:\n",
        "            sample = []\n",
        "            for j in range(lstm_seq_length + 1):\n",
        "                sample.append(self.memory[i + j])\n",
        "\n",
        "            sample = np.array(sample, dtype=object)\n",
        "            mini_batch.append((np.stack(sample[:, 0], axis=0), sample[lstm_seq_length - 1, 1], sample[lstm_seq_length - 1, 2], sample[lstm_seq_length - 1, 3]))\n",
        "\n",
        "        return mini_batch"
      ],
      "metadata": {
        "id": "dy3VvGT-J26n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6sJvrGtgPt1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "# from model import DQN, DQN_LSTM\n",
        "# from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mg9-fBn6FpSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hgd0emMgPt2"
      },
      "source": [
        "## Understanding the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66McSNGFgPt3"
      },
      "source": [
        "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs.\n",
        "\n",
        "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mAJIwzggPt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe7a4a4-f32d-44ab-d492-3384322f412b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4yXjtmugPt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3a0477-ec54-4f5d-8c72-347bc701bb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybkVrzeYgPt4"
      },
      "source": [
        "## Creating a DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvq0civ4gPt5"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# from memory import ReplayMemory, ReplayMemoryLSTM\n",
        "# from model import DQN, DQN_LSTM\n",
        "# from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "# from config import *\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, action_size):\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # These are hyper parameters for the DQN\n",
        "        self.discount_factor = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.explore_step = 500000\n",
        "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
        "        self.train_start = 100000\n",
        "        self.update_target = 1000\n",
        "\n",
        "        # Generate the memory\n",
        "        self.memory = ReplayMemory()\n",
        "\n",
        "        # Create the policy net\n",
        "        self.policy_net = DQN(action_size)\n",
        "        self.policy_net.to(device)\n",
        "\n",
        "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
        "\n",
        "    def load_policy_net(self, path):\n",
        "        self.policy_net = torch.load(path)\n",
        "\n",
        "    \"\"\"Get action using policy net using epsilon-greedy policy\"\"\"\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "           return torch.tensor([random.randrange(self.action_size)], device=device)\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.policy_net(state)\n",
        "            return q_values.max(1)[1].view(1, 1)\n",
        "\n",
        "    # pick samples randomly from replay memory (with batch_size)\n",
        "    def train_policy_net(self, frame):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "\n",
        "        mini_batch = self.memory.sample_mini_batch(frame)\n",
        "        mini_batch = np.array(mini_batch, dtype=object).transpose()\n",
        "\n",
        "        history = np.stack(mini_batch[0], axis=0)\n",
        "        states = np.float32(history[:, :4, :, :]) / 255.\n",
        "        states = torch.from_numpy(states).cuda()\n",
        "        actions = list(mini_batch[1])\n",
        "        actions = torch.LongTensor(actions).cuda()\n",
        "        rewards = list(mini_batch[2])\n",
        "        rewards = torch.FloatTensor(rewards).cuda()\n",
        "        next_states = np.float32(history[:, 1:, :, :]) / 255.\n",
        "        dones = mini_batch[3] # checks if the game is over\n",
        "        mask = torch.tensor(list(map(int, dones==False)),dtype=torch.uint8).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        q_vals = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        next_state =torch.tensor(next_states).to(device)\n",
        "        next_q_vals = self.policy_net(next_state).detach().max(1)[0]\n",
        "\n",
        "\n",
        "        expected = rewards + (self.discount_factor * next_q_vals * mask.float())\n",
        "\n",
        "\n",
        "        loss = F.smooth_l1_loss(q_vals, expected.unsqueeze(1))\n",
        "\n",
        "\n",
        "        # q_vals = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # next_state_values = torch.zeros(len(mini_batch), device=device).cuda()\n",
        "        # non_final_mask = mask == False\n",
        "        # non_final_next_states = next_states[non_final_mask]\n",
        "        # if len(non_final_next_states) > 0:\n",
        "        #     non_final_next_state_values = self.policy_net(non_final_next_states).max(1)[0].detach()\n",
        "        #     next_state_values[non_final_mask] = non_final_next_state_values\n",
        "\n",
        "\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n"
      ],
      "metadata": {
        "id": "K0YglP36qMzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# from memory import ReplayMemory\n",
        "# from model import DQN\n",
        "# from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "# from config import *\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DoubleAgent():\n",
        "    def __init__(self, action_size):\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # These are hyper parameters for the DQN\n",
        "        self.discount_factor = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.explore_step = 500000\n",
        "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
        "        self.train_start = 100000\n",
        "        self.update_target = 1000\n",
        "\n",
        "        # Generate the memory\n",
        "        self.memory = ReplayMemory()\n",
        "\n",
        "        # Create the policy net and the target net\n",
        "        self.policy_net = DQN(action_size)\n",
        "        self.policy_net.to(device)\n",
        "\n",
        "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
        "\n",
        "        # Initialize a target network and initialize the target network to the policy net\n",
        "        self.target_net = DQN(action_size).to(device)\n",
        "        self.target_net.eval()\n",
        "        self.update_target_net()\n",
        "\n",
        "\n",
        "    def load_policy_net(self, path):\n",
        "        self.policy_net = torch.load(path)\n",
        "\n",
        "    # after some time interval update the target net to be same with policy net\n",
        "    def update_target_net(self):\n",
        "          self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "\n",
        "    \"\"\"Get action using policy net using epsilon-greedy policy\"\"\"\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "           return random.randrange(self.action_size)\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            # state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            return self.policy_net(state).max(1)[1].item()\n",
        "            # with torch.no_grad():\n",
        "            #     q_values = self.policy_net(state)\n",
        "\n",
        "    # pick samples randomly from replay memory (with batch_size)\n",
        "    def train_policy_net(self, frame):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "\n",
        "        mini_batch = self.memory.sample_mini_batch(frame)\n",
        "        mini_batch = np.array(mini_batch, dtype=object).transpose()\n",
        "\n",
        "        history = np.stack(mini_batch[0], axis=0)\n",
        "        states = np.float32(history[:, :4, :, :]) / 255.\n",
        "        states = torch.from_numpy(states).cuda()\n",
        "        actions = list(mini_batch[1])\n",
        "        actions = torch.LongTensor(actions).cuda()\n",
        "        rewards = list(mini_batch[2])\n",
        "        rewards = torch.FloatTensor(rewards).cuda()\n",
        "        next_states = np.float32(history[:, 1:, :, :]) / 255.\n",
        "        dones = mini_batch[3] # checks if the game is over\n",
        "        mask = torch.tensor(list(map(int, dones==False)),dtype=torch.uint8).to(device)\n",
        "\n",
        "\n",
        "        q_vals = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        next_state =torch.tensor(next_states).to(device)\n",
        "        next_q_vals = self.policy_net(next_state).detach().max(1)[1]\n",
        "\n",
        "        next_actions = self.target_policy_net(next_state).detach()\n",
        "\n",
        "\n",
        "        next_s_vals = next_actions.gather(1, next_q_vals.unsqueeze(1)).squeeze(1) * mask.float()\n",
        "        expected = rewards + self.discount_factor*next_s_vals\n",
        "\n",
        "        loss = F.smooth_l1_loss(q_vals, expected.unsqueeze(1))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()"
      ],
      "metadata": {
        "id": "-3-DifKLqMYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTFMeAasgPt5"
      },
      "outputs": [],
      "source": [
        "double_dqn = False # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    agent = DoubleAgent(action_size)\n",
        "else:\n",
        "    agent = Agent(action_size)\n",
        "\n",
        "# agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toZLT5FCgPt5"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSIFQy8egPt5"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "scrolled": true,
        "id": "kL54merzgPt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c738fcab-0eb8-4fa9-d610-68a283545612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0   score: 2.0   memory length: 198   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 1   score: 0.0   memory length: 322   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 2   score: 2.0   memory length: 541   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 3   score: 2.0   memory length: 740   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 4   score: 3.0   memory length: 1007   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 5   score: 0.0   memory length: 1130   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 6   score: 3.0   memory length: 1399   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
            "episode: 7   score: 2.0   memory length: 1597   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 8   score: 1.0   memory length: 1748   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
            "episode: 9   score: 1.0   memory length: 1901   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 10   score: 2.0   memory length: 2120   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
            "episode: 11   score: 2.0   memory length: 2303   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
            "episode: 12   score: 0.0   memory length: 2427   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5384615384615385\n",
            "episode: 13   score: 3.0   memory length: 2676   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
            "episode: 14   score: 0.0   memory length: 2800   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5333333333333334\n",
            "episode: 15   score: 2.0   memory length: 3018   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5625\n",
            "episode: 16   score: 5.0   memory length: 3356   epsilon: 1.0    steps: 338    lr: 0.0001     evaluation reward: 1.7647058823529411\n",
            "episode: 17   score: 1.0   memory length: 3529   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.7222222222222223\n",
            "episode: 18   score: 1.0   memory length: 3698   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6842105263157894\n",
            "episode: 19   score: 0.0   memory length: 3821   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 20   score: 2.0   memory length: 4022   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.619047619047619\n",
            "episode: 21   score: 2.0   memory length: 4243   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
            "episode: 22   score: 0.0   memory length: 4367   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.565217391304348\n",
            "episode: 23   score: 0.0   memory length: 4491   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 24   score: 1.0   memory length: 4642   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 25   score: 1.0   memory length: 4793   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
            "episode: 26   score: 2.0   memory length: 4992   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4814814814814814\n",
            "episode: 27   score: 2.0   memory length: 5192   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 28   score: 0.0   memory length: 5315   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4482758620689655\n",
            "episode: 29   score: 0.0   memory length: 5438   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 30   score: 2.0   memory length: 5656   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4193548387096775\n",
            "episode: 31   score: 2.0   memory length: 5855   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4375\n",
            "episode: 32   score: 1.0   memory length: 6025   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4242424242424243\n",
            "episode: 33   score: 2.0   memory length: 6227   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.4411764705882353\n",
            "episode: 34   score: 1.0   memory length: 6397   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
            "episode: 35   score: 0.0   memory length: 6521   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3888888888888888\n",
            "episode: 36   score: 3.0   memory length: 6768   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4324324324324325\n",
            "episode: 37   score: 2.0   memory length: 6970   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.4473684210526316\n",
            "episode: 38   score: 3.0   memory length: 7218   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.4871794871794872\n",
            "episode: 39   score: 2.0   memory length: 7417   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 40   score: 1.0   memory length: 7589   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4878048780487805\n",
            "episode: 41   score: 2.0   memory length: 7789   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 42   score: 0.0   memory length: 7913   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4651162790697674\n",
            "episode: 43   score: 0.0   memory length: 8037   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4318181818181819\n",
            "episode: 44   score: 2.0   memory length: 8235   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
            "episode: 45   score: 0.0   memory length: 8359   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4130434782608696\n",
            "episode: 46   score: 0.0   memory length: 8483   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3829787234042554\n",
            "episode: 47   score: 1.0   memory length: 8654   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.375\n",
            "episode: 48   score: 2.0   memory length: 8873   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.3877551020408163\n",
            "episode: 49   score: 1.0   memory length: 9042   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 50   score: 7.0   memory length: 9342   epsilon: 1.0    steps: 300    lr: 0.0001     evaluation reward: 1.4901960784313726\n",
            "episode: 51   score: 6.0   memory length: 9708   epsilon: 1.0    steps: 366    lr: 0.0001     evaluation reward: 1.5769230769230769\n",
            "episode: 52   score: 1.0   memory length: 9860   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5660377358490567\n",
            "episode: 53   score: 1.0   memory length: 10012   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
            "episode: 54   score: 1.0   memory length: 10183   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5454545454545454\n",
            "episode: 55   score: 0.0   memory length: 10307   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5178571428571428\n",
            "episode: 56   score: 1.0   memory length: 10477   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5087719298245614\n",
            "episode: 57   score: 1.0   memory length: 10647   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 58   score: 2.0   memory length: 10846   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5084745762711864\n",
            "episode: 59   score: 0.0   memory length: 10970   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4833333333333334\n",
            "episode: 60   score: 3.0   memory length: 11217   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5081967213114753\n",
            "episode: 61   score: 0.0   memory length: 11341   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4838709677419355\n",
            "episode: 62   score: 0.0   memory length: 11465   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4603174603174602\n",
            "episode: 63   score: 1.0   memory length: 11616   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.453125\n",
            "episode: 64   score: 0.0   memory length: 11740   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4307692307692308\n",
            "episode: 65   score: 3.0   memory length: 11985   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
            "episode: 66   score: 0.0   memory length: 12109   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4328358208955223\n",
            "episode: 67   score: 2.0   memory length: 12308   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4411764705882353\n",
            "episode: 68   score: 0.0   memory length: 12432   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4202898550724639\n",
            "episode: 69   score: 2.0   memory length: 12633   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
            "episode: 70   score: 1.0   memory length: 12803   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4225352112676057\n",
            "episode: 71   score: 2.0   memory length: 13021   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4305555555555556\n",
            "episode: 72   score: 4.0   memory length: 13297   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.4657534246575343\n",
            "episode: 73   score: 4.0   memory length: 13574   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 74   score: 8.0   memory length: 13901   epsilon: 1.0    steps: 327    lr: 0.0001     evaluation reward: 1.5866666666666667\n",
            "episode: 75   score: 3.0   memory length: 14128   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.605263157894737\n",
            "episode: 76   score: 0.0   memory length: 14251   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5844155844155845\n",
            "episode: 77   score: 2.0   memory length: 14467   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.5897435897435896\n",
            "episode: 78   score: 1.0   memory length: 14619   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5822784810126582\n",
            "episode: 79   score: 2.0   memory length: 14819   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5875\n",
            "episode: 80   score: 0.0   memory length: 14943   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5679012345679013\n",
            "episode: 81   score: 0.0   memory length: 15067   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.548780487804878\n",
            "episode: 82   score: 1.0   memory length: 15219   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5421686746987953\n",
            "episode: 83   score: 2.0   memory length: 15402   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.5476190476190477\n",
            "episode: 84   score: 0.0   memory length: 15526   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5294117647058822\n",
            "episode: 85   score: 3.0   memory length: 15775   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.5465116279069768\n",
            "episode: 86   score: 4.0   memory length: 16072   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.5747126436781609\n",
            "episode: 87   score: 0.0   memory length: 16195   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5568181818181819\n",
            "episode: 88   score: 1.0   memory length: 16347   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.550561797752809\n",
            "episode: 89   score: 3.0   memory length: 16596   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.5666666666666667\n",
            "episode: 90   score: 3.0   memory length: 16863   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.5824175824175823\n",
            "episode: 91   score: 1.0   memory length: 17033   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.576086956521739\n",
            "episode: 92   score: 4.0   memory length: 17311   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.6021505376344085\n",
            "episode: 93   score: 1.0   memory length: 17463   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5957446808510638\n",
            "episode: 94   score: 5.0   memory length: 17797   epsilon: 1.0    steps: 334    lr: 0.0001     evaluation reward: 1.631578947368421\n",
            "episode: 95   score: 0.0   memory length: 17921   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6145833333333333\n",
            "episode: 96   score: 2.0   memory length: 18120   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6185567010309279\n",
            "episode: 97   score: 2.0   memory length: 18319   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6224489795918366\n",
            "episode: 98   score: 2.0   memory length: 18517   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6262626262626263\n",
            "episode: 99   score: 3.0   memory length: 18765   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 100   score: 0.0   memory length: 18889   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 101   score: 0.0   memory length: 19013   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 102   score: 1.0   memory length: 19165   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 103   score: 2.0   memory length: 19384   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 104   score: 4.0   memory length: 19660   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 105   score: 1.0   memory length: 19830   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 106   score: 2.0   memory length: 20050   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 107   score: 1.0   memory length: 20201   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 108   score: 0.0   memory length: 20325   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 109   score: 3.0   memory length: 20554   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 110   score: 0.0   memory length: 20677   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 111   score: 1.0   memory length: 20828   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 112   score: 6.0   memory length: 21218   epsilon: 1.0    steps: 390    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 113   score: 1.0   memory length: 21370   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 114   score: 1.0   memory length: 21522   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 115   score: 2.0   memory length: 21720   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 116   score: 2.0   memory length: 21937   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 117   score: 0.0   memory length: 22060   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 118   score: 2.0   memory length: 22259   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 119   score: 2.0   memory length: 22458   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 120   score: 2.0   memory length: 22656   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 121   score: 2.0   memory length: 22875   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 122   score: 3.0   memory length: 23123   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 123   score: 2.0   memory length: 23322   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 124   score: 2.0   memory length: 23521   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 125   score: 2.0   memory length: 23722   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 126   score: 0.0   memory length: 23846   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 127   score: 2.0   memory length: 24046   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 128   score: 4.0   memory length: 24361   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 129   score: 2.0   memory length: 24582   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 130   score: 3.0   memory length: 24849   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 131   score: 1.0   memory length: 25001   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 132   score: 1.0   memory length: 25152   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 133   score: 0.0   memory length: 25276   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 134   score: 2.0   memory length: 25475   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 135   score: 0.0   memory length: 25599   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 136   score: 2.0   memory length: 25817   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 137   score: 2.0   memory length: 26016   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 138   score: 2.0   memory length: 26217   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 139   score: 2.0   memory length: 26434   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 140   score: 2.0   memory length: 26655   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 141   score: 0.0   memory length: 26779   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 142   score: 1.0   memory length: 26931   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 143   score: 0.0   memory length: 27055   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 144   score: 2.0   memory length: 27272   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 145   score: 2.0   memory length: 27490   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 146   score: 2.0   memory length: 27689   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 147   score: 2.0   memory length: 27890   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 148   score: 2.0   memory length: 28088   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 149   score: 2.0   memory length: 28306   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 150   score: 4.0   memory length: 28622   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 151   score: 2.0   memory length: 28822   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 152   score: 0.0   memory length: 28945   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 153   score: 3.0   memory length: 29190   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 154   score: 0.0   memory length: 29314   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 155   score: 1.0   memory length: 29483   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 156   score: 0.0   memory length: 29607   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 157   score: 2.0   memory length: 29826   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 158   score: 7.0   memory length: 30201   epsilon: 1.0    steps: 375    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 159   score: 0.0   memory length: 30325   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 160   score: 1.0   memory length: 30497   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 161   score: 2.0   memory length: 30695   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 162   score: 0.0   memory length: 30819   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 163   score: 6.0   memory length: 31211   epsilon: 1.0    steps: 392    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 164   score: 3.0   memory length: 31478   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 165   score: 0.0   memory length: 31602   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 166   score: 0.0   memory length: 31725   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 167   score: 2.0   memory length: 31944   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 168   score: 2.0   memory length: 32143   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 169   score: 4.0   memory length: 32441   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 170   score: 2.0   memory length: 32642   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 171   score: 2.0   memory length: 32861   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 172   score: 1.0   memory length: 33012   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 173   score: 2.0   memory length: 33213   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 174   score: 0.0   memory length: 33336   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 175   score: 2.0   memory length: 33554   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 176   score: 2.0   memory length: 33752   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 177   score: 3.0   memory length: 33999   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 178   score: 1.0   memory length: 34150   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 179   score: 3.0   memory length: 34398   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 180   score: 6.0   memory length: 34744   epsilon: 1.0    steps: 346    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 181   score: 1.0   memory length: 34896   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 182   score: 5.0   memory length: 35246   epsilon: 1.0    steps: 350    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 183   score: 3.0   memory length: 35509   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 184   score: 2.0   memory length: 35708   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 185   score: 1.0   memory length: 35878   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 186   score: 2.0   memory length: 36098   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 187   score: 0.0   memory length: 36222   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 188   score: 3.0   memory length: 36467   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 189   score: 2.0   memory length: 36666   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 190   score: 1.0   memory length: 36818   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 191   score: 0.0   memory length: 36942   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 192   score: 0.0   memory length: 37066   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 193   score: 1.0   memory length: 37236   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 194   score: 2.0   memory length: 37453   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 195   score: 0.0   memory length: 37577   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 196   score: 0.0   memory length: 37700   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 197   score: 6.0   memory length: 38077   epsilon: 1.0    steps: 377    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 198   score: 2.0   memory length: 38294   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 199   score: 3.0   memory length: 38522   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 200   score: 0.0   memory length: 38646   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 201   score: 3.0   memory length: 38874   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 202   score: 3.0   memory length: 39122   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 203   score: 0.0   memory length: 39245   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 204   score: 1.0   memory length: 39415   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 205   score: 0.0   memory length: 39539   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 206   score: 1.0   memory length: 39691   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 207   score: 1.0   memory length: 39843   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 208   score: 2.0   memory length: 40024   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 209   score: 0.0   memory length: 40148   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 210   score: 3.0   memory length: 40374   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 211   score: 3.0   memory length: 40642   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 212   score: 1.0   memory length: 40813   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 213   score: 0.0   memory length: 40936   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 214   score: 0.0   memory length: 41060   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 215   score: 0.0   memory length: 41183   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 216   score: 1.0   memory length: 41335   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 217   score: 5.0   memory length: 41666   epsilon: 1.0    steps: 331    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 218   score: 0.0   memory length: 41790   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 219   score: 1.0   memory length: 41960   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 220   score: 4.0   memory length: 42256   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 221   score: 1.0   memory length: 42427   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 222   score: 0.0   memory length: 42550   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 223   score: 1.0   memory length: 42701   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 224   score: 1.0   memory length: 42870   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 225   score: 1.0   memory length: 43040   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 226   score: 0.0   memory length: 43164   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 227   score: 2.0   memory length: 43381   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 228   score: 0.0   memory length: 43505   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 229   score: 1.0   memory length: 43675   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 230   score: 3.0   memory length: 43943   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 231   score: 2.0   memory length: 44141   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 232   score: 0.0   memory length: 44265   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 233   score: 3.0   memory length: 44513   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 234   score: 3.0   memory length: 44747   epsilon: 1.0    steps: 234    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 235   score: 2.0   memory length: 44964   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 236   score: 1.0   memory length: 45136   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 237   score: 1.0   memory length: 45306   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 238   score: 2.0   memory length: 45525   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 239   score: 2.0   memory length: 45723   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 240   score: 1.0   memory length: 45893   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 241   score: 0.0   memory length: 46017   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 242   score: 2.0   memory length: 46216   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 243   score: 2.0   memory length: 46414   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 244   score: 3.0   memory length: 46662   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 245   score: 2.0   memory length: 46882   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 246   score: 3.0   memory length: 47113   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 247   score: 2.0   memory length: 47332   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 248   score: 3.0   memory length: 47598   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 249   score: 0.0   memory length: 47722   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 250   score: 3.0   memory length: 47971   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 251   score: 1.0   memory length: 48141   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 252   score: 1.0   memory length: 48314   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 253   score: 5.0   memory length: 48657   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 254   score: 0.0   memory length: 48781   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 255   score: 2.0   memory length: 48962   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 256   score: 3.0   memory length: 49209   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 257   score: 1.0   memory length: 49361   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 258   score: 1.0   memory length: 49513   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 259   score: 1.0   memory length: 49683   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 260   score: 0.0   memory length: 49807   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 261   score: 2.0   memory length: 50024   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 262   score: 0.0   memory length: 50147   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 263   score: 3.0   memory length: 50361   epsilon: 1.0    steps: 214    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 264   score: 0.0   memory length: 50485   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 265   score: 3.0   memory length: 50735   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 266   score: 3.0   memory length: 51000   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 267   score: 0.0   memory length: 51124   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 268   score: 1.0   memory length: 51295   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 269   score: 4.0   memory length: 51573   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 270   score: 1.0   memory length: 51743   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 271   score: 0.0   memory length: 51867   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 272   score: 2.0   memory length: 52066   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 273   score: 4.0   memory length: 52327   epsilon: 1.0    steps: 261    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 274   score: 2.0   memory length: 52546   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 275   score: 1.0   memory length: 52718   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 276   score: 0.0   memory length: 52842   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 277   score: 1.0   memory length: 53014   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 278   score: 2.0   memory length: 53215   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 279   score: 2.0   memory length: 53413   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 280   score: 2.0   memory length: 53611   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 281   score: 2.0   memory length: 53833   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 282   score: 3.0   memory length: 54059   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 283   score: 0.0   memory length: 54182   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 284   score: 3.0   memory length: 54453   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 285   score: 0.0   memory length: 54577   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 286   score: 2.0   memory length: 54797   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 287   score: 0.0   memory length: 54921   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 288   score: 0.0   memory length: 55045   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 289   score: 1.0   memory length: 55218   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 290   score: 2.0   memory length: 55420   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 291   score: 1.0   memory length: 55572   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 292   score: 0.0   memory length: 55696   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 293   score: 1.0   memory length: 55848   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 294   score: 1.0   memory length: 56019   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 295   score: 0.0   memory length: 56142   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 296   score: 0.0   memory length: 56266   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 297   score: 1.0   memory length: 56435   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 298   score: 2.0   memory length: 56634   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 299   score: 1.0   memory length: 56804   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 300   score: 3.0   memory length: 57030   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 301   score: 4.0   memory length: 57325   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 302   score: 3.0   memory length: 57571   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 303   score: 2.0   memory length: 57769   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 304   score: 4.0   memory length: 58068   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 305   score: 1.0   memory length: 58220   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 306   score: 1.0   memory length: 58392   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 307   score: 2.0   memory length: 58611   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 308   score: 1.0   memory length: 58763   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 309   score: 2.0   memory length: 58962   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 310   score: 0.0   memory length: 59085   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 311   score: 1.0   memory length: 59236   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 312   score: 0.0   memory length: 59359   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 313   score: 1.0   memory length: 59510   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 314   score: 3.0   memory length: 59739   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 315   score: 2.0   memory length: 59958   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 316   score: 4.0   memory length: 60253   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 317   score: 1.0   memory length: 60404   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 318   score: 2.0   memory length: 60620   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 319   score: 2.0   memory length: 60839   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 320   score: 0.0   memory length: 60962   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 321   score: 0.0   memory length: 61086   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 322   score: 4.0   memory length: 61366   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 323   score: 1.0   memory length: 61535   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 324   score: 3.0   memory length: 61764   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 325   score: 0.0   memory length: 61888   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 326   score: 2.0   memory length: 62087   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 327   score: 2.0   memory length: 62304   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 328   score: 1.0   memory length: 62476   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 329   score: 3.0   memory length: 62705   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 330   score: 0.0   memory length: 62829   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 331   score: 0.0   memory length: 62953   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 332   score: 1.0   memory length: 63124   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 333   score: 0.0   memory length: 63247   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 334   score: 3.0   memory length: 63474   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 335   score: 1.0   memory length: 63626   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 336   score: 4.0   memory length: 63925   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 337   score: 1.0   memory length: 64095   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 338   score: 2.0   memory length: 64314   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 339   score: 2.0   memory length: 64513   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 340   score: 3.0   memory length: 64760   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 341   score: 2.0   memory length: 64978   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 342   score: 3.0   memory length: 65226   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 343   score: 2.0   memory length: 65426   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 344   score: 2.0   memory length: 65624   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 345   score: 0.0   memory length: 65748   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 346   score: 0.0   memory length: 65871   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 347   score: 4.0   memory length: 66169   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 348   score: 1.0   memory length: 66338   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 349   score: 0.0   memory length: 66462   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 350   score: 1.0   memory length: 66632   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 351   score: 0.0   memory length: 66756   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 352   score: 0.0   memory length: 66879   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 353   score: 2.0   memory length: 67078   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 354   score: 0.0   memory length: 67202   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 355   score: 0.0   memory length: 67326   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 356   score: 0.0   memory length: 67449   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 357   score: 4.0   memory length: 67723   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 358   score: 1.0   memory length: 67875   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 359   score: 0.0   memory length: 67999   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 360   score: 1.0   memory length: 68169   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 361   score: 1.0   memory length: 68339   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 362   score: 2.0   memory length: 68537   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 363   score: 0.0   memory length: 68661   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 364   score: 4.0   memory length: 68941   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 365   score: 3.0   memory length: 69207   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 366   score: 1.0   memory length: 69377   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 367   score: 2.0   memory length: 69594   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 368   score: 0.0   memory length: 69718   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 369   score: 0.0   memory length: 69841   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 370   score: 1.0   memory length: 70010   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 371   score: 0.0   memory length: 70134   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 372   score: 1.0   memory length: 70303   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 373   score: 0.0   memory length: 70426   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 374   score: 0.0   memory length: 70550   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 375   score: 1.0   memory length: 70702   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 376   score: 0.0   memory length: 70826   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 377   score: 0.0   memory length: 70949   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 378   score: 0.0   memory length: 71073   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 379   score: 3.0   memory length: 71323   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 380   score: 2.0   memory length: 71524   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 381   score: 1.0   memory length: 71694   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 382   score: 0.0   memory length: 71817   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 383   score: 0.0   memory length: 71940   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 384   score: 1.0   memory length: 72109   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 385   score: 2.0   memory length: 72329   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 386   score: 3.0   memory length: 72558   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 387   score: 1.0   memory length: 72728   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 388   score: 1.0   memory length: 72901   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 389   score: 1.0   memory length: 73053   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 390   score: 0.0   memory length: 73176   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 391   score: 1.0   memory length: 73346   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 392   score: 2.0   memory length: 73562   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 393   score: 0.0   memory length: 73686   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 394   score: 3.0   memory length: 73936   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 395   score: 3.0   memory length: 74167   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 396   score: 2.0   memory length: 74389   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 397   score: 1.0   memory length: 74558   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 398   score: 2.0   memory length: 74775   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 399   score: 3.0   memory length: 75001   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 400   score: 1.0   memory length: 75153   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 401   score: 0.0   memory length: 75277   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 402   score: 1.0   memory length: 75450   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 403   score: 0.0   memory length: 75573   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 404   score: 3.0   memory length: 75806   epsilon: 1.0    steps: 233    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 405   score: 3.0   memory length: 76033   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 406   score: 2.0   memory length: 76231   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 407   score: 2.0   memory length: 76430   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 408   score: 1.0   memory length: 76581   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 409   score: 1.0   memory length: 76733   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 410   score: 2.0   memory length: 76951   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 411   score: 2.0   memory length: 77150   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 412   score: 1.0   memory length: 77303   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 413   score: 2.0   memory length: 77502   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 414   score: 1.0   memory length: 77675   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 415   score: 3.0   memory length: 77922   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 416   score: 3.0   memory length: 78171   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 417   score: 0.0   memory length: 78295   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 418   score: 3.0   memory length: 78544   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 419   score: 1.0   memory length: 78695   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 420   score: 0.0   memory length: 78819   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 421   score: 0.0   memory length: 78943   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 422   score: 2.0   memory length: 79163   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 423   score: 1.0   memory length: 79315   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 424   score: 1.0   memory length: 79467   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 425   score: 0.0   memory length: 79591   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 426   score: 2.0   memory length: 79771   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 427   score: 0.0   memory length: 79894   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 428   score: 2.0   memory length: 80093   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 429   score: 1.0   memory length: 80262   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 430   score: 4.0   memory length: 80558   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 431   score: 0.0   memory length: 80682   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 432   score: 2.0   memory length: 80881   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 433   score: 0.0   memory length: 81004   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 434   score: 2.0   memory length: 81204   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 435   score: 1.0   memory length: 81373   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 436   score: 2.0   memory length: 81572   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 437   score: 0.0   memory length: 81696   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 438   score: 3.0   memory length: 81922   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 439   score: 1.0   memory length: 82074   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 440   score: 1.0   memory length: 82245   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 441   score: 2.0   memory length: 82467   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 442   score: 0.0   memory length: 82591   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 443   score: 1.0   memory length: 82761   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 444   score: 2.0   memory length: 82979   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 445   score: 5.0   memory length: 83325   epsilon: 1.0    steps: 346    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 446   score: 3.0   memory length: 83570   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 447   score: 0.0   memory length: 83693   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 448   score: 2.0   memory length: 83892   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 449   score: 2.0   memory length: 84073   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 450   score: 1.0   memory length: 84225   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 451   score: 0.0   memory length: 84348   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 452   score: 0.0   memory length: 84472   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 453   score: 2.0   memory length: 84655   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 454   score: 0.0   memory length: 84778   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 455   score: 2.0   memory length: 84977   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 456   score: 0.0   memory length: 85101   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 457   score: 0.0   memory length: 85225   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 458   score: 0.0   memory length: 85348   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 459   score: 1.0   memory length: 85518   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 460   score: 3.0   memory length: 85764   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 461   score: 3.0   memory length: 86011   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 462   score: 2.0   memory length: 86192   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 463   score: 2.0   memory length: 86411   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 464   score: 5.0   memory length: 86726   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 465   score: 3.0   memory length: 86953   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 466   score: 4.0   memory length: 87244   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 467   score: 0.0   memory length: 87368   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 468   score: 3.0   memory length: 87615   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 469   score: 0.0   memory length: 87738   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 470   score: 2.0   memory length: 87958   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 471   score: 0.0   memory length: 88082   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 472   score: 2.0   memory length: 88302   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 473   score: 4.0   memory length: 88614   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 474   score: 3.0   memory length: 88859   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 475   score: 0.0   memory length: 88982   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 476   score: 2.0   memory length: 89180   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 477   score: 0.0   memory length: 89304   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 478   score: 1.0   memory length: 89456   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 479   score: 0.0   memory length: 89579   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 480   score: 0.0   memory length: 89703   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 481   score: 0.0   memory length: 89827   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 482   score: 1.0   memory length: 89996   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 483   score: 2.0   memory length: 90196   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 484   score: 0.0   memory length: 90319   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 485   score: 0.0   memory length: 90443   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 486   score: 0.0   memory length: 90566   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 487   score: 1.0   memory length: 90739   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 488   score: 2.0   memory length: 90937   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 489   score: 1.0   memory length: 91109   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 490   score: 1.0   memory length: 91278   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 491   score: 0.0   memory length: 91402   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 492   score: 0.0   memory length: 91526   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 493   score: 1.0   memory length: 91678   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 494   score: 4.0   memory length: 91975   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 495   score: 1.0   memory length: 92144   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 496   score: 0.0   memory length: 92267   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 497   score: 2.0   memory length: 92466   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 498   score: 0.0   memory length: 92589   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 499   score: 1.0   memory length: 92741   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 500   score: 1.0   memory length: 92914   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 501   score: 3.0   memory length: 93143   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 502   score: 0.0   memory length: 93267   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 503   score: 0.0   memory length: 93390   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 504   score: 2.0   memory length: 93589   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 505   score: 7.0   memory length: 93998   epsilon: 1.0    steps: 409    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 506   score: 0.0   memory length: 94121   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 507   score: 3.0   memory length: 94348   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 508   score: 6.0   memory length: 94724   epsilon: 1.0    steps: 376    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 509   score: 2.0   memory length: 94942   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 510   score: 0.0   memory length: 95065   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 511   score: 3.0   memory length: 95329   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 512   score: 0.0   memory length: 95453   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 513   score: 0.0   memory length: 95576   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 514   score: 0.0   memory length: 95699   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 515   score: 0.0   memory length: 95823   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 516   score: 0.0   memory length: 95947   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 517   score: 1.0   memory length: 96101   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 518   score: 0.0   memory length: 96225   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 519   score: 3.0   memory length: 96474   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 520   score: 0.0   memory length: 96598   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 521   score: 2.0   memory length: 96796   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 522   score: 0.0   memory length: 96919   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 523   score: 1.0   memory length: 97089   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 524   score: 1.0   memory length: 97259   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 525   score: 1.0   memory length: 97432   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 526   score: 1.0   memory length: 97602   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 527   score: 1.0   memory length: 97754   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 528   score: 0.0   memory length: 97878   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 529   score: 2.0   memory length: 98080   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 530   score: 2.0   memory length: 98279   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 531   score: 0.0   memory length: 98402   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 532   score: 0.0   memory length: 98525   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 533   score: 3.0   memory length: 98752   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 534   score: 0.0   memory length: 98875   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 535   score: 1.0   memory length: 99045   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 536   score: 2.0   memory length: 99244   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 537   score: 2.0   memory length: 99465   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 538   score: 3.0   memory length: 99712   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 539   score: 0.0   memory length: 99835   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 540   score: 5.0   memory length: 100151   epsilon: 0.9996990400000065    steps: 316    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 541   score: 3.0   memory length: 100399   epsilon: 0.9992080000000172    steps: 248    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 542   score: 1.0   memory length: 100551   epsilon: 0.9989070400000237    steps: 152    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 543   score: 5.0   memory length: 100914   epsilon: 0.9981883000000393    steps: 363    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 544   score: 1.0   memory length: 101083   epsilon: 0.9978536800000466    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 545   score: 0.0   memory length: 101207   epsilon: 0.9976081600000519    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 546   score: 0.0   memory length: 101331   epsilon: 0.9973626400000573    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 547   score: 2.0   memory length: 101548   epsilon: 0.9969329800000666    steps: 217    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 548   score: 1.0   memory length: 101718   epsilon: 0.9965963800000739    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 549   score: 2.0   memory length: 101937   epsilon: 0.9961627600000833    steps: 219    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 550   score: 2.0   memory length: 102135   epsilon: 0.9957707200000918    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 551   score: 0.0   memory length: 102259   epsilon: 0.9955252000000971    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 552   score: 2.0   memory length: 102477   epsilon: 0.9950935600001065    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 553   score: 2.0   memory length: 102676   epsilon: 0.9946995400001151    steps: 199    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 554   score: 2.0   memory length: 102895   epsilon: 0.9942659200001245    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 555   score: 2.0   memory length: 103095   epsilon: 0.9938699200001331    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 556   score: 1.0   memory length: 103265   epsilon: 0.9935333200001404    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 557   score: 4.0   memory length: 103581   epsilon: 0.992907640000154    steps: 316    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 558   score: 1.0   memory length: 103751   epsilon: 0.9925710400001613    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 559   score: 4.0   memory length: 104030   epsilon: 0.9920186200001733    steps: 279    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 560   score: 0.0   memory length: 104154   epsilon: 0.9917731000001786    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 561   score: 2.0   memory length: 104358   epsilon: 0.9913691800001874    steps: 204    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 562   score: 3.0   memory length: 104630   epsilon: 0.9908306200001991    steps: 272    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 563   score: 0.0   memory length: 104753   epsilon: 0.9905870800002043    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 564   score: 2.0   memory length: 104953   epsilon: 0.9901910800002129    steps: 200    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 565   score: 2.0   memory length: 105152   epsilon: 0.9897970600002215    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 566   score: 3.0   memory length: 105401   epsilon: 0.9893040400002322    steps: 249    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 567   score: 2.0   memory length: 105600   epsilon: 0.9889100200002408    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 568   score: 0.0   memory length: 105724   epsilon: 0.9886645000002461    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 569   score: 1.0   memory length: 105875   epsilon: 0.9883655200002526    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 570   score: 0.0   memory length: 105998   epsilon: 0.9881219800002579    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 571   score: 0.0   memory length: 106122   epsilon: 0.9878764600002632    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 572   score: 3.0   memory length: 106367   epsilon: 0.9873913600002737    steps: 245    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 573   score: 0.0   memory length: 106491   epsilon: 0.987145840000279    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 574   score: 1.0   memory length: 106645   epsilon: 0.9868409200002857    steps: 154    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 575   score: 2.0   memory length: 106845   epsilon: 0.9864449200002943    steps: 200    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 576   score: 1.0   memory length: 106997   epsilon: 0.9861439600003008    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 577   score: 0.0   memory length: 107120   epsilon: 0.9859004200003061    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 578   score: 3.0   memory length: 107388   epsilon: 0.9853697800003176    steps: 268    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 579   score: 2.0   memory length: 107605   epsilon: 0.9849401200003269    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 580   score: 2.0   memory length: 107806   epsilon: 0.9845421400003356    steps: 201    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 581   score: 2.0   memory length: 108026   epsilon: 0.984106540000345    steps: 220    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 582   score: 2.0   memory length: 108230   epsilon: 0.9837026200003538    steps: 204    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 583   score: 0.0   memory length: 108353   epsilon: 0.9834590800003591    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 584   score: 2.0   memory length: 108551   epsilon: 0.9830670400003676    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 585   score: 4.0   memory length: 108828   epsilon: 0.9825185800003795    steps: 277    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 586   score: 0.0   memory length: 108951   epsilon: 0.9822750400003848    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 587   score: 2.0   memory length: 109132   epsilon: 0.9819166600003926    steps: 181    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 588   score: 1.0   memory length: 109305   epsilon: 0.9815741200004    steps: 173    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 589   score: 1.0   memory length: 109475   epsilon: 0.9812375200004073    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 590   score: 2.0   memory length: 109692   epsilon: 0.9808078600004166    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 591   score: 2.0   memory length: 109891   epsilon: 0.9804138400004252    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 592   score: 2.0   memory length: 110112   epsilon: 0.9799762600004347    steps: 221    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 593   score: 1.0   memory length: 110264   epsilon: 0.9796753000004412    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 594   score: 3.0   memory length: 110493   epsilon: 0.9792218800004511    steps: 229    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 595   score: 1.0   memory length: 110646   epsilon: 0.9789189400004576    steps: 153    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 596   score: 0.0   memory length: 110769   epsilon: 0.9786754000004629    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 597   score: 6.0   memory length: 111096   epsilon: 0.978027940000477    steps: 327    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 598   score: 1.0   memory length: 111265   epsilon: 0.9776933200004843    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 599   score: 1.0   memory length: 111435   epsilon: 0.9773567200004916    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 600   score: 2.0   memory length: 111653   epsilon: 0.9769250800005009    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 601   score: 0.0   memory length: 111777   epsilon: 0.9766795600005063    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 602   score: 1.0   memory length: 111946   epsilon: 0.9763449400005135    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 603   score: 2.0   memory length: 112145   epsilon: 0.9759509200005221    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 604   score: 1.0   memory length: 112296   epsilon: 0.9756519400005286    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 605   score: 2.0   memory length: 112513   epsilon: 0.9752222800005379    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 606   score: 2.0   memory length: 112712   epsilon: 0.9748282600005465    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 607   score: 3.0   memory length: 112979   epsilon: 0.9742996000005579    steps: 267    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 608   score: 2.0   memory length: 113200   epsilon: 0.9738620200005674    steps: 221    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 609   score: 1.0   memory length: 113352   epsilon: 0.973561060000574    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 610   score: 1.0   memory length: 113503   epsilon: 0.9732620800005805    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 611   score: 0.0   memory length: 113627   epsilon: 0.9730165600005858    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 612   score: 2.0   memory length: 113826   epsilon: 0.9726225400005943    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 613   score: 1.0   memory length: 113995   epsilon: 0.9722879200006016    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 614   score: 0.0   memory length: 114119   epsilon: 0.9720424000006069    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 615   score: 2.0   memory length: 114339   epsilon: 0.9716068000006164    steps: 220    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 616   score: 1.0   memory length: 114490   epsilon: 0.9713078200006229    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 617   score: 0.0   memory length: 114613   epsilon: 0.9710642800006282    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 618   score: 1.0   memory length: 114783   epsilon: 0.9707276800006355    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 619   score: 2.0   memory length: 114982   epsilon: 0.970333660000644    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 620   score: 5.0   memory length: 115282   epsilon: 0.9697396600006569    steps: 300    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 621   score: 2.0   memory length: 115481   epsilon: 0.9693456400006655    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 622   score: 1.0   memory length: 115650   epsilon: 0.9690110200006727    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 623   score: 0.0   memory length: 115774   epsilon: 0.9687655000006781    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 624   score: 0.0   memory length: 115898   epsilon: 0.9685199800006834    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 625   score: 0.0   memory length: 116021   epsilon: 0.9682764400006887    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 626   score: 1.0   memory length: 116192   epsilon: 0.967937860000696    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 627   score: 0.0   memory length: 116316   epsilon: 0.9676923400007014    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 628   score: 0.0   memory length: 116440   epsilon: 0.9674468200007067    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 629   score: 2.0   memory length: 116639   epsilon: 0.9670528000007153    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 630   score: 2.0   memory length: 116858   epsilon: 0.9666191800007247    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 631   score: 3.0   memory length: 117069   epsilon: 0.9662014000007337    steps: 211    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 632   score: 3.0   memory length: 117320   epsilon: 0.9657044200007445    steps: 251    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 633   score: 0.0   memory length: 117443   epsilon: 0.9654608800007498    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 634   score: 2.0   memory length: 117641   epsilon: 0.9650688400007583    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 635   score: 1.0   memory length: 117793   epsilon: 0.9647678800007649    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 636   score: 1.0   memory length: 117962   epsilon: 0.9644332600007721    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 637   score: 0.0   memory length: 118086   epsilon: 0.9641877400007774    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 638   score: 4.0   memory length: 118400   epsilon: 0.963566020000791    steps: 314    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 639   score: 2.0   memory length: 118618   epsilon: 0.9631343800008003    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 640   score: 2.0   memory length: 118838   epsilon: 0.9626987800008098    steps: 220    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 641   score: 1.0   memory length: 119009   epsilon: 0.9623602000008171    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 642   score: 0.0   memory length: 119133   epsilon: 0.9621146800008225    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 643   score: 0.0   memory length: 119256   epsilon: 0.9618711400008277    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 644   score: 0.0   memory length: 119380   epsilon: 0.9616256200008331    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 645   score: 2.0   memory length: 119582   epsilon: 0.9612256600008418    steps: 202    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 646   score: 1.0   memory length: 119755   epsilon: 0.9608831200008492    steps: 173    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 647   score: 3.0   memory length: 120023   epsilon: 0.9603524800008607    steps: 268    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 648   score: 2.0   memory length: 120222   epsilon: 0.9599584600008693    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 649   score: 0.0   memory length: 120345   epsilon: 0.9597149200008745    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 650   score: 2.0   memory length: 120564   epsilon: 0.959281300000884    steps: 219    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 651   score: 0.0   memory length: 120688   epsilon: 0.9590357800008893    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 652   score: 2.0   memory length: 120887   epsilon: 0.9586417600008978    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 653   score: 0.0   memory length: 121011   epsilon: 0.9583962400009032    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 654   score: 3.0   memory length: 121240   epsilon: 0.957942820000913    steps: 229    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 655   score: 1.0   memory length: 121411   epsilon: 0.9576042400009204    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 656   score: 2.0   memory length: 121629   epsilon: 0.9571726000009297    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 657   score: 2.0   memory length: 121827   epsilon: 0.9567805600009383    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 658   score: 0.0   memory length: 121951   epsilon: 0.9565350400009436    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 659   score: 2.0   memory length: 122150   epsilon: 0.9561410200009521    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 660   score: 2.0   memory length: 122349   epsilon: 0.9557470000009607    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 661   score: 2.0   memory length: 122567   epsilon: 0.9553153600009701    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 662   score: 0.0   memory length: 122691   epsilon: 0.9550698400009754    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 663   score: 5.0   memory length: 122996   epsilon: 0.9544659400009885    steps: 305    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 664   score: 2.0   memory length: 123196   epsilon: 0.9540699400009971    steps: 200    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 665   score: 3.0   memory length: 123425   epsilon: 0.9536165200010069    steps: 229    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 666   score: 3.0   memory length: 123651   epsilon: 0.9531690400010167    steps: 226    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 667   score: 1.0   memory length: 123802   epsilon: 0.9528700600010231    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 668   score: 1.0   memory length: 123954   epsilon: 0.9525691000010297    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 669   score: 1.0   memory length: 124126   epsilon: 0.9522285400010371    steps: 172    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 670   score: 2.0   memory length: 124325   epsilon: 0.9518345200010456    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 671   score: 0.0   memory length: 124448   epsilon: 0.9515909800010509    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 672   score: 1.0   memory length: 124600   epsilon: 0.9512900200010574    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 673   score: 1.0   memory length: 124752   epsilon: 0.950989060001064    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 674   score: 3.0   memory length: 124984   epsilon: 0.950529700001074    steps: 232    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 675   score: 0.0   memory length: 125107   epsilon: 0.9502861600010792    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 676   score: 1.0   memory length: 125278   epsilon: 0.9499475800010866    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 677   score: 2.0   memory length: 125476   epsilon: 0.9495555400010951    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 678   score: 2.0   memory length: 125675   epsilon: 0.9491615200011037    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 679   score: 1.0   memory length: 125844   epsilon: 0.9488269000011109    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 680   score: 2.0   memory length: 126045   epsilon: 0.9484289200011196    steps: 201    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 681   score: 2.0   memory length: 126245   epsilon: 0.9480329200011282    steps: 200    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 682   score: 1.0   memory length: 126418   epsilon: 0.9476903800011356    steps: 173    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 683   score: 0.0   memory length: 126541   epsilon: 0.9474468400011409    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 684   score: 0.0   memory length: 126664   epsilon: 0.9472033000011462    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 685   score: 0.0   memory length: 126787   epsilon: 0.9469597600011515    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 686   score: 3.0   memory length: 127035   epsilon: 0.9464687200011621    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 687   score: 5.0   memory length: 127384   epsilon: 0.9457777000011771    steps: 349    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 688   score: 0.0   memory length: 127508   epsilon: 0.9455321800011824    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 689   score: 0.0   memory length: 127632   epsilon: 0.9452866600011878    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 690   score: 2.0   memory length: 127851   epsilon: 0.9448530400011972    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 691   score: 0.0   memory length: 127974   epsilon: 0.9446095000012025    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 692   score: 3.0   memory length: 128203   epsilon: 0.9441560800012123    steps: 229    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 693   score: 0.0   memory length: 128326   epsilon: 0.9439125400012176    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 694   score: 0.0   memory length: 128450   epsilon: 0.9436670200012229    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 695   score: 4.0   memory length: 128726   epsilon: 0.9431205400012348    steps: 276    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 696   score: 2.0   memory length: 128908   epsilon: 0.9427601800012426    steps: 182    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 697   score: 2.0   memory length: 129107   epsilon: 0.9423661600012512    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 698   score: 1.0   memory length: 129280   epsilon: 0.9420236200012586    steps: 173    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 699   score: 2.0   memory length: 129460   epsilon: 0.9416672200012663    steps: 180    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 700   score: 1.0   memory length: 129630   epsilon: 0.9413306200012737    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 701   score: 1.0   memory length: 129801   epsilon: 0.940992040001281    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 702   score: 0.0   memory length: 129924   epsilon: 0.9407485000012863    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 703   score: 0.0   memory length: 130048   epsilon: 0.9405029800012916    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 704   score: 4.0   memory length: 130325   epsilon: 0.9399545200013035    steps: 277    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 705   score: 3.0   memory length: 130596   epsilon: 0.9394179400013152    steps: 271    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 706   score: 2.0   memory length: 130795   epsilon: 0.9390239200013237    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 707   score: 1.0   memory length: 130968   epsilon: 0.9386813800013312    steps: 173    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 708   score: 2.0   memory length: 131167   epsilon: 0.9382873600013397    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 709   score: 2.0   memory length: 131348   epsilon: 0.9379289800013475    steps: 181    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 710   score: 0.0   memory length: 131472   epsilon: 0.9376834600013528    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 711   score: 2.0   memory length: 131691   epsilon: 0.9372498400013622    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 712   score: 2.0   memory length: 131908   epsilon: 0.9368201800013716    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 713   score: 3.0   memory length: 132157   epsilon: 0.9363271600013823    steps: 249    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 714   score: 2.0   memory length: 132356   epsilon: 0.9359331400013908    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 715   score: 2.0   memory length: 132575   epsilon: 0.9354995200014002    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 716   score: 1.0   memory length: 132745   epsilon: 0.9351629200014075    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 717   score: 1.0   memory length: 132897   epsilon: 0.9348619600014141    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 718   score: 2.0   memory length: 133115   epsilon: 0.9344303200014235    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 719   score: 0.0   memory length: 133239   epsilon: 0.9341848000014288    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 720   score: 2.0   memory length: 133438   epsilon: 0.9337907800014373    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 721   score: 0.0   memory length: 133562   epsilon: 0.9335452600014427    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 722   score: 1.0   memory length: 133714   epsilon: 0.9332443000014492    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 723   score: 0.0   memory length: 133838   epsilon: 0.9329987800014545    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 724   score: 3.0   memory length: 134103   epsilon: 0.9324740800014659    steps: 265    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 725   score: 1.0   memory length: 134273   epsilon: 0.9321374800014732    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 726   score: 2.0   memory length: 134471   epsilon: 0.9317454400014817    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 727   score: 0.0   memory length: 134594   epsilon: 0.931501900001487    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 728   score: 0.0   memory length: 134718   epsilon: 0.9312563800014924    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 729   score: 4.0   memory length: 135038   epsilon: 0.9306227800015061    steps: 320    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 730   score: 3.0   memory length: 135268   epsilon: 0.930167380001516    steps: 230    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 731   score: 0.0   memory length: 135392   epsilon: 0.9299218600015213    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 732   score: 2.0   memory length: 135592   epsilon: 0.9295258600015299    steps: 200    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 733   score: 1.0   memory length: 135762   epsilon: 0.9291892600015372    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 734   score: 0.0   memory length: 135886   epsilon: 0.9289437400015426    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 735   score: 1.0   memory length: 136038   epsilon: 0.9286427800015491    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 736   score: 3.0   memory length: 136265   epsilon: 0.9281933200015589    steps: 227    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 737   score: 0.0   memory length: 136389   epsilon: 0.9279478000015642    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 738   score: 0.0   memory length: 136513   epsilon: 0.9277022800015695    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 739   score: 1.0   memory length: 136685   epsilon: 0.9273617200015769    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 740   score: 2.0   memory length: 136884   epsilon: 0.9269677000015855    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 741   score: 4.0   memory length: 137147   epsilon: 0.9264469600015968    steps: 263    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 742   score: 3.0   memory length: 137412   epsilon: 0.9259222600016082    steps: 265    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 743   score: 3.0   memory length: 137660   epsilon: 0.9254312200016188    steps: 248    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 744   score: 0.0   memory length: 137784   epsilon: 0.9251857000016241    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 745   score: 0.0   memory length: 137908   epsilon: 0.9249401800016295    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 746   score: 2.0   memory length: 138106   epsilon: 0.924548140001638    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 747   score: 0.0   memory length: 138229   epsilon: 0.9243046000016433    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 748   score: 3.0   memory length: 138480   epsilon: 0.9238076200016541    steps: 251    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 749   score: 3.0   memory length: 138709   epsilon: 0.9233542000016639    steps: 229    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 750   score: 3.0   memory length: 138977   epsilon: 0.9228235600016754    steps: 268    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 751   score: 1.0   memory length: 139148   epsilon: 0.9224849800016828    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 752   score: 0.0   memory length: 139272   epsilon: 0.9222394600016881    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 753   score: 5.0   memory length: 139617   epsilon: 0.9215563600017029    steps: 345    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 754   score: 1.0   memory length: 139789   epsilon: 0.9212158000017103    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 755   score: 2.0   memory length: 139988   epsilon: 0.9208217800017189    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 756   score: 2.0   memory length: 140207   epsilon: 0.9203881600017283    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 757   score: 1.0   memory length: 140376   epsilon: 0.9200535400017356    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 758   score: 2.0   memory length: 140574   epsilon: 0.9196615000017441    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 759   score: 2.0   memory length: 140794   epsilon: 0.9192259000017535    steps: 220    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 760   score: 0.0   memory length: 140918   epsilon: 0.9189803800017589    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 761   score: 3.0   memory length: 141183   epsilon: 0.9184556800017702    steps: 265    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 762   score: 3.0   memory length: 141416   epsilon: 0.9179943400017803    steps: 233    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 763   score: 3.0   memory length: 141661   epsilon: 0.9175092400017908    steps: 245    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 764   score: 3.0   memory length: 141890   epsilon: 0.9170558200018006    steps: 229    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 765   score: 2.0   memory length: 142114   epsilon: 0.9166123000018103    steps: 224    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 766   score: 1.0   memory length: 142266   epsilon: 0.9163113400018168    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 767   score: 2.0   memory length: 142447   epsilon: 0.9159529600018246    steps: 181    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 768   score: 2.0   memory length: 142646   epsilon: 0.9155589400018331    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 769   score: 4.0   memory length: 142942   epsilon: 0.9149728600018459    steps: 296    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 770   score: 1.0   memory length: 143112   epsilon: 0.9146362600018532    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 771   score: 2.0   memory length: 143311   epsilon: 0.9142422400018617    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 772   score: 1.0   memory length: 143484   epsilon: 0.9138997000018692    steps: 173    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 773   score: 0.0   memory length: 143607   epsilon: 0.9136561600018744    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 774   score: 2.0   memory length: 143826   epsilon: 0.9132225400018839    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 775   score: 3.0   memory length: 144074   epsilon: 0.9127315000018945    steps: 248    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 776   score: 3.0   memory length: 144301   epsilon: 0.9122820400019043    steps: 227    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 777   score: 3.0   memory length: 144531   epsilon: 0.9118266400019142    steps: 230    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 778   score: 2.0   memory length: 144730   epsilon: 0.9114326200019227    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 779   score: 2.0   memory length: 144954   epsilon: 0.9109891000019323    steps: 224    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 780   score: 2.0   memory length: 145170   epsilon: 0.9105614200019416    steps: 216    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 781   score: 5.0   memory length: 145518   epsilon: 0.9098723800019566    steps: 348    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 782   score: 3.0   memory length: 145730   epsilon: 0.9094526200019657    steps: 212    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 783   score: 3.0   memory length: 145999   epsilon: 0.9089200000019773    steps: 269    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 784   score: 2.0   memory length: 146219   epsilon: 0.9084844000019867    steps: 220    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 785   score: 0.0   memory length: 146343   epsilon: 0.908238880001992    steps: 124    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 786   score: 7.0   memory length: 146752   epsilon: 0.9074290600020096    steps: 409    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 787   score: 1.0   memory length: 146904   epsilon: 0.9071281000020162    steps: 152    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 788   score: 2.0   memory length: 147087   epsilon: 0.906765760002024    steps: 183    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 789   score: 2.0   memory length: 147289   epsilon: 0.9063658000020327    steps: 202    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 790   score: 3.0   memory length: 147518   epsilon: 0.9059123800020425    steps: 229    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 791   score: 1.0   memory length: 147689   epsilon: 0.9055738000020499    steps: 171    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 792   score: 3.0   memory length: 147922   epsilon: 0.9051124600020599    steps: 233    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 793   score: 4.0   memory length: 148198   epsilon: 0.9045659800020718    steps: 276    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 794   score: 2.0   memory length: 148417   epsilon: 0.9041323600020812    steps: 219    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 795   score: 0.0   memory length: 148540   epsilon: 0.9038888200020865    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 796   score: 1.0   memory length: 148713   epsilon: 0.9035462800020939    steps: 173    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 797   score: 3.0   memory length: 148962   epsilon: 0.9030532600021046    steps: 249    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 798   score: 0.0   memory length: 149086   epsilon: 0.90280774000211    steps: 124    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 799   score: 3.0   memory length: 149356   epsilon: 0.9022731400021216    steps: 270    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 800   score: 2.0   memory length: 149555   epsilon: 0.9018791200021301    steps: 199    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 801   score: 2.0   memory length: 149754   epsilon: 0.9014851000021387    steps: 199    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 802   score: 0.0   memory length: 149877   epsilon: 0.901241560002144    steps: 123    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 803   score: 1.0   memory length: 150029   epsilon: 0.9009406000021505    steps: 152    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 804   score: 2.0   memory length: 150227   epsilon: 0.900548560002159    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 805   score: 2.0   memory length: 150426   epsilon: 0.9001545400021675    steps: 199    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 806   score: 3.0   memory length: 150691   epsilon: 0.8996298400021789    steps: 265    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 807   score: 5.0   memory length: 151000   epsilon: 0.8990180200021922    steps: 309    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 808   score: 1.0   memory length: 151170   epsilon: 0.8986814200021995    steps: 170    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 809   score: 2.0   memory length: 151368   epsilon: 0.898289380002208    steps: 198    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 810   score: 1.0   memory length: 151520   epsilon: 0.8979884200022146    steps: 152    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 811   score: 3.0   memory length: 151785   epsilon: 0.897463720002226    steps: 265    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 812   score: 2.0   memory length: 151983   epsilon: 0.8970716800022345    steps: 198    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 813   score: 0.0   memory length: 152107   epsilon: 0.8968261600022398    steps: 124    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 814   score: 2.0   memory length: 152309   epsilon: 0.8964262000022485    steps: 202    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 815   score: 1.0   memory length: 152461   epsilon: 0.896125240002255    steps: 152    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 816   score: 5.0   memory length: 152788   epsilon: 0.8954777800022691    steps: 327    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 817   score: 0.0   memory length: 152911   epsilon: 0.8952342400022744    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 818   score: 1.0   memory length: 153081   epsilon: 0.8948976400022817    steps: 170    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 819   score: 0.0   memory length: 153205   epsilon: 0.894652120002287    steps: 124    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 820   score: 1.0   memory length: 153376   epsilon: 0.8943135400022943    steps: 171    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 821   score: 4.0   memory length: 153675   epsilon: 0.8937215200023072    steps: 299    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 822   score: 0.0   memory length: 153799   epsilon: 0.8934760000023125    steps: 124    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 823   score: 4.0   memory length: 154075   epsilon: 0.8929295200023244    steps: 276    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 824   score: 1.0   memory length: 154227   epsilon: 0.8926285600023309    steps: 152    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 825   score: 0.0   memory length: 154351   epsilon: 0.8923830400023363    steps: 124    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 826   score: 2.0   memory length: 154552   epsilon: 0.8919850600023449    steps: 201    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 827   score: 2.0   memory length: 154753   epsilon: 0.8915870800023535    steps: 201    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 828   score: 0.0   memory length: 154876   epsilon: 0.8913435400023588    steps: 123    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 829   score: 4.0   memory length: 155174   epsilon: 0.8907535000023716    steps: 298    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 830   score: 1.0   memory length: 155344   epsilon: 0.8904169000023789    steps: 170    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 831   score: 3.0   memory length: 155608   epsilon: 0.8898941800023903    steps: 264    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 832   score: 4.0   memory length: 155887   epsilon: 0.8893417600024023    steps: 279    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 833   score: 1.0   memory length: 156058   epsilon: 0.8890031800024096    steps: 171    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 834   score: 1.0   memory length: 156230   epsilon: 0.888662620002417    steps: 172    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 835   score: 1.0   memory length: 156400   epsilon: 0.8883260200024243    steps: 170    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 836   score: 2.0   memory length: 156598   epsilon: 0.8879339800024328    steps: 198    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 837   score: 4.0   memory length: 156885   epsilon: 0.8873657200024452    steps: 287    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 838   score: 5.0   memory length: 157193   epsilon: 0.8867558800024584    steps: 308    lr: 0.0001     evaluation reward: 2.04\n",
            "episode: 839   score: 2.0   memory length: 157411   epsilon: 0.8863242400024678    steps: 218    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 840   score: 1.0   memory length: 157563   epsilon: 0.8860232800024743    steps: 152    lr: 0.0001     evaluation reward: 2.04\n",
            "episode: 841   score: 3.0   memory length: 157791   epsilon: 0.8855718400024841    steps: 228    lr: 0.0001     evaluation reward: 2.03\n",
            "episode: 842   score: 4.0   memory length: 158067   epsilon: 0.885025360002496    steps: 276    lr: 0.0001     evaluation reward: 2.04\n",
            "episode: 843   score: 0.0   memory length: 158191   epsilon: 0.8847798400025013    steps: 124    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 844   score: 2.0   memory length: 158374   epsilon: 0.8844175000025092    steps: 183    lr: 0.0001     evaluation reward: 2.03\n",
            "episode: 845   score: 4.0   memory length: 158668   epsilon: 0.8838353800025218    steps: 294    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 846   score: 1.0   memory length: 158838   epsilon: 0.8834987800025291    steps: 170    lr: 0.0001     evaluation reward: 2.06\n",
            "episode: 847   score: 0.0   memory length: 158962   epsilon: 0.8832532600025345    steps: 124    lr: 0.0001     evaluation reward: 2.06\n",
            "episode: 848   score: 2.0   memory length: 159161   epsilon: 0.882859240002543    steps: 199    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 849   score: 0.0   memory length: 159284   epsilon: 0.8826157000025483    steps: 123    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 850   score: 0.0   memory length: 159408   epsilon: 0.8823701800025536    steps: 124    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 851   score: 2.0   memory length: 159608   epsilon: 0.8819741800025622    steps: 200    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 852   score: 1.0   memory length: 159760   epsilon: 0.8816732200025688    steps: 152    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 853   score: 0.0   memory length: 159884   epsilon: 0.8814277000025741    steps: 124    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 854   score: 1.0   memory length: 160036   epsilon: 0.8811267400025806    steps: 152    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 855   score: 1.0   memory length: 160188   epsilon: 0.8808257800025872    steps: 152    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 856   score: 2.0   memory length: 160407   epsilon: 0.8803921600025966    steps: 219    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 857   score: 3.0   memory length: 160653   epsilon: 0.8799050800026071    steps: 246    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 858   score: 2.0   memory length: 160852   epsilon: 0.8795110600026157    steps: 199    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 859   score: 3.0   memory length: 161099   epsilon: 0.8790220000026263    steps: 247    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 860   score: 2.0   memory length: 161298   epsilon: 0.8786279800026349    steps: 199    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 861   score: 0.0   memory length: 161422   epsilon: 0.8783824600026402    steps: 124    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 862   score: 2.0   memory length: 161641   epsilon: 0.8779488400026496    steps: 219    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 863   score: 5.0   memory length: 161969   epsilon: 0.8772994000026637    steps: 328    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 864   score: 0.0   memory length: 162093   epsilon: 0.877053880002669    steps: 124    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 865   score: 1.0   memory length: 162262   epsilon: 0.8767192600026763    steps: 169    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 866   score: 2.0   memory length: 162460   epsilon: 0.8763272200026848    steps: 198    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 867   score: 0.0   memory length: 162583   epsilon: 0.8760836800026901    steps: 123    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 868   score: 4.0   memory length: 162848   epsilon: 0.8755589800027015    steps: 265    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 869   score: 1.0   memory length: 163018   epsilon: 0.8752223800027088    steps: 170    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 870   score: 1.0   memory length: 163190   epsilon: 0.8748818200027162    steps: 172    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 871   score: 0.0   memory length: 163314   epsilon: 0.8746363000027215    steps: 124    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 872   score: 1.0   memory length: 163466   epsilon: 0.8743353400027281    steps: 152    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 873   score: 4.0   memory length: 163724   epsilon: 0.8738245000027391    steps: 258    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 874   score: 2.0   memory length: 163944   epsilon: 0.8733889000027486    steps: 220    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 875   score: 2.0   memory length: 164143   epsilon: 0.8729948800027572    steps: 199    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 876   score: 0.0   memory length: 164266   epsilon: 0.8727513400027624    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 877   score: 1.0   memory length: 164436   epsilon: 0.8724147400027698    steps: 170    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 878   score: 2.0   memory length: 164635   epsilon: 0.8720207200027783    steps: 199    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 879   score: 0.0   memory length: 164759   epsilon: 0.8717752000027836    steps: 124    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 880   score: 2.0   memory length: 164940   epsilon: 0.8714168200027914    steps: 181    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 881   score: 0.0   memory length: 165064   epsilon: 0.8711713000027967    steps: 124    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 882   score: 1.0   memory length: 165215   epsilon: 0.8708723200028032    steps: 151    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 883   score: 1.0   memory length: 165386   epsilon: 0.8705337400028106    steps: 171    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 884   score: 3.0   memory length: 165631   epsilon: 0.8700486400028211    steps: 245    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 885   score: 2.0   memory length: 165832   epsilon: 0.8696506600028298    steps: 201    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 886   score: 3.0   memory length: 166078   epsilon: 0.8691635800028403    steps: 246    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 887   score: 5.0   memory length: 166406   epsilon: 0.8685141400028544    steps: 328    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 888   score: 1.0   memory length: 166576   epsilon: 0.8681775400028617    steps: 170    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 889   score: 1.0   memory length: 166730   epsilon: 0.8678726200028684    steps: 154    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 890   score: 4.0   memory length: 166990   epsilon: 0.8673578200028795    steps: 260    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 891   score: 4.0   memory length: 167248   epsilon: 0.8668469800028906    steps: 258    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 892   score: 0.0   memory length: 167372   epsilon: 0.866601460002896    steps: 124    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 893   score: 0.0   memory length: 167496   epsilon: 0.8663559400029013    steps: 124    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 894   score: 2.0   memory length: 167695   epsilon: 0.8659619200029098    steps: 199    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 895   score: 2.0   memory length: 167913   epsilon: 0.8655302800029192    steps: 218    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 896   score: 2.0   memory length: 168111   epsilon: 0.8651382400029277    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 897   score: 0.0   memory length: 168235   epsilon: 0.864892720002933    steps: 124    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 898   score: 1.0   memory length: 168387   epsilon: 0.8645917600029396    steps: 152    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 899   score: 0.0   memory length: 168511   epsilon: 0.8643462400029449    steps: 124    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 900   score: 1.0   memory length: 168663   epsilon: 0.8640452800029514    steps: 152    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 901   score: 2.0   memory length: 168862   epsilon: 0.86365126000296    steps: 199    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 902   score: 2.0   memory length: 169079   epsilon: 0.8632216000029693    steps: 217    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 903   score: 2.0   memory length: 169278   epsilon: 0.8628275800029779    steps: 199    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 904   score: 1.0   memory length: 169448   epsilon: 0.8624909800029852    steps: 170    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 905   score: 6.0   memory length: 169804   epsilon: 0.8617861000030005    steps: 356    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 906   score: 2.0   memory length: 169985   epsilon: 0.8614277200030083    steps: 181    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 907   score: 3.0   memory length: 170198   epsilon: 0.8610059800030174    steps: 213    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 908   score: 4.0   memory length: 170474   epsilon: 0.8604595000030293    steps: 276    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 909   score: 6.0   memory length: 170853   epsilon: 0.8597090800030456    steps: 379    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 910   score: 3.0   memory length: 171100   epsilon: 0.8592200200030562    steps: 247    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 911   score: 1.0   memory length: 171273   epsilon: 0.8588774800030636    steps: 173    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 912   score: 7.0   memory length: 171676   epsilon: 0.858079540003081    steps: 403    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 913   score: 2.0   memory length: 171897   epsilon: 0.8576419600030905    steps: 221    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 914   score: 10.0   memory length: 172281   epsilon: 0.856881640003107    steps: 384    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 915   score: 0.0   memory length: 172405   epsilon: 0.8566361200031123    steps: 124    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 916   score: 4.0   memory length: 172682   epsilon: 0.8560876600031242    steps: 277    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 917   score: 2.0   memory length: 172882   epsilon: 0.8556916600031328    steps: 200    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 918   score: 0.0   memory length: 173005   epsilon: 0.8554481200031381    steps: 123    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 919   score: 2.0   memory length: 173227   epsilon: 0.8550085600031476    steps: 222    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 920   score: 1.0   memory length: 173396   epsilon: 0.8546739400031549    steps: 169    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 921   score: 2.0   memory length: 173614   epsilon: 0.8542423000031643    steps: 218    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 922   score: 3.0   memory length: 173882   epsilon: 0.8537116600031758    steps: 268    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 923   score: 1.0   memory length: 174054   epsilon: 0.8533711000031832    steps: 172    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 924   score: 0.0   memory length: 174178   epsilon: 0.8531255800031885    steps: 124    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 925   score: 0.0   memory length: 174302   epsilon: 0.8528800600031938    steps: 124    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 926   score: 4.0   memory length: 174563   epsilon: 0.852363280003205    steps: 261    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 927   score: 5.0   memory length: 174855   epsilon: 0.8517851200032176    steps: 292    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 928   score: 2.0   memory length: 175054   epsilon: 0.8513911000032262    steps: 199    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 929   score: 2.0   memory length: 175275   epsilon: 0.8509535200032357    steps: 221    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 930   score: 2.0   memory length: 175493   epsilon: 0.850521880003245    steps: 218    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 931   score: 2.0   memory length: 175691   epsilon: 0.8501298400032535    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 932   score: 3.0   memory length: 175918   epsilon: 0.8496803800032633    steps: 227    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 933   score: 0.0   memory length: 176042   epsilon: 0.8494348600032686    steps: 124    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 934   score: 2.0   memory length: 176241   epsilon: 0.8490408400032772    steps: 199    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 935   score: 3.0   memory length: 176471   epsilon: 0.8485854400032871    steps: 230    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 936   score: 0.0   memory length: 176594   epsilon: 0.8483419000032923    steps: 123    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 937   score: 6.0   memory length: 176971   epsilon: 0.8475954400033086    steps: 377    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 938   score: 2.0   memory length: 177169   epsilon: 0.8472034000033171    steps: 198    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 939   score: 2.0   memory length: 177388   epsilon: 0.8467697800033265    steps: 219    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 940   score: 5.0   memory length: 177696   epsilon: 0.8461599400033397    steps: 308    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 941   score: 4.0   memory length: 177992   epsilon: 0.8455738600033524    steps: 296    lr: 0.0001     evaluation reward: 2.03\n",
            "episode: 942   score: 3.0   memory length: 178258   epsilon: 0.8450471800033639    steps: 266    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 943   score: 4.0   memory length: 178532   epsilon: 0.8445046600033757    steps: 274    lr: 0.0001     evaluation reward: 2.06\n",
            "episode: 944   score: 4.0   memory length: 178812   epsilon: 0.8439502600033877    steps: 280    lr: 0.0001     evaluation reward: 2.08\n",
            "episode: 945   score: 3.0   memory length: 179041   epsilon: 0.8434968400033975    steps: 229    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 946   score: 2.0   memory length: 179240   epsilon: 0.8431028200034061    steps: 199    lr: 0.0001     evaluation reward: 2.08\n",
            "episode: 947   score: 2.0   memory length: 179439   epsilon: 0.8427088000034146    steps: 199    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 948   score: 4.0   memory length: 179697   epsilon: 0.8421979600034257    steps: 258    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 949   score: 2.0   memory length: 179898   epsilon: 0.8417999800034344    steps: 201    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 950   score: 1.0   memory length: 180049   epsilon: 0.8415010000034409    steps: 151    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 951   score: 0.0   memory length: 180172   epsilon: 0.8412574600034461    steps: 123    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 952   score: 2.0   memory length: 180391   epsilon: 0.8408238400034556    steps: 219    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 953   score: 2.0   memory length: 180609   epsilon: 0.8403922000034649    steps: 218    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 954   score: 4.0   memory length: 180919   epsilon: 0.8397784000034783    steps: 310    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 955   score: 5.0   memory length: 181264   epsilon: 0.8390953000034931    steps: 345    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 956   score: 2.0   memory length: 181462   epsilon: 0.8387032600035016    steps: 198    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 957   score: 3.0   memory length: 181714   epsilon: 0.8382043000035124    steps: 252    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 958   score: 3.0   memory length: 181965   epsilon: 0.8377073200035232    steps: 251    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 959   score: 4.0   memory length: 182257   epsilon: 0.8371291600035358    steps: 292    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 960   score: 2.0   memory length: 182456   epsilon: 0.8367351400035443    steps: 199    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 961   score: 2.0   memory length: 182658   epsilon: 0.836335180003553    steps: 202    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 962   score: 5.0   memory length: 182983   epsilon: 0.835691680003567    steps: 325    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 963   score: 2.0   memory length: 183200   epsilon: 0.8352620200035763    steps: 217    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 964   score: 3.0   memory length: 183445   epsilon: 0.8347769200035868    steps: 245    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 965   score: 1.0   memory length: 183615   epsilon: 0.8344403200035941    steps: 170    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 966   score: 3.0   memory length: 183862   epsilon: 0.8339512600036048    steps: 247    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 967   score: 3.0   memory length: 184129   epsilon: 0.8334226000036162    steps: 267    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 968   score: 0.0   memory length: 184253   epsilon: 0.8331770800036216    steps: 124    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 969   score: 5.0   memory length: 184614   epsilon: 0.8324623000036371    steps: 361    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 970   score: 0.0   memory length: 184738   epsilon: 0.8322167800036424    steps: 124    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 971   score: 0.0   memory length: 184861   epsilon: 0.8319732400036477    steps: 123    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 972   score: 1.0   memory length: 185013   epsilon: 0.8316722800036542    steps: 152    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 973   score: 2.0   memory length: 185230   epsilon: 0.8312426200036636    steps: 217    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 974   score: 3.0   memory length: 185477   epsilon: 0.8307535600036742    steps: 247    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 975   score: 2.0   memory length: 185676   epsilon: 0.8303595400036827    steps: 199    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 976   score: 4.0   memory length: 185957   epsilon: 0.8298031600036948    steps: 281    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 977   score: 2.0   memory length: 186156   epsilon: 0.8294091400037034    steps: 199    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 978   score: 2.0   memory length: 186355   epsilon: 0.8290151200037119    steps: 199    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 979   score: 4.0   memory length: 186670   epsilon: 0.8283914200037255    steps: 315    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 980   score: 4.0   memory length: 186967   epsilon: 0.8278033600037382    steps: 297    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 981   score: 4.0   memory length: 187271   epsilon: 0.8272014400037513    steps: 304    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 982   score: 2.0   memory length: 187488   epsilon: 0.8267717800037606    steps: 217    lr: 0.0001     evaluation reward: 2.48\n",
            "episode: 983   score: 5.0   memory length: 187833   epsilon: 0.8260886800037754    steps: 345    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 984   score: 4.0   memory length: 188129   epsilon: 0.8255026000037882    steps: 296    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 985   score: 3.0   memory length: 188355   epsilon: 0.8250551200037979    steps: 226    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 986   score: 3.0   memory length: 188585   epsilon: 0.8245997200038078    steps: 230    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 987   score: 3.0   memory length: 188814   epsilon: 0.8241463000038176    steps: 229    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 988   score: 3.0   memory length: 189042   epsilon: 0.8236948600038274    steps: 228    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 989   score: 2.0   memory length: 189225   epsilon: 0.8233325200038353    steps: 183    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 990   score: 2.0   memory length: 189423   epsilon: 0.8229404800038438    steps: 198    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 991   score: 0.0   memory length: 189546   epsilon: 0.8226969400038491    steps: 123    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 992   score: 0.0   memory length: 189670   epsilon: 0.8224514200038544    steps: 124    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 993   score: 3.0   memory length: 189919   epsilon: 0.8219584000038651    steps: 249    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 994   score: 3.0   memory length: 190188   epsilon: 0.8214257800038767    steps: 269    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 995   score: 1.0   memory length: 190358   epsilon: 0.821089180003884    steps: 170    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 996   score: 5.0   memory length: 190701   epsilon: 0.8204100400038987    steps: 343    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 997   score: 1.0   memory length: 190874   epsilon: 0.8200675000039062    steps: 173    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 998   score: 1.0   memory length: 191025   epsilon: 0.8197685200039126    steps: 151    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 999   score: 3.0   memory length: 191275   epsilon: 0.8192735200039234    steps: 250    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 1000   score: 2.0   memory length: 191496   epsilon: 0.8188359400039329    steps: 221    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 1001   score: 3.0   memory length: 191741   epsilon: 0.8183508400039434    steps: 245    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 1002   score: 2.0   memory length: 191940   epsilon: 0.817956820003952    steps: 199    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 1003   score: 1.0   memory length: 192092   epsilon: 0.8176558600039585    steps: 152    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 1004   score: 2.0   memory length: 192312   epsilon: 0.817220260003968    steps: 220    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 1005   score: 3.0   memory length: 192538   epsilon: 0.8167727800039777    steps: 226    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 1006   score: 5.0   memory length: 192826   epsilon: 0.8162025400039901    steps: 288    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 1007   score: 3.0   memory length: 193056   epsilon: 0.815747140004    steps: 230    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 1008   score: 2.0   memory length: 193257   epsilon: 0.8153491600040086    steps: 201    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 1009   score: 3.0   memory length: 193525   epsilon: 0.8148185200040201    steps: 268    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 1010   score: 3.0   memory length: 193774   epsilon: 0.8143255000040308    steps: 249    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 1011   score: 3.0   memory length: 194022   epsilon: 0.8138344600040415    steps: 248    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 1012   score: 3.0   memory length: 194249   epsilon: 0.8133850000040512    steps: 227    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 1013   score: 0.0   memory length: 194372   epsilon: 0.8131414600040565    steps: 123    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 1014   score: 4.0   memory length: 194648   epsilon: 0.8125949800040684    steps: 276    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 1015   score: 0.0   memory length: 194771   epsilon: 0.8123514400040737    steps: 123    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 1016   score: 1.0   memory length: 194941   epsilon: 0.812014840004081    steps: 170    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 1017   score: 0.0   memory length: 195065   epsilon: 0.8117693200040863    steps: 124    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 1018   score: 1.0   memory length: 195236   epsilon: 0.8114307400040937    steps: 171    lr: 0.0001     evaluation reward: 2.42\n",
            "episode: 1019   score: 0.0   memory length: 195360   epsilon: 0.811185220004099    steps: 124    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 1020   score: 2.0   memory length: 195579   epsilon: 0.8107516000041084    steps: 219    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 1021   score: 4.0   memory length: 195835   epsilon: 0.8102447200041194    steps: 256    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 1022   score: 2.0   memory length: 196034   epsilon: 0.809850700004128    steps: 199    lr: 0.0001     evaluation reward: 2.42\n",
            "episode: 1023   score: 3.0   memory length: 196261   epsilon: 0.8094012400041377    steps: 227    lr: 0.0001     evaluation reward: 2.44\n",
            "episode: 1024   score: 3.0   memory length: 196527   epsilon: 0.8088745600041491    steps: 266    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1025   score: 2.0   memory length: 196725   epsilon: 0.8084825200041577    steps: 198    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 1026   score: 0.0   memory length: 196848   epsilon: 0.8082389800041629    steps: 123    lr: 0.0001     evaluation reward: 2.45\n",
            "episode: 1027   score: 3.0   memory length: 197095   epsilon: 0.8077499200041736    steps: 247    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 1028   score: 2.0   memory length: 197293   epsilon: 0.8073578800041821    steps: 198    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 1029   score: 2.0   memory length: 197492   epsilon: 0.8069638600041906    steps: 199    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 1030   score: 3.0   memory length: 197737   epsilon: 0.8064787600042012    steps: 245    lr: 0.0001     evaluation reward: 2.44\n",
            "episode: 1031   score: 2.0   memory length: 197956   epsilon: 0.8060451400042106    steps: 219    lr: 0.0001     evaluation reward: 2.44\n",
            "episode: 1032   score: 4.0   memory length: 198234   epsilon: 0.8054947000042225    steps: 278    lr: 0.0001     evaluation reward: 2.45\n",
            "episode: 1033   score: 2.0   memory length: 198455   epsilon: 0.805057120004232    steps: 221    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1034   score: 3.0   memory length: 198703   epsilon: 0.8045660800042427    steps: 248    lr: 0.0001     evaluation reward: 2.48\n",
            "episode: 1035   score: 2.0   memory length: 198901   epsilon: 0.8041740400042512    steps: 198    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1036   score: 2.0   memory length: 199119   epsilon: 0.8037424000042606    steps: 218    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 1037   score: 3.0   memory length: 199348   epsilon: 0.8032889800042704    steps: 229    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 1038   score: 10.0   memory length: 199718   epsilon: 0.8025563800042863    steps: 370    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 1039   score: 5.0   memory length: 200044   epsilon: 0.8019109000043003    steps: 326    lr: 4e-05     evaluation reward: 2.57\n",
            "episode: 1040   score: 8.0   memory length: 200354   epsilon: 0.8012971000043136    steps: 310    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1041   score: 4.0   memory length: 200631   epsilon: 0.8007486400043256    steps: 277    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1042   score: 3.0   memory length: 200878   epsilon: 0.8002595800043362    steps: 247    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1043   score: 2.0   memory length: 201077   epsilon: 0.7998655600043447    steps: 199    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1044   score: 0.0   memory length: 201201   epsilon: 0.79962004000435    steps: 124    lr: 4e-05     evaluation reward: 2.54\n",
            "episode: 1045   score: 2.0   memory length: 201403   epsilon: 0.7992200800043587    steps: 202    lr: 4e-05     evaluation reward: 2.53\n",
            "episode: 1046   score: 2.0   memory length: 201621   epsilon: 0.7987884400043681    steps: 218    lr: 4e-05     evaluation reward: 2.53\n",
            "episode: 1047   score: 3.0   memory length: 201869   epsilon: 0.7982974000043788    steps: 248    lr: 4e-05     evaluation reward: 2.54\n",
            "episode: 1048   score: 2.0   memory length: 202068   epsilon: 0.7979033800043873    steps: 199    lr: 4e-05     evaluation reward: 2.52\n",
            "episode: 1049   score: 2.0   memory length: 202269   epsilon: 0.797505400004396    steps: 201    lr: 4e-05     evaluation reward: 2.52\n",
            "episode: 1050   score: 2.0   memory length: 202467   epsilon: 0.7971133600044045    steps: 198    lr: 4e-05     evaluation reward: 2.53\n",
            "episode: 1051   score: 1.0   memory length: 202638   epsilon: 0.7967747800044118    steps: 171    lr: 4e-05     evaluation reward: 2.54\n",
            "episode: 1052   score: 0.0   memory length: 202762   epsilon: 0.7965292600044172    steps: 124    lr: 4e-05     evaluation reward: 2.52\n",
            "episode: 1053   score: 3.0   memory length: 203009   epsilon: 0.7960402000044278    steps: 247    lr: 4e-05     evaluation reward: 2.53\n",
            "episode: 1054   score: 0.0   memory length: 203132   epsilon: 0.795796660004433    steps: 123    lr: 4e-05     evaluation reward: 2.49\n",
            "episode: 1055   score: 1.0   memory length: 203284   epsilon: 0.7954957000044396    steps: 152    lr: 4e-05     evaluation reward: 2.45\n",
            "episode: 1056   score: 3.0   memory length: 203529   epsilon: 0.7950106000044501    steps: 245    lr: 4e-05     evaluation reward: 2.46\n",
            "episode: 1057   score: 5.0   memory length: 203855   epsilon: 0.7943651200044641    steps: 326    lr: 4e-05     evaluation reward: 2.48\n",
            "episode: 1058   score: 3.0   memory length: 204121   epsilon: 0.7938384400044756    steps: 266    lr: 4e-05     evaluation reward: 2.48\n",
            "episode: 1059   score: 4.0   memory length: 204378   epsilon: 0.7933295800044866    steps: 257    lr: 4e-05     evaluation reward: 2.48\n",
            "episode: 1060   score: 9.0   memory length: 204751   epsilon: 0.7925910400045026    steps: 373    lr: 4e-05     evaluation reward: 2.55\n",
            "episode: 1061   score: 3.0   memory length: 204998   epsilon: 0.7921019800045133    steps: 247    lr: 4e-05     evaluation reward: 2.56\n",
            "episode: 1062   score: 2.0   memory length: 205179   epsilon: 0.791743600004521    steps: 181    lr: 4e-05     evaluation reward: 2.53\n",
            "episode: 1063   score: 3.0   memory length: 205410   epsilon: 0.791286220004531    steps: 231    lr: 4e-05     evaluation reward: 2.54\n",
            "episode: 1064   score: 2.0   memory length: 205609   epsilon: 0.7908922000045395    steps: 199    lr: 4e-05     evaluation reward: 2.53\n",
            "episode: 1065   score: 5.0   memory length: 205919   epsilon: 0.7902784000045529    steps: 310    lr: 4e-05     evaluation reward: 2.57\n",
            "episode: 1066   score: 4.0   memory length: 206199   epsilon: 0.7897240000045649    steps: 280    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1067   score: 6.0   memory length: 206561   epsilon: 0.7890072400045804    steps: 362    lr: 4e-05     evaluation reward: 2.61\n",
            "episode: 1068   score: 3.0   memory length: 206795   epsilon: 0.7885439200045905    steps: 234    lr: 4e-05     evaluation reward: 2.64\n",
            "episode: 1069   score: 3.0   memory length: 207023   epsilon: 0.7880924800046003    steps: 228    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1070   score: 1.0   memory length: 207175   epsilon: 0.7877915200046068    steps: 152    lr: 4e-05     evaluation reward: 2.63\n",
            "episode: 1071   score: 3.0   memory length: 207402   epsilon: 0.7873420600046166    steps: 227    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1072   score: 2.0   memory length: 207603   epsilon: 0.7869440800046252    steps: 201    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1073   score: 1.0   memory length: 207774   epsilon: 0.7866055000046326    steps: 171    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1074   score: 1.0   memory length: 207926   epsilon: 0.7863045400046391    steps: 152    lr: 4e-05     evaluation reward: 2.64\n",
            "episode: 1075   score: 2.0   memory length: 208124   epsilon: 0.7859125000046476    steps: 198    lr: 4e-05     evaluation reward: 2.64\n",
            "episode: 1076   score: 7.0   memory length: 208530   epsilon: 0.7851086200046651    steps: 406    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1077   score: 1.0   memory length: 208682   epsilon: 0.7848076600046716    steps: 152    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1078   score: 3.0   memory length: 208910   epsilon: 0.7843562200046814    steps: 228    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1079   score: 3.0   memory length: 209136   epsilon: 0.7839087400046911    steps: 226    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1080   score: 4.0   memory length: 209434   epsilon: 0.7833187000047039    steps: 298    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1081   score: 4.0   memory length: 209721   epsilon: 0.7827504400047163    steps: 287    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1082   score: 2.0   memory length: 209938   epsilon: 0.7823207800047256    steps: 217    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1083   score: 1.0   memory length: 210089   epsilon: 0.7820218000047321    steps: 151    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1084   score: 2.0   memory length: 210288   epsilon: 0.7816277800047406    steps: 199    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1085   score: 3.0   memory length: 210535   epsilon: 0.7811387200047513    steps: 247    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1086   score: 2.0   memory length: 210738   epsilon: 0.78073678000476    steps: 203    lr: 4e-05     evaluation reward: 2.59\n",
            "episode: 1087   score: 2.0   memory length: 210919   epsilon: 0.7803784000047678    steps: 181    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1088   score: 3.0   memory length: 211187   epsilon: 0.7798477600047793    steps: 268    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1089   score: 4.0   memory length: 211445   epsilon: 0.7793369200047904    steps: 258    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1090   score: 2.0   memory length: 211644   epsilon: 0.7789429000047989    steps: 199    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1091   score: 2.0   memory length: 211863   epsilon: 0.7785092800048083    steps: 219    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1092   score: 0.0   memory length: 211987   epsilon: 0.7782637600048137    steps: 124    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1093   score: 0.0   memory length: 212110   epsilon: 0.778020220004819    steps: 123    lr: 4e-05     evaluation reward: 2.59\n",
            "episode: 1094   score: 3.0   memory length: 212359   epsilon: 0.7775272000048297    steps: 249    lr: 4e-05     evaluation reward: 2.59\n",
            "episode: 1095   score: 2.0   memory length: 212557   epsilon: 0.7771351600048382    steps: 198    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1096   score: 1.0   memory length: 212727   epsilon: 0.7767985600048455    steps: 170    lr: 4e-05     evaluation reward: 2.56\n",
            "episode: 1097   score: 4.0   memory length: 213004   epsilon: 0.7762501000048574    steps: 277    lr: 4e-05     evaluation reward: 2.59\n",
            "episode: 1098   score: 7.0   memory length: 213461   epsilon: 0.775345240004877    steps: 457    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1099   score: 1.0   memory length: 213613   epsilon: 0.7750442800048836    steps: 152    lr: 4e-05     evaluation reward: 2.63\n",
            "episode: 1100   score: 4.0   memory length: 213890   epsilon: 0.7744958200048955    steps: 277    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1101   score: 3.0   memory length: 214121   epsilon: 0.7740384400049054    steps: 231    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1102   score: 1.0   memory length: 214291   epsilon: 0.7737018400049127    steps: 170    lr: 4e-05     evaluation reward: 2.64\n",
            "episode: 1103   score: 2.0   memory length: 214490   epsilon: 0.7733078200049213    steps: 199    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1104   score: 5.0   memory length: 214783   epsilon: 0.7727276800049339    steps: 293    lr: 4e-05     evaluation reward: 2.68\n",
            "episode: 1105   score: 5.0   memory length: 215113   epsilon: 0.772074280004948    steps: 330    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1106   score: 6.0   memory length: 215490   epsilon: 0.7713278200049642    steps: 377    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1107   score: 1.0   memory length: 215642   epsilon: 0.7710268600049708    steps: 152    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1108   score: 5.0   memory length: 215922   epsilon: 0.7704724600049828    steps: 280    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1109   score: 2.0   memory length: 216121   epsilon: 0.7700784400049914    steps: 199    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1110   score: 5.0   memory length: 216429   epsilon: 0.7694686000050046    steps: 308    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1111   score: 4.0   memory length: 216726   epsilon: 0.7688805400050174    steps: 297    lr: 4e-05     evaluation reward: 2.74\n",
            "episode: 1112   score: 2.0   memory length: 216946   epsilon: 0.7684449400050268    steps: 220    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1113   score: 4.0   memory length: 217222   epsilon: 0.7678984600050387    steps: 276    lr: 4e-05     evaluation reward: 2.77\n",
            "episode: 1114   score: 2.0   memory length: 217440   epsilon: 0.7674668200050481    steps: 218    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1115   score: 3.0   memory length: 217710   epsilon: 0.7669322200050597    steps: 270    lr: 4e-05     evaluation reward: 2.78\n",
            "episode: 1116   score: 2.0   memory length: 217909   epsilon: 0.7665382000050682    steps: 199    lr: 4e-05     evaluation reward: 2.79\n",
            "episode: 1117   score: 2.0   memory length: 218108   epsilon: 0.7661441800050768    steps: 199    lr: 4e-05     evaluation reward: 2.81\n",
            "episode: 1118   score: 3.0   memory length: 218336   epsilon: 0.7656927400050866    steps: 228    lr: 4e-05     evaluation reward: 2.83\n",
            "episode: 1119   score: 0.0   memory length: 218460   epsilon: 0.7654472200050919    steps: 124    lr: 4e-05     evaluation reward: 2.83\n",
            "episode: 1120   score: 8.0   memory length: 218846   epsilon: 0.7646829400051085    steps: 386    lr: 4e-05     evaluation reward: 2.89\n",
            "episode: 1121   score: 3.0   memory length: 219093   epsilon: 0.7641938800051191    steps: 247    lr: 4e-05     evaluation reward: 2.88\n",
            "episode: 1122   score: 5.0   memory length: 219382   epsilon: 0.7636216600051315    steps: 289    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1123   score: 3.0   memory length: 219612   epsilon: 0.7631662600051414    steps: 230    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1124   score: 2.0   memory length: 219811   epsilon: 0.76277224000515    steps: 199    lr: 4e-05     evaluation reward: 2.9\n",
            "episode: 1125   score: 2.0   memory length: 220013   epsilon: 0.7623722800051587    steps: 202    lr: 4e-05     evaluation reward: 2.9\n",
            "episode: 1126   score: 1.0   memory length: 220165   epsilon: 0.7620713200051652    steps: 152    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1127   score: 3.0   memory length: 220392   epsilon: 0.761621860005175    steps: 227    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1128   score: 5.0   memory length: 220757   epsilon: 0.7608991600051906    steps: 365    lr: 4e-05     evaluation reward: 2.94\n",
            "episode: 1129   score: 4.0   memory length: 221036   epsilon: 0.7603467400052026    steps: 279    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1130   score: 3.0   memory length: 221267   epsilon: 0.7598893600052126    steps: 231    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1131   score: 2.0   memory length: 221466   epsilon: 0.7594953400052211    steps: 199    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1132   score: 5.0   memory length: 221798   epsilon: 0.7588379800052354    steps: 332    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1133   score: 4.0   memory length: 222095   epsilon: 0.7582499200052482    steps: 297    lr: 4e-05     evaluation reward: 2.99\n",
            "episode: 1134   score: 2.0   memory length: 222280   epsilon: 0.7578836200052561    steps: 185    lr: 4e-05     evaluation reward: 2.98\n",
            "episode: 1135   score: 1.0   memory length: 222450   epsilon: 0.7575470200052634    steps: 170    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1136   score: 3.0   memory length: 222698   epsilon: 0.7570559800052741    steps: 248    lr: 4e-05     evaluation reward: 2.98\n",
            "episode: 1137   score: 10.0   memory length: 223216   epsilon: 0.7560303400052963    steps: 518    lr: 4e-05     evaluation reward: 3.05\n",
            "episode: 1138   score: 4.0   memory length: 223512   epsilon: 0.7554442600053091    steps: 296    lr: 4e-05     evaluation reward: 2.99\n",
            "episode: 1139   score: 7.0   memory length: 223918   epsilon: 0.7546403800053265    steps: 406    lr: 4e-05     evaluation reward: 3.01\n",
            "episode: 1140   score: 4.0   memory length: 224160   epsilon: 0.7541612200053369    steps: 242    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1141   score: 3.0   memory length: 224430   epsilon: 0.7536266200053485    steps: 270    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1142   score: 1.0   memory length: 224582   epsilon: 0.7533256600053551    steps: 152    lr: 4e-05     evaluation reward: 2.94\n",
            "episode: 1143   score: 5.0   memory length: 224910   epsilon: 0.7526762200053692    steps: 328    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1144   score: 5.0   memory length: 225256   epsilon: 0.751991140005384    steps: 346    lr: 4e-05     evaluation reward: 3.02\n",
            "episode: 1145   score: 5.0   memory length: 225547   epsilon: 0.7514149600053965    steps: 291    lr: 4e-05     evaluation reward: 3.05\n",
            "episode: 1146   score: 2.0   memory length: 225728   epsilon: 0.7510565800054043    steps: 181    lr: 4e-05     evaluation reward: 3.05\n",
            "episode: 1147   score: 1.0   memory length: 225879   epsilon: 0.7507576000054108    steps: 151    lr: 4e-05     evaluation reward: 3.03\n",
            "episode: 1148   score: 1.0   memory length: 226031   epsilon: 0.7504566400054173    steps: 152    lr: 4e-05     evaluation reward: 3.02\n",
            "episode: 1149   score: 3.0   memory length: 226258   epsilon: 0.7500071800054271    steps: 227    lr: 4e-05     evaluation reward: 3.03\n",
            "episode: 1150   score: 1.0   memory length: 226429   epsilon: 0.7496686000054344    steps: 171    lr: 4e-05     evaluation reward: 3.02\n",
            "episode: 1151   score: 3.0   memory length: 226678   epsilon: 0.7491755800054452    steps: 249    lr: 4e-05     evaluation reward: 3.04\n",
            "episode: 1152   score: 2.0   memory length: 226859   epsilon: 0.7488172000054529    steps: 181    lr: 4e-05     evaluation reward: 3.06\n",
            "episode: 1153   score: 1.0   memory length: 227011   epsilon: 0.7485162400054595    steps: 152    lr: 4e-05     evaluation reward: 3.04\n",
            "episode: 1154   score: 4.0   memory length: 227268   epsilon: 0.7480073800054705    steps: 257    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1155   score: 3.0   memory length: 227514   epsilon: 0.7475203000054811    steps: 246    lr: 4e-05     evaluation reward: 3.1\n",
            "episode: 1156   score: 2.0   memory length: 227697   epsilon: 0.747157960005489    steps: 183    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1157   score: 4.0   memory length: 227994   epsilon: 0.7465699000055017    steps: 297    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1158   score: 2.0   memory length: 228192   epsilon: 0.7461778600055102    steps: 198    lr: 4e-05     evaluation reward: 3.07\n",
            "episode: 1159   score: 3.0   memory length: 228419   epsilon: 0.74572840000552    steps: 227    lr: 4e-05     evaluation reward: 3.06\n",
            "episode: 1160   score: 3.0   memory length: 228648   epsilon: 0.7452749800055298    steps: 229    lr: 4e-05     evaluation reward: 3.0\n",
            "episode: 1161   score: 0.0   memory length: 228772   epsilon: 0.7450294600055352    steps: 124    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1162   score: 4.0   memory length: 229040   epsilon: 0.7444988200055467    steps: 268    lr: 4e-05     evaluation reward: 2.99\n",
            "episode: 1163   score: 4.0   memory length: 229298   epsilon: 0.7439879800055578    steps: 258    lr: 4e-05     evaluation reward: 3.0\n",
            "episode: 1164   score: 0.0   memory length: 229421   epsilon: 0.7437444400055631    steps: 123    lr: 4e-05     evaluation reward: 2.98\n",
            "episode: 1165   score: 1.0   memory length: 229573   epsilon: 0.7434434800055696    steps: 152    lr: 4e-05     evaluation reward: 2.94\n",
            "episode: 1166   score: 0.0   memory length: 229696   epsilon: 0.7431999400055749    steps: 123    lr: 4e-05     evaluation reward: 2.9\n",
            "episode: 1167   score: 3.0   memory length: 229927   epsilon: 0.7427425600055848    steps: 231    lr: 4e-05     evaluation reward: 2.87\n",
            "episode: 1168   score: 4.0   memory length: 230203   epsilon: 0.7421960800055967    steps: 276    lr: 4e-05     evaluation reward: 2.88\n",
            "episode: 1169   score: 2.0   memory length: 230402   epsilon: 0.7418020600056052    steps: 199    lr: 4e-05     evaluation reward: 2.87\n",
            "episode: 1170   score: 3.0   memory length: 230631   epsilon: 0.7413486400056151    steps: 229    lr: 4e-05     evaluation reward: 2.89\n",
            "episode: 1171   score: 3.0   memory length: 230858   epsilon: 0.7408991800056248    steps: 227    lr: 4e-05     evaluation reward: 2.89\n",
            "episode: 1172   score: 2.0   memory length: 231057   epsilon: 0.7405051600056334    steps: 199    lr: 4e-05     evaluation reward: 2.89\n",
            "episode: 1173   score: 3.0   memory length: 231304   epsilon: 0.740016100005644    steps: 247    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1174   score: 1.0   memory length: 231456   epsilon: 0.7397151400056505    steps: 152    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1175   score: 2.0   memory length: 231654   epsilon: 0.739323100005659    steps: 198    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1176   score: 7.0   memory length: 232063   epsilon: 0.7385132800056766    steps: 409    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1177   score: 1.0   memory length: 232215   epsilon: 0.7382123200056832    steps: 152    lr: 4e-05     evaluation reward: 2.91\n",
            "episode: 1178   score: 9.0   memory length: 232634   epsilon: 0.7373827000057012    steps: 419    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1179   score: 2.0   memory length: 232835   epsilon: 0.7369847200057098    steps: 201    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1180   score: 4.0   memory length: 233147   epsilon: 0.7363669600057232    steps: 312    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1181   score: 4.0   memory length: 233445   epsilon: 0.735776920005736    steps: 298    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1182   score: 3.0   memory length: 233715   epsilon: 0.7352423200057476    steps: 270    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1183   score: 5.0   memory length: 234041   epsilon: 0.7345968400057616    steps: 326    lr: 4e-05     evaluation reward: 3.01\n",
            "episode: 1184   score: 0.0   memory length: 234165   epsilon: 0.734351320005767    steps: 124    lr: 4e-05     evaluation reward: 2.99\n",
            "episode: 1185   score: 4.0   memory length: 234424   epsilon: 0.7338385000057781    steps: 259    lr: 4e-05     evaluation reward: 3.0\n",
            "episode: 1186   score: 0.0   memory length: 234547   epsilon: 0.7335949600057834    steps: 123    lr: 4e-05     evaluation reward: 2.98\n",
            "episode: 1187   score: 3.0   memory length: 234795   epsilon: 0.733103920005794    steps: 248    lr: 4e-05     evaluation reward: 2.99\n",
            "episode: 1188   score: 5.0   memory length: 235141   epsilon: 0.7324188400058089    steps: 346    lr: 4e-05     evaluation reward: 3.01\n",
            "episode: 1189   score: 6.0   memory length: 235494   epsilon: 0.7317199000058241    steps: 353    lr: 4e-05     evaluation reward: 3.03\n",
            "episode: 1190   score: 2.0   memory length: 235675   epsilon: 0.7313615200058319    steps: 181    lr: 4e-05     evaluation reward: 3.03\n",
            "episode: 1191   score: 4.0   memory length: 235972   epsilon: 0.7307734600058446    steps: 297    lr: 4e-05     evaluation reward: 3.05\n",
            "episode: 1192   score: 3.0   memory length: 236221   epsilon: 0.7302804400058553    steps: 249    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1193   score: 1.0   memory length: 236393   epsilon: 0.7299398800058627    steps: 172    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1194   score: 6.0   memory length: 236747   epsilon: 0.729238960005878    steps: 354    lr: 4e-05     evaluation reward: 3.12\n",
            "episode: 1195   score: 4.0   memory length: 237048   epsilon: 0.7286429800058909    steps: 301    lr: 4e-05     evaluation reward: 3.14\n",
            "episode: 1196   score: 6.0   memory length: 237404   epsilon: 0.7279381000059062    steps: 356    lr: 4e-05     evaluation reward: 3.19\n",
            "episode: 1197   score: 1.0   memory length: 237556   epsilon: 0.7276371400059127    steps: 152    lr: 4e-05     evaluation reward: 3.16\n",
            "episode: 1198   score: 2.0   memory length: 237775   epsilon: 0.7272035200059221    steps: 219    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1199   score: 3.0   memory length: 238022   epsilon: 0.7267144600059328    steps: 247    lr: 4e-05     evaluation reward: 3.13\n",
            "episode: 1200   score: 4.0   memory length: 238302   epsilon: 0.7261600600059448    steps: 280    lr: 4e-05     evaluation reward: 3.13\n",
            "episode: 1201   score: 3.0   memory length: 238549   epsilon: 0.7256710000059554    steps: 247    lr: 4e-05     evaluation reward: 3.13\n",
            "episode: 1202   score: 1.0   memory length: 238700   epsilon: 0.7253720200059619    steps: 151    lr: 4e-05     evaluation reward: 3.13\n",
            "episode: 1203   score: 3.0   memory length: 238932   epsilon: 0.7249126600059719    steps: 232    lr: 4e-05     evaluation reward: 3.14\n",
            "episode: 1204   score: 5.0   memory length: 239258   epsilon: 0.7242671800059859    steps: 326    lr: 4e-05     evaluation reward: 3.14\n",
            "episode: 1205   score: 0.0   memory length: 239382   epsilon: 0.7240216600059912    steps: 124    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1206   score: 2.0   memory length: 239562   epsilon: 0.723665260005999    steps: 180    lr: 4e-05     evaluation reward: 3.05\n",
            "episode: 1207   score: 1.0   memory length: 239714   epsilon: 0.7233643000060055    steps: 152    lr: 4e-05     evaluation reward: 3.05\n",
            "episode: 1208   score: 4.0   memory length: 239954   epsilon: 0.7228891000060158    steps: 240    lr: 4e-05     evaluation reward: 3.04\n",
            "episode: 1209   score: 2.0   memory length: 240173   epsilon: 0.7224554800060252    steps: 219    lr: 4e-05     evaluation reward: 3.04\n",
            "episode: 1210   score: 4.0   memory length: 240485   epsilon: 0.7218377200060386    steps: 312    lr: 4e-05     evaluation reward: 3.03\n",
            "episode: 1211   score: 4.0   memory length: 240803   epsilon: 0.7212080800060523    steps: 318    lr: 4e-05     evaluation reward: 3.03\n",
            "episode: 1212   score: 5.0   memory length: 241130   epsilon: 0.7205606200060664    steps: 327    lr: 4e-05     evaluation reward: 3.06\n",
            "episode: 1213   score: 5.0   memory length: 241427   epsilon: 0.7199725600060791    steps: 297    lr: 4e-05     evaluation reward: 3.07\n",
            "episode: 1214   score: 4.0   memory length: 241724   epsilon: 0.7193845000060919    steps: 297    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1215   score: 0.0   memory length: 241848   epsilon: 0.7191389800060972    steps: 124    lr: 4e-05     evaluation reward: 3.06\n",
            "episode: 1216   score: 4.0   memory length: 242111   epsilon: 0.7186182400061085    steps: 263    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1217   score: 2.0   memory length: 242292   epsilon: 0.7182598600061163    steps: 181    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1218   score: 6.0   memory length: 242633   epsilon: 0.717584680006131    steps: 341    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1219   score: 5.0   memory length: 242959   epsilon: 0.716939200006145    steps: 326    lr: 4e-05     evaluation reward: 3.16\n",
            "episode: 1220   score: 3.0   memory length: 243204   epsilon: 0.7164541000061555    steps: 245    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1221   score: 3.0   memory length: 243432   epsilon: 0.7160026600061653    steps: 228    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1222   score: 3.0   memory length: 243667   epsilon: 0.7155373600061754    steps: 235    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1223   score: 4.0   memory length: 243963   epsilon: 0.7149512800061881    steps: 296    lr: 4e-05     evaluation reward: 3.1\n",
            "episode: 1224   score: 3.0   memory length: 244190   epsilon: 0.7145018200061979    steps: 227    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1225   score: 4.0   memory length: 244506   epsilon: 0.7138761400062115    steps: 316    lr: 4e-05     evaluation reward: 3.13\n",
            "episode: 1226   score: 2.0   memory length: 244706   epsilon: 0.7134801400062201    steps: 200    lr: 4e-05     evaluation reward: 3.14\n",
            "episode: 1227   score: 6.0   memory length: 245042   epsilon: 0.7128148600062345    steps: 336    lr: 4e-05     evaluation reward: 3.17\n",
            "episode: 1228   score: 3.0   memory length: 245270   epsilon: 0.7123634200062443    steps: 228    lr: 4e-05     evaluation reward: 3.15\n",
            "episode: 1229   score: 0.0   memory length: 245394   epsilon: 0.7121179000062496    steps: 124    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1230   score: 2.0   memory length: 245574   epsilon: 0.7117615000062574    steps: 180    lr: 4e-05     evaluation reward: 3.1\n",
            "episode: 1231   score: 1.0   memory length: 245726   epsilon: 0.7114605400062639    steps: 152    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1232   score: 4.0   memory length: 246022   epsilon: 0.7108744600062766    steps: 296    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1233   score: 4.0   memory length: 246280   epsilon: 0.7103636200062877    steps: 258    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1234   score: 3.0   memory length: 246529   epsilon: 0.7098706000062984    steps: 249    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1235   score: 8.0   memory length: 247004   epsilon: 0.7089301000063188    steps: 475    lr: 4e-05     evaluation reward: 3.16\n",
            "episode: 1236   score: 3.0   memory length: 247252   epsilon: 0.7084390600063295    steps: 248    lr: 4e-05     evaluation reward: 3.16\n",
            "episode: 1237   score: 2.0   memory length: 247451   epsilon: 0.7080450400063381    steps: 199    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1238   score: 2.0   memory length: 247650   epsilon: 0.7076510200063466    steps: 199    lr: 4e-05     evaluation reward: 3.06\n",
            "episode: 1239   score: 2.0   memory length: 247867   epsilon: 0.7072213600063559    steps: 217    lr: 4e-05     evaluation reward: 3.01\n",
            "episode: 1240   score: 5.0   memory length: 248191   epsilon: 0.7065798400063699    steps: 324    lr: 4e-05     evaluation reward: 3.02\n",
            "episode: 1241   score: 5.0   memory length: 248496   epsilon: 0.705975940006383    steps: 305    lr: 4e-05     evaluation reward: 3.04\n",
            "episode: 1242   score: 10.0   memory length: 248922   epsilon: 0.7051324600064013    steps: 426    lr: 4e-05     evaluation reward: 3.13\n",
            "episode: 1243   score: 4.0   memory length: 249198   epsilon: 0.7045859800064131    steps: 276    lr: 4e-05     evaluation reward: 3.12\n",
            "episode: 1244   score: 5.0   memory length: 249545   epsilon: 0.7038989200064281    steps: 347    lr: 4e-05     evaluation reward: 3.12\n",
            "episode: 1245   score: 5.0   memory length: 249870   epsilon: 0.703255420006442    steps: 325    lr: 4e-05     evaluation reward: 3.12\n",
            "episode: 1246   score: 2.0   memory length: 250053   epsilon: 0.7028930800064499    steps: 183    lr: 4e-05     evaluation reward: 3.12\n",
            "episode: 1247   score: 5.0   memory length: 250398   epsilon: 0.7022099800064647    steps: 345    lr: 4e-05     evaluation reward: 3.16\n",
            "episode: 1248   score: 4.0   memory length: 250713   epsilon: 0.7015862800064783    steps: 315    lr: 4e-05     evaluation reward: 3.19\n",
            "episode: 1249   score: 6.0   memory length: 251068   epsilon: 0.7008833800064935    steps: 355    lr: 4e-05     evaluation reward: 3.22\n",
            "episode: 1250   score: 3.0   memory length: 251300   epsilon: 0.7004240200065035    steps: 232    lr: 4e-05     evaluation reward: 3.24\n",
            "episode: 1251   score: 2.0   memory length: 251482   epsilon: 0.7000636600065113    steps: 182    lr: 4e-05     evaluation reward: 3.23\n",
            "episode: 1252   score: 2.0   memory length: 251683   epsilon: 0.69966568000652    steps: 201    lr: 4e-05     evaluation reward: 3.23\n",
            "episode: 1253   score: 4.0   memory length: 251980   epsilon: 0.6990776200065327    steps: 297    lr: 4e-05     evaluation reward: 3.26\n",
            "episode: 1254   score: 3.0   memory length: 252190   epsilon: 0.6986618200065418    steps: 210    lr: 4e-05     evaluation reward: 3.25\n",
            "episode: 1255   score: 7.0   memory length: 252576   epsilon: 0.6978975400065583    steps: 386    lr: 4e-05     evaluation reward: 3.29\n",
            "episode: 1256   score: 6.0   memory length: 252905   epsilon: 0.6972461200065725    steps: 329    lr: 4e-05     evaluation reward: 3.33\n",
            "episode: 1257   score: 8.0   memory length: 253354   epsilon: 0.6963571000065918    steps: 449    lr: 4e-05     evaluation reward: 3.37\n",
            "episode: 1258   score: 4.0   memory length: 253615   epsilon: 0.695840320006603    steps: 261    lr: 4e-05     evaluation reward: 3.39\n",
            "episode: 1259   score: 1.0   memory length: 253766   epsilon: 0.6955413400066095    steps: 151    lr: 4e-05     evaluation reward: 3.37\n",
            "episode: 1260   score: 4.0   memory length: 254043   epsilon: 0.6949928800066214    steps: 277    lr: 4e-05     evaluation reward: 3.38\n",
            "episode: 1261   score: 4.0   memory length: 254325   epsilon: 0.6944345200066335    steps: 282    lr: 4e-05     evaluation reward: 3.42\n",
            "episode: 1262   score: 11.0   memory length: 254787   epsilon: 0.6935197600066534    steps: 462    lr: 4e-05     evaluation reward: 3.49\n",
            "episode: 1263   score: 3.0   memory length: 254998   epsilon: 0.6931019800066625    steps: 211    lr: 4e-05     evaluation reward: 3.48\n",
            "episode: 1264   score: 2.0   memory length: 255181   epsilon: 0.6927396400066703    steps: 183    lr: 4e-05     evaluation reward: 3.5\n",
            "episode: 1265   score: 6.0   memory length: 255569   epsilon: 0.691971400006687    steps: 388    lr: 4e-05     evaluation reward: 3.55\n",
            "episode: 1266   score: 4.0   memory length: 255847   epsilon: 0.691420960006699    steps: 278    lr: 4e-05     evaluation reward: 3.59\n",
            "episode: 1267   score: 7.0   memory length: 256297   epsilon: 0.6905299600067183    steps: 450    lr: 4e-05     evaluation reward: 3.63\n",
            "episode: 1268   score: 2.0   memory length: 256514   epsilon: 0.6901003000067276    steps: 217    lr: 4e-05     evaluation reward: 3.61\n",
            "episode: 1269   score: 3.0   memory length: 256761   epsilon: 0.6896112400067382    steps: 247    lr: 4e-05     evaluation reward: 3.62\n",
            "episode: 1270   score: 3.0   memory length: 257010   epsilon: 0.6891182200067489    steps: 249    lr: 4e-05     evaluation reward: 3.62\n",
            "episode: 1271   score: 4.0   memory length: 257268   epsilon: 0.68860738000676    steps: 258    lr: 4e-05     evaluation reward: 3.63\n",
            "episode: 1272   score: 3.0   memory length: 257500   epsilon: 0.68814802000677    steps: 232    lr: 4e-05     evaluation reward: 3.64\n",
            "episode: 1273   score: 3.0   memory length: 257769   epsilon: 0.6876154000067816    steps: 269    lr: 4e-05     evaluation reward: 3.64\n",
            "episode: 1274   score: 9.0   memory length: 258129   epsilon: 0.686902600006797    steps: 360    lr: 4e-05     evaluation reward: 3.72\n",
            "episode: 1275   score: 4.0   memory length: 258404   epsilon: 0.6863581000068089    steps: 275    lr: 4e-05     evaluation reward: 3.74\n",
            "episode: 1276   score: 16.0   memory length: 258933   epsilon: 0.6853106800068316    steps: 529    lr: 4e-05     evaluation reward: 3.83\n",
            "episode: 1277   score: 2.0   memory length: 259134   epsilon: 0.6849127000068402    steps: 201    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1278   score: 1.0   memory length: 259285   epsilon: 0.6846137200068467    steps: 151    lr: 4e-05     evaluation reward: 3.76\n",
            "episode: 1279   score: 4.0   memory length: 259560   epsilon: 0.6840692200068585    steps: 275    lr: 4e-05     evaluation reward: 3.78\n",
            "episode: 1280   score: 2.0   memory length: 259743   epsilon: 0.6837068800068664    steps: 183    lr: 4e-05     evaluation reward: 3.76\n",
            "episode: 1281   score: 5.0   memory length: 260056   epsilon: 0.6830871400068799    steps: 313    lr: 4e-05     evaluation reward: 3.77\n",
            "episode: 1282   score: 1.0   memory length: 260208   epsilon: 0.6827861800068864    steps: 152    lr: 4e-05     evaluation reward: 3.75\n",
            "episode: 1283   score: 1.0   memory length: 260360   epsilon: 0.6824852200068929    steps: 152    lr: 4e-05     evaluation reward: 3.71\n",
            "episode: 1284   score: 5.0   memory length: 260668   epsilon: 0.6818753800069062    steps: 308    lr: 4e-05     evaluation reward: 3.76\n",
            "episode: 1285   score: 4.0   memory length: 260925   epsilon: 0.6813665200069172    steps: 257    lr: 4e-05     evaluation reward: 3.76\n",
            "episode: 1286   score: 7.0   memory length: 261308   epsilon: 0.6806081800069337    steps: 383    lr: 4e-05     evaluation reward: 3.83\n",
            "episode: 1287   score: 5.0   memory length: 261632   epsilon: 0.6799666600069476    steps: 324    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1288   score: 4.0   memory length: 261926   epsilon: 0.6793845400069602    steps: 294    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1289   score: 3.0   memory length: 262137   epsilon: 0.6789667600069693    steps: 211    lr: 4e-05     evaluation reward: 3.81\n",
            "episode: 1290   score: 5.0   memory length: 262503   epsilon: 0.678242080006985    steps: 366    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1291   score: 4.0   memory length: 262782   epsilon: 0.677689660006997    steps: 279    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1292   score: 4.0   memory length: 263081   epsilon: 0.6770976400070099    steps: 299    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1293   score: 4.0   memory length: 263375   epsilon: 0.6765155200070225    steps: 294    lr: 4e-05     evaluation reward: 3.88\n",
            "episode: 1294   score: 5.0   memory length: 263682   epsilon: 0.6759076600070357    steps: 307    lr: 4e-05     evaluation reward: 3.87\n",
            "episode: 1295   score: 7.0   memory length: 264127   epsilon: 0.6750265600070549    steps: 445    lr: 4e-05     evaluation reward: 3.9\n",
            "episode: 1296   score: 3.0   memory length: 264373   epsilon: 0.6745394800070654    steps: 246    lr: 4e-05     evaluation reward: 3.87\n",
            "episode: 1297   score: 5.0   memory length: 264723   epsilon: 0.6738464800070805    steps: 350    lr: 4e-05     evaluation reward: 3.91\n",
            "episode: 1298   score: 7.0   memory length: 265147   epsilon: 0.6730069600070987    steps: 424    lr: 4e-05     evaluation reward: 3.96\n",
            "episode: 1299   score: 2.0   memory length: 265348   epsilon: 0.6726089800071073    steps: 201    lr: 4e-05     evaluation reward: 3.95\n",
            "episode: 1300   score: 3.0   memory length: 265595   epsilon: 0.672119920007118    steps: 247    lr: 4e-05     evaluation reward: 3.94\n",
            "episode: 1301   score: 1.0   memory length: 265747   epsilon: 0.6718189600071245    steps: 152    lr: 4e-05     evaluation reward: 3.92\n",
            "episode: 1302   score: 5.0   memory length: 266093   epsilon: 0.6711338800071394    steps: 346    lr: 4e-05     evaluation reward: 3.96\n",
            "episode: 1303   score: 3.0   memory length: 266340   epsilon: 0.67064482000715    steps: 247    lr: 4e-05     evaluation reward: 3.96\n",
            "episode: 1304   score: 8.0   memory length: 266785   epsilon: 0.6697637200071691    steps: 445    lr: 4e-05     evaluation reward: 3.99\n",
            "episode: 1305   score: 2.0   memory length: 266984   epsilon: 0.6693697000071777    steps: 199    lr: 4e-05     evaluation reward: 4.01\n",
            "episode: 1306   score: 6.0   memory length: 267343   epsilon: 0.6686588800071931    steps: 359    lr: 4e-05     evaluation reward: 4.05\n",
            "episode: 1307   score: 7.0   memory length: 267764   epsilon: 0.6678253000072112    steps: 421    lr: 4e-05     evaluation reward: 4.11\n",
            "episode: 1308   score: 3.0   memory length: 267974   epsilon: 0.6674095000072202    steps: 210    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1309   score: 5.0   memory length: 268282   epsilon: 0.6667996600072335    steps: 308    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1310   score: 1.0   memory length: 268454   epsilon: 0.6664591000072408    steps: 172    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1311   score: 5.0   memory length: 268799   epsilon: 0.6657760000072557    steps: 345    lr: 4e-05     evaluation reward: 4.11\n",
            "episode: 1312   score: 2.0   memory length: 268982   epsilon: 0.6654136600072635    steps: 183    lr: 4e-05     evaluation reward: 4.08\n",
            "episode: 1313   score: 3.0   memory length: 269230   epsilon: 0.6649226200072742    steps: 248    lr: 4e-05     evaluation reward: 4.06\n",
            "episode: 1314   score: 7.0   memory length: 269649   epsilon: 0.6640930000072922    steps: 419    lr: 4e-05     evaluation reward: 4.09\n",
            "episode: 1315   score: 6.0   memory length: 269968   epsilon: 0.6634613800073059    steps: 319    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1316   score: 6.0   memory length: 270329   epsilon: 0.6627466000073214    steps: 361    lr: 4e-05     evaluation reward: 4.17\n",
            "episode: 1317   score: 3.0   memory length: 270576   epsilon: 0.6622575400073321    steps: 247    lr: 4e-05     evaluation reward: 4.18\n",
            "episode: 1318   score: 3.0   memory length: 270820   epsilon: 0.6617744200073425    steps: 244    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1319   score: 4.0   memory length: 271097   epsilon: 0.6612259600073545    steps: 277    lr: 4e-05     evaluation reward: 4.14\n",
            "episode: 1320   score: 5.0   memory length: 271402   epsilon: 0.6606220600073676    steps: 305    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1321   score: 3.0   memory length: 271649   epsilon: 0.6601330000073782    steps: 247    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1322   score: 2.0   memory length: 271853   epsilon: 0.659729080007387    steps: 204    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1323   score: 5.0   memory length: 272161   epsilon: 0.6591192400074002    steps: 308    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1324   score: 4.0   memory length: 272437   epsilon: 0.658572760007412    steps: 276    lr: 4e-05     evaluation reward: 4.17\n",
            "episode: 1325   score: 6.0   memory length: 272765   epsilon: 0.6579233200074261    steps: 328    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1326   score: 6.0   memory length: 273162   epsilon: 0.6571372600074432    steps: 397    lr: 4e-05     evaluation reward: 4.23\n",
            "episode: 1327   score: 4.0   memory length: 273457   epsilon: 0.6565531600074559    steps: 295    lr: 4e-05     evaluation reward: 4.21\n",
            "episode: 1328   score: 4.0   memory length: 273750   epsilon: 0.6559730200074685    steps: 293    lr: 4e-05     evaluation reward: 4.22\n",
            "episode: 1329   score: 4.0   memory length: 273994   epsilon: 0.655489900007479    steps: 244    lr: 4e-05     evaluation reward: 4.26\n",
            "episode: 1330   score: 5.0   memory length: 274321   epsilon: 0.654842440007493    steps: 327    lr: 4e-05     evaluation reward: 4.29\n",
            "episode: 1331   score: 0.0   memory length: 274445   epsilon: 0.6545969200074984    steps: 124    lr: 4e-05     evaluation reward: 4.28\n",
            "episode: 1332   score: 7.0   memory length: 274779   epsilon: 0.6539356000075127    steps: 334    lr: 4e-05     evaluation reward: 4.31\n",
            "episode: 1333   score: 3.0   memory length: 274989   epsilon: 0.6535198000075217    steps: 210    lr: 4e-05     evaluation reward: 4.3\n",
            "episode: 1334   score: 1.0   memory length: 275140   epsilon: 0.6532208200075282    steps: 151    lr: 4e-05     evaluation reward: 4.28\n",
            "episode: 1335   score: 3.0   memory length: 275367   epsilon: 0.652771360007538    steps: 227    lr: 4e-05     evaluation reward: 4.23\n",
            "episode: 1336   score: 4.0   memory length: 275641   epsilon: 0.6522288400075498    steps: 274    lr: 4e-05     evaluation reward: 4.24\n",
            "episode: 1337   score: 3.0   memory length: 275886   epsilon: 0.6517437400075603    steps: 245    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1338   score: 8.0   memory length: 276352   epsilon: 0.6508210600075803    steps: 466    lr: 4e-05     evaluation reward: 4.31\n",
            "episode: 1339   score: 3.0   memory length: 276563   epsilon: 0.6504032800075894    steps: 211    lr: 4e-05     evaluation reward: 4.32\n",
            "episode: 1340   score: 5.0   memory length: 276852   epsilon: 0.6498310600076018    steps: 289    lr: 4e-05     evaluation reward: 4.32\n",
            "episode: 1341   score: 3.0   memory length: 277100   epsilon: 0.6493400200076125    steps: 248    lr: 4e-05     evaluation reward: 4.3\n",
            "episode: 1342   score: 3.0   memory length: 277331   epsilon: 0.6488826400076224    steps: 231    lr: 4e-05     evaluation reward: 4.23\n",
            "episode: 1343   score: 0.0   memory length: 277455   epsilon: 0.6486371200076277    steps: 124    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1344   score: 4.0   memory length: 277733   epsilon: 0.6480866800076397    steps: 278    lr: 4e-05     evaluation reward: 4.18\n",
            "episode: 1345   score: 4.0   memory length: 278015   epsilon: 0.6475283200076518    steps: 282    lr: 4e-05     evaluation reward: 4.17\n",
            "episode: 1346   score: 3.0   memory length: 278246   epsilon: 0.6470709400076617    steps: 231    lr: 4e-05     evaluation reward: 4.18\n",
            "episode: 1347   score: 8.0   memory length: 278669   epsilon: 0.6462334000076799    steps: 423    lr: 4e-05     evaluation reward: 4.21\n",
            "episode: 1348   score: 9.0   memory length: 279101   epsilon: 0.6453780400076985    steps: 432    lr: 4e-05     evaluation reward: 4.26\n",
            "episode: 1349   score: 0.0   memory length: 279225   epsilon: 0.6451325200077038    steps: 124    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1350   score: 4.0   memory length: 279484   epsilon: 0.644619700007715    steps: 259    lr: 4e-05     evaluation reward: 4.21\n",
            "episode: 1351   score: 5.0   memory length: 279789   epsilon: 0.6440158000077281    steps: 305    lr: 4e-05     evaluation reward: 4.24\n",
            "episode: 1352   score: 3.0   memory length: 280036   epsilon: 0.6435267400077387    steps: 247    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1353   score: 5.0   memory length: 280330   epsilon: 0.6429446200077513    steps: 294    lr: 4e-05     evaluation reward: 4.26\n",
            "episode: 1354   score: 7.0   memory length: 280746   epsilon: 0.6421209400077692    steps: 416    lr: 4e-05     evaluation reward: 4.3\n",
            "episode: 1355   score: 4.0   memory length: 281022   epsilon: 0.6415744600077811    steps: 276    lr: 4e-05     evaluation reward: 4.27\n",
            "episode: 1356   score: 4.0   memory length: 281303   epsilon: 0.6410180800077931    steps: 281    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1357   score: 3.0   memory length: 281515   epsilon: 0.6405983200078023    steps: 212    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1358   score: 5.0   memory length: 281864   epsilon: 0.6399073000078173    steps: 349    lr: 4e-05     evaluation reward: 4.21\n",
            "episode: 1359   score: 4.0   memory length: 282138   epsilon: 0.639364780007829    steps: 274    lr: 4e-05     evaluation reward: 4.24\n",
            "episode: 1360   score: 4.0   memory length: 282401   epsilon: 0.6388440400078403    steps: 263    lr: 4e-05     evaluation reward: 4.24\n",
            "episode: 1361   score: 4.0   memory length: 282679   epsilon: 0.6382936000078523    steps: 278    lr: 4e-05     evaluation reward: 4.24\n",
            "episode: 1362   score: 3.0   memory length: 282926   epsilon: 0.6378045400078629    steps: 247    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1363   score: 3.0   memory length: 283153   epsilon: 0.6373550800078727    steps: 227    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1364   score: 4.0   memory length: 283429   epsilon: 0.6368086000078845    steps: 276    lr: 4e-05     evaluation reward: 4.18\n",
            "episode: 1365   score: 5.0   memory length: 283779   epsilon: 0.6361156000078996    steps: 350    lr: 4e-05     evaluation reward: 4.17\n",
            "episode: 1366   score: 3.0   memory length: 283993   epsilon: 0.6356918800079088    steps: 214    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1367   score: 3.0   memory length: 284225   epsilon: 0.6352325200079187    steps: 232    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1368   score: 2.0   memory length: 284425   epsilon: 0.6348365200079273    steps: 200    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1369   score: 5.0   memory length: 284741   epsilon: 0.6342108400079409    steps: 316    lr: 4e-05     evaluation reward: 4.14\n",
            "episode: 1370   score: 2.0   memory length: 284940   epsilon: 0.6338168200079495    steps: 199    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1371   score: 3.0   memory length: 285188   epsilon: 0.6333257800079601    steps: 248    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1372   score: 3.0   memory length: 285438   epsilon: 0.6328307800079709    steps: 250    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1373   score: 4.0   memory length: 285694   epsilon: 0.6323239000079819    steps: 256    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1374   score: 6.0   memory length: 286066   epsilon: 0.6315873400079979    steps: 372    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1375   score: 5.0   memory length: 286392   epsilon: 0.6309418600080119    steps: 326    lr: 4e-05     evaluation reward: 4.11\n",
            "episode: 1376   score: 9.0   memory length: 286916   epsilon: 0.6299043400080344    steps: 524    lr: 4e-05     evaluation reward: 4.04\n",
            "episode: 1377   score: 3.0   memory length: 287161   epsilon: 0.629419240008045    steps: 245    lr: 4e-05     evaluation reward: 4.05\n",
            "episode: 1378   score: 3.0   memory length: 287390   epsilon: 0.6289658200080548    steps: 229    lr: 4e-05     evaluation reward: 4.07\n",
            "episode: 1379   score: 3.0   memory length: 287635   epsilon: 0.6284807200080653    steps: 245    lr: 4e-05     evaluation reward: 4.06\n",
            "episode: 1380   score: 2.0   memory length: 287834   epsilon: 0.6280867000080739    steps: 199    lr: 4e-05     evaluation reward: 4.06\n",
            "episode: 1381   score: 5.0   memory length: 288144   epsilon: 0.6274729000080872    steps: 310    lr: 4e-05     evaluation reward: 4.06\n",
            "episode: 1382   score: 4.0   memory length: 288403   epsilon: 0.6269600800080983    steps: 259    lr: 4e-05     evaluation reward: 4.09\n",
            "episode: 1383   score: 3.0   memory length: 288617   epsilon: 0.6265363600081075    steps: 214    lr: 4e-05     evaluation reward: 4.11\n",
            "episode: 1384   score: 2.0   memory length: 288836   epsilon: 0.6261027400081169    steps: 219    lr: 4e-05     evaluation reward: 4.08\n",
            "episode: 1385   score: 9.0   memory length: 289209   epsilon: 0.625364200008133    steps: 373    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1386   score: 3.0   memory length: 289438   epsilon: 0.6249107800081428    steps: 229    lr: 4e-05     evaluation reward: 4.09\n",
            "episode: 1387   score: 7.0   memory length: 289831   epsilon: 0.6241326400081597    steps: 393    lr: 4e-05     evaluation reward: 4.11\n",
            "episode: 1388   score: 3.0   memory length: 290080   epsilon: 0.6236396200081704    steps: 249    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1389   score: 2.0   memory length: 290279   epsilon: 0.623245600008179    steps: 199    lr: 4e-05     evaluation reward: 4.09\n",
            "episode: 1390   score: 3.0   memory length: 290527   epsilon: 0.6227545600081896    steps: 248    lr: 4e-05     evaluation reward: 4.07\n",
            "episode: 1391   score: 5.0   memory length: 290874   epsilon: 0.6220675000082045    steps: 347    lr: 4e-05     evaluation reward: 4.08\n",
            "episode: 1392   score: 3.0   memory length: 291123   epsilon: 0.6215744800082152    steps: 249    lr: 4e-05     evaluation reward: 4.07\n",
            "episode: 1393   score: 7.0   memory length: 291461   epsilon: 0.6209052400082298    steps: 338    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1394   score: 6.0   memory length: 291819   epsilon: 0.6201964000082452    steps: 358    lr: 4e-05     evaluation reward: 4.11\n",
            "episode: 1395   score: 1.0   memory length: 291989   epsilon: 0.6198598000082525    steps: 170    lr: 4e-05     evaluation reward: 4.05\n",
            "episode: 1396   score: 8.0   memory length: 292436   epsilon: 0.6189747400082717    steps: 447    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1397   score: 5.0   memory length: 292783   epsilon: 0.6182876800082866    steps: 347    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1398   score: 1.0   memory length: 292935   epsilon: 0.6179867200082931    steps: 152    lr: 4e-05     evaluation reward: 4.04\n",
            "episode: 1399   score: 5.0   memory length: 293258   epsilon: 0.617347180008307    steps: 323    lr: 4e-05     evaluation reward: 4.07\n",
            "episode: 1400   score: 1.0   memory length: 293410   epsilon: 0.6170462200083136    steps: 152    lr: 4e-05     evaluation reward: 4.05\n",
            "episode: 1401   score: 6.0   memory length: 293772   epsilon: 0.6163294600083291    steps: 362    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1402   score: 7.0   memory length: 294197   epsilon: 0.6154879600083474    steps: 425    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1403   score: 6.0   memory length: 294594   epsilon: 0.6147019000083644    steps: 397    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1404   score: 6.0   memory length: 294931   epsilon: 0.6140346400083789    steps: 337    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1405   score: 9.0   memory length: 295471   epsilon: 0.6129654400084021    steps: 540    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1406   score: 6.0   memory length: 295852   epsilon: 0.6122110600084185    steps: 381    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1407   score: 10.0   memory length: 296243   epsilon: 0.6114368800084353    steps: 391    lr: 4e-05     evaluation reward: 4.23\n",
            "episode: 1408   score: 5.0   memory length: 296572   epsilon: 0.6107854600084495    steps: 329    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1409   score: 5.0   memory length: 296878   epsilon: 0.6101795800084626    steps: 306    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1410   score: 3.0   memory length: 297108   epsilon: 0.6097241800084725    steps: 230    lr: 4e-05     evaluation reward: 4.27\n",
            "episode: 1411   score: 4.0   memory length: 297404   epsilon: 0.6091381000084852    steps: 296    lr: 4e-05     evaluation reward: 4.26\n",
            "episode: 1412   score: 5.0   memory length: 297710   epsilon: 0.6085322200084984    steps: 306    lr: 4e-05     evaluation reward: 4.29\n",
            "episode: 1413   score: 5.0   memory length: 298020   epsilon: 0.6079184200085117    steps: 310    lr: 4e-05     evaluation reward: 4.31\n",
            "episode: 1414   score: 5.0   memory length: 298349   epsilon: 0.6072670000085258    steps: 329    lr: 4e-05     evaluation reward: 4.29\n",
            "episode: 1415   score: 3.0   memory length: 298560   epsilon: 0.6068492200085349    steps: 211    lr: 4e-05     evaluation reward: 4.26\n",
            "episode: 1416   score: 5.0   memory length: 298885   epsilon: 0.6062057200085489    steps: 325    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1417   score: 6.0   memory length: 299242   epsilon: 0.6054988600085642    steps: 357    lr: 4e-05     evaluation reward: 4.28\n",
            "episode: 1418   score: 6.0   memory length: 299615   epsilon: 0.6047603200085803    steps: 373    lr: 4e-05     evaluation reward: 4.31\n",
            "episode: 1419   score: 4.0   memory length: 299896   epsilon: 0.6042039400085923    steps: 281    lr: 4e-05     evaluation reward: 4.31\n",
            "episode: 1420   score: 6.0   memory length: 300256   epsilon: 0.6034911400086078    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
            "episode: 1421   score: 5.0   memory length: 300584   epsilon: 0.6028417000086219    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
            "episode: 1422   score: 3.0   memory length: 300795   epsilon: 0.602423920008631    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.35\n",
            "episode: 1423   score: 2.0   memory length: 300996   epsilon: 0.6020259400086396    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
            "episode: 1424   score: 7.0   memory length: 301399   epsilon: 0.601228000008657    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 4.35\n",
            "episode: 1425   score: 6.0   memory length: 301790   epsilon: 0.6004538200086738    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 4.35\n",
            "episode: 1426   score: 5.0   memory length: 302096   epsilon: 0.5998479400086869    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
            "episode: 1427   score: 11.0   memory length: 302543   epsilon: 0.5989628800087061    steps: 447    lr: 1.6000000000000003e-05     evaluation reward: 4.41\n",
            "episode: 1428   score: 2.0   memory length: 302744   epsilon: 0.5985649000087148    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
            "episode: 1429   score: 6.0   memory length: 303137   epsilon: 0.5977867600087317    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 4.41\n",
            "episode: 1430   score: 4.0   memory length: 303415   epsilon: 0.5972363200087436    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
            "episode: 1431   score: 3.0   memory length: 303644   epsilon: 0.5967829000087534    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.43\n",
            "episode: 1432   score: 5.0   memory length: 303917   epsilon: 0.5962423600087652    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 4.41\n",
            "episode: 1433   score: 1.0   memory length: 304069   epsilon: 0.5959414000087717    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
            "episode: 1434   score: 2.0   memory length: 304269   epsilon: 0.5955454000087803    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
            "episode: 1435   score: 14.0   memory length: 304767   epsilon: 0.5945593600088017    steps: 498    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
            "episode: 1436   score: 5.0   memory length: 305049   epsilon: 0.5940010000088138    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
            "episode: 1437   score: 4.0   memory length: 305327   epsilon: 0.5934505600088258    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.53\n",
            "episode: 1438   score: 5.0   memory length: 305618   epsilon: 0.5928743800088383    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
            "episode: 1439   score: 3.0   memory length: 305832   epsilon: 0.5924506600088475    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
            "episode: 1440   score: 6.0   memory length: 306187   epsilon: 0.5917477600088628    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
            "episode: 1441   score: 3.0   memory length: 306401   epsilon: 0.591324040008872    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
            "episode: 1442   score: 5.0   memory length: 306692   epsilon: 0.5907478600088845    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 4.53\n",
            "episode: 1443   score: 3.0   memory length: 306922   epsilon: 0.5902924600088943    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
            "episode: 1444   score: 4.0   memory length: 307197   epsilon: 0.5897479600089062    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
            "episode: 1445   score: 4.0   memory length: 307496   epsilon: 0.589155940008919    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
            "episode: 1446   score: 6.0   memory length: 307873   epsilon: 0.5884094800089352    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
            "episode: 1447   score: 5.0   memory length: 308198   epsilon: 0.5877659800089492    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
            "episode: 1448   score: 4.0   memory length: 308457   epsilon: 0.5872531600089603    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
            "episode: 1449   score: 3.0   memory length: 308689   epsilon: 0.5867938000089703    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 4.54\n",
            "episode: 1450   score: 6.0   memory length: 309010   epsilon: 0.5861582200089841    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
            "episode: 1451   score: 6.0   memory length: 309384   epsilon: 0.5854177000090002    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 4.57\n",
            "episode: 1452   score: 1.0   memory length: 309536   epsilon: 0.5851167400090067    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
            "episode: 1453   score: 10.0   memory length: 310100   epsilon: 0.584000020009031    steps: 564    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
            "episode: 1454   score: 5.0   memory length: 310422   epsilon: 0.5833624600090448    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
            "episode: 1455   score: 4.0   memory length: 310737   epsilon: 0.5827387600090583    steps: 315    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
            "episode: 1456   score: 6.0   memory length: 311134   epsilon: 0.5819527000090754    steps: 397    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
            "episode: 1457   score: 3.0   memory length: 311402   epsilon: 0.5814220600090869    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
            "episode: 1458   score: 3.0   memory length: 311629   epsilon: 0.5809726000090967    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
            "episode: 1459   score: 5.0   memory length: 311972   epsilon: 0.5802934600091114    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
            "episode: 1460   score: 4.0   memory length: 312286   epsilon: 0.5796717400091249    steps: 314    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
            "episode: 1461   score: 7.0   memory length: 312707   epsilon: 0.578838160009143    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
            "episode: 1462   score: 8.0   memory length: 313144   epsilon: 0.5779729000091618    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
            "episode: 1463   score: 6.0   memory length: 313506   epsilon: 0.5772561400091774    steps: 362    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
            "episode: 1464   score: 5.0   memory length: 313855   epsilon: 0.5765651200091924    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
            "episode: 1465   score: 1.0   memory length: 314007   epsilon: 0.5762641600091989    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
            "episode: 1466   score: 5.0   memory length: 314325   epsilon: 0.5756345200092126    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1467   score: 2.0   memory length: 314548   epsilon: 0.5751929800092221    steps: 223    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
            "episode: 1468   score: 3.0   memory length: 314777   epsilon: 0.574739560009232    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1469   score: 2.0   memory length: 314976   epsilon: 0.5743455400092405    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1470   score: 5.0   memory length: 315300   epsilon: 0.5737040200092545    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1471   score: 2.0   memory length: 315483   epsilon: 0.5733416800092623    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
            "episode: 1472   score: 5.0   memory length: 315797   epsilon: 0.5727199600092758    steps: 314    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
            "episode: 1473   score: 6.0   memory length: 316152   epsilon: 0.5720170600092911    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
            "episode: 1474   score: 6.0   memory length: 316487   epsilon: 0.5713537600093055    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
            "episode: 1475   score: 4.0   memory length: 316746   epsilon: 0.5708409400093166    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
            "episode: 1476   score: 2.0   memory length: 316929   epsilon: 0.5704786000093245    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1477   score: 4.0   memory length: 317205   epsilon: 0.5699321200093364    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1478   score: 4.0   memory length: 317464   epsilon: 0.5694193000093475    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1479   score: 4.0   memory length: 317742   epsilon: 0.5688688600093594    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
            "episode: 1480   score: 4.0   memory length: 318021   epsilon: 0.5683164400093714    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1481   score: 4.0   memory length: 318320   epsilon: 0.5677244200093843    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
            "episode: 1482   score: 3.0   memory length: 318549   epsilon: 0.5672710000093941    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
            "episode: 1483   score: 8.0   memory length: 318992   epsilon: 0.5663938600094132    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
            "episode: 1484   score: 6.0   memory length: 319341   epsilon: 0.5657028400094282    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
            "episode: 1485   score: 2.0   memory length: 319540   epsilon: 0.5653088200094367    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1486   score: 3.0   memory length: 319752   epsilon: 0.5648890600094458    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1487   score: 2.0   memory length: 319971   epsilon: 0.5644554400094552    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1488   score: 3.0   memory length: 320220   epsilon: 0.563962420009466    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1489   score: 2.0   memory length: 320439   epsilon: 0.5635288000094754    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1490   score: 5.0   memory length: 320733   epsilon: 0.562946680009488    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1491   score: 6.0   memory length: 321086   epsilon: 0.5622477400095032    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
            "episode: 1492   score: 3.0   memory length: 321314   epsilon: 0.561796300009513    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
            "episode: 1493   score: 5.0   memory length: 321603   epsilon: 0.5612240800095254    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1494   score: 4.0   memory length: 321877   epsilon: 0.5606815600095372    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
            "episode: 1495   score: 4.0   memory length: 322118   epsilon: 0.5602043800095475    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1496   score: 3.0   memory length: 322332   epsilon: 0.5597806600095567    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
            "episode: 1497   score: 8.0   memory length: 322772   epsilon: 0.5589094600095756    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1498   score: 5.0   memory length: 323075   epsilon: 0.5583095200095887    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
            "episode: 1499   score: 6.0   memory length: 323398   epsilon: 0.5576699800096026    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1500   score: 1.0   memory length: 323550   epsilon: 0.5573690200096091    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1501   score: 3.0   memory length: 323818   epsilon: 0.5568383800096206    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1502   score: 3.0   memory length: 324048   epsilon: 0.5563829800096305    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
            "episode: 1503   score: 3.0   memory length: 324261   epsilon: 0.5559612400096396    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
            "episode: 1504   score: 4.0   memory length: 324537   epsilon: 0.5554147600096515    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.57\n",
            "episode: 1505   score: 4.0   memory length: 324813   epsilon: 0.5548682800096634    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
            "episode: 1506   score: 9.0   memory length: 325320   epsilon: 0.5538644200096852    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
            "episode: 1507   score: 2.0   memory length: 325519   epsilon: 0.5534704000096937    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
            "episode: 1508   score: 5.0   memory length: 325824   epsilon: 0.5528665000097068    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
            "episode: 1509   score: 7.0   memory length: 326234   epsilon: 0.5520547000097245    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
            "episode: 1510   score: 9.0   memory length: 326752   epsilon: 0.5510290600097467    steps: 518    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
            "episode: 1511   score: 7.0   memory length: 327142   epsilon: 0.5502568600097635    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
            "episode: 1512   score: 3.0   memory length: 327393   epsilon: 0.5497598800097743    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
            "episode: 1513   score: 7.0   memory length: 327784   epsilon: 0.5489857000097911    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
            "episode: 1514   score: 3.0   memory length: 327996   epsilon: 0.5485659400098002    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
            "episode: 1515   score: 6.0   memory length: 328351   epsilon: 0.5478630400098154    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
            "episode: 1516   score: 6.0   memory length: 328691   epsilon: 0.5471898400098301    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
            "episode: 1517   score: 7.0   memory length: 329120   epsilon: 0.5463404200098485    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
            "episode: 1518   score: 5.0   memory length: 329434   epsilon: 0.545718700009862    steps: 314    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
            "episode: 1519   score: 3.0   memory length: 329647   epsilon: 0.5452969600098712    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
            "episode: 1520   score: 11.0   memory length: 330207   epsilon: 0.5441881600098952    steps: 560    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1521   score: 4.0   memory length: 330505   epsilon: 0.543598120009908    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
            "episode: 1522   score: 5.0   memory length: 330847   epsilon: 0.5429209600099227    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1523   score: 3.0   memory length: 331094   epsilon: 0.5424319000099334    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1524   score: 4.0   memory length: 331355   epsilon: 0.5419151200099446    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
            "episode: 1525   score: 4.0   memory length: 331597   epsilon: 0.541435960009955    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
            "episode: 1526   score: 8.0   memory length: 331909   epsilon: 0.5408182000099684    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1527   score: 6.0   memory length: 332265   epsilon: 0.5401133200099837    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
            "episode: 1528   score: 5.0   memory length: 332596   epsilon: 0.5394579400099979    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
            "episode: 1529   score: 9.0   memory length: 333070   epsilon: 0.5385194200100183    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1530   score: 4.0   memory length: 333331   epsilon: 0.5380026400100295    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1531   score: 3.0   memory length: 333545   epsilon: 0.5375789200100387    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1532   score: 2.0   memory length: 333746   epsilon: 0.5371809400100473    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
            "episode: 1533   score: 8.0   memory length: 334129   epsilon: 0.5364226000100638    steps: 383    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1534   score: 7.0   memory length: 334534   epsilon: 0.5356207000100812    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
            "episode: 1535   score: 6.0   memory length: 334875   epsilon: 0.5349455200100959    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1536   score: 3.0   memory length: 335122   epsilon: 0.5344564600101065    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1537   score: 6.0   memory length: 335498   epsilon: 0.5337119800101227    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1538   score: 5.0   memory length: 335827   epsilon: 0.5330605600101368    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1539   score: 3.0   memory length: 336073   epsilon: 0.5325734800101474    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1540   score: 2.0   memory length: 336271   epsilon: 0.5321814400101559    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
            "episode: 1541   score: 2.0   memory length: 336470   epsilon: 0.5317874200101644    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
            "episode: 1542   score: 5.0   memory length: 336816   epsilon: 0.5311023400101793    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
            "episode: 1543   score: 5.0   memory length: 337142   epsilon: 0.5304568600101933    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
            "episode: 1544   score: 6.0   memory length: 337485   epsilon: 0.5297777200102081    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1545   score: 3.0   memory length: 337717   epsilon: 0.529318360010218    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1546   score: 7.0   memory length: 337964   epsilon: 0.5288293000102287    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1547   score: 4.0   memory length: 338258   epsilon: 0.5282471800102413    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1548   score: 5.0   memory length: 338603   epsilon: 0.5275640800102561    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
            "episode: 1549   score: 4.0   memory length: 338882   epsilon: 0.5270116600102681    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1550   score: 6.0   memory length: 339239   epsilon: 0.5263048000102835    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1551   score: 4.0   memory length: 339498   epsilon: 0.5257919800102946    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
            "episode: 1552   score: 6.0   memory length: 339833   epsilon: 0.525128680010309    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
            "episode: 1553   score: 3.0   memory length: 340082   epsilon: 0.5246356600103197    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
            "episode: 1554   score: 9.0   memory length: 340522   epsilon: 0.5237644600103386    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1555   score: 4.0   memory length: 340799   epsilon: 0.5232160000103505    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
            "episode: 1556   score: 7.0   memory length: 341166   epsilon: 0.5224893400103663    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
            "episode: 1557   score: 4.0   memory length: 341429   epsilon: 0.5219686000103776    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
            "episode: 1558   score: 5.0   memory length: 341711   epsilon: 0.5214102400103897    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
            "episode: 1559   score: 7.0   memory length: 342080   epsilon: 0.5206796200104056    steps: 369    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
            "episode: 1560   score: 7.0   memory length: 342416   epsilon: 0.52001434001042    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
            "episode: 1561   score: 5.0   memory length: 342708   epsilon: 0.5194361800104326    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
            "episode: 1562   score: 7.0   memory length: 343149   epsilon: 0.5185630000104515    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
            "episode: 1563   score: 5.0   memory length: 343421   epsilon: 0.5180244400104632    steps: 272    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
            "episode: 1564   score: 5.0   memory length: 343767   epsilon: 0.5173393600104781    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
            "episode: 1565   score: 5.0   memory length: 344076   epsilon: 0.5167275400104914    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
            "episode: 1566   score: 4.0   memory length: 344354   epsilon: 0.5161771000105033    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
            "episode: 1567   score: 4.0   memory length: 344629   epsilon: 0.5156326000105151    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
            "episode: 1568   score: 7.0   memory length: 345057   epsilon: 0.5147851600105335    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
            "episode: 1569   score: 3.0   memory length: 345285   epsilon: 0.5143337200105433    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
            "episode: 1570   score: 5.0   memory length: 345570   epsilon: 0.5137694200105556    steps: 285    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
            "episode: 1571   score: 2.0   memory length: 345753   epsilon: 0.5134070800105635    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
            "episode: 1572   score: 6.0   memory length: 346085   epsilon: 0.5127497200105777    steps: 332    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
            "episode: 1573   score: 9.0   memory length: 346532   epsilon: 0.5118646600105969    steps: 447    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
            "episode: 1574   score: 5.0   memory length: 346826   epsilon: 0.5112825400106096    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
            "episode: 1575   score: 7.0   memory length: 347206   epsilon: 0.5105301400106259    steps: 380    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
            "episode: 1576   score: 10.0   memory length: 347573   epsilon: 0.5098034800106417    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
            "episode: 1577   score: 3.0   memory length: 347822   epsilon: 0.5093104600106524    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
            "episode: 1578   score: 4.0   memory length: 348065   epsilon: 0.5088293200106628    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
            "episode: 1579   score: 7.0   memory length: 348461   epsilon: 0.5080452400106799    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 4.97\n",
            "episode: 1580   score: 7.0   memory length: 348873   epsilon: 0.5072294800106976    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
            "episode: 1581   score: 4.0   memory length: 349134   epsilon: 0.5067127000107088    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
            "episode: 1582   score: 2.0   memory length: 349333   epsilon: 0.5063186800107173    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
            "episode: 1583   score: 3.0   memory length: 349547   epsilon: 0.5058949600107265    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
            "episode: 1584   score: 4.0   memory length: 349789   epsilon: 0.5054158000107369    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
            "episode: 1585   score: 5.0   memory length: 350134   epsilon: 0.5047327000107518    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
            "episode: 1586   score: 4.0   memory length: 350427   epsilon: 0.5041525600107644    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 4.96\n",
            "episode: 1587   score: 9.0   memory length: 350928   epsilon: 0.5031605800107859    steps: 501    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
            "episode: 1588   score: 5.0   memory length: 351239   epsilon: 0.5025448000107993    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
            "episode: 1589   score: 8.0   memory length: 351642   epsilon: 0.5017468600108166    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
            "episode: 1590   score: 8.0   memory length: 352098   epsilon: 0.5008439800108362    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
            "episode: 1591   score: 4.0   memory length: 352376   epsilon: 0.5002935400108481    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
            "episode: 1592   score: 7.0   memory length: 352802   epsilon: 0.499450060010851    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
            "episode: 1593   score: 3.0   memory length: 353033   epsilon: 0.4989926800108481    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
            "episode: 1594   score: 8.0   memory length: 353442   epsilon: 0.498182860010843    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
            "episode: 1595   score: 3.0   memory length: 353654   epsilon: 0.49776310001084034    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
            "episode: 1596   score: 3.0   memory length: 353901   epsilon: 0.49727404001083725    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
            "episode: 1597   score: 4.0   memory length: 354161   epsilon: 0.496759240010834    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.13\n",
            "episode: 1598   score: 6.0   memory length: 354506   epsilon: 0.49607614001082967    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
            "episode: 1599   score: 3.0   memory length: 354753   epsilon: 0.4955870800108266    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
            "episode: 1600   score: 3.0   memory length: 354965   epsilon: 0.4951673200108239    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.13\n",
            "episode: 1601   score: 4.0   memory length: 355226   epsilon: 0.49465054001082065    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
            "episode: 1602   score: 4.0   memory length: 355504   epsilon: 0.49410010001081717    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.15\n",
            "episode: 1603   score: 6.0   memory length: 355875   epsilon: 0.4933655200108125    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
            "episode: 1604   score: 5.0   memory length: 356182   epsilon: 0.4927576600108087    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
            "episode: 1605   score: 4.0   memory length: 356445   epsilon: 0.4922369200108054    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
            "episode: 1606   score: 9.0   memory length: 356903   epsilon: 0.49133008001079964    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
            "episode: 1607   score: 7.0   memory length: 357311   epsilon: 0.49052224001079453    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 5.24\n",
            "episode: 1608   score: 7.0   memory length: 357739   epsilon: 0.48967480001078917    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1609   score: 8.0   memory length: 358194   epsilon: 0.48877390001078347    steps: 455    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
            "episode: 1610   score: 8.0   memory length: 358637   epsilon: 0.4878967600107779    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1611   score: 10.0   memory length: 359120   epsilon: 0.48694042001077187    steps: 483    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1612   score: 4.0   memory length: 359400   epsilon: 0.48638602001076836    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
            "episode: 1613   score: 3.0   memory length: 359614   epsilon: 0.4859623000107657    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1614   score: 6.0   memory length: 359972   epsilon: 0.4852534600107612    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1615   score: 5.0   memory length: 360246   epsilon: 0.48471094001075776    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
            "episode: 1616   score: 6.0   memory length: 360595   epsilon: 0.4840199200107534    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
            "episode: 1617   score: 6.0   memory length: 360934   epsilon: 0.48334870001074914    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
            "episode: 1618   score: 4.0   memory length: 361232   epsilon: 0.4827586600107454    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1619   score: 7.0   memory length: 361625   epsilon: 0.4819805200107405    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
            "episode: 1620   score: 1.0   memory length: 361776   epsilon: 0.4816815400107386    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
            "episode: 1621   score: 4.0   memory length: 362037   epsilon: 0.4811647600107353    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
            "episode: 1622   score: 8.0   memory length: 362514   epsilon: 0.48022030001072935    steps: 477    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
            "episode: 1623   score: 3.0   memory length: 362728   epsilon: 0.47979658001072667    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
            "episode: 1624   score: 7.0   memory length: 363132   epsilon: 0.4789966600107216    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1625   score: 4.0   memory length: 363410   epsilon: 0.4784462200107181    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1626   score: 4.0   memory length: 363671   epsilon: 0.47792944001071486    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
            "episode: 1627   score: 13.0   memory length: 364183   epsilon: 0.47691568001070844    steps: 512    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1628   score: 3.0   memory length: 364433   epsilon: 0.4764206800107053    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
            "episode: 1629   score: 4.0   memory length: 364712   epsilon: 0.4758682600107018    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
            "episode: 1630   score: 5.0   memory length: 365056   epsilon: 0.4751871400106975    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
            "episode: 1631   score: 10.0   memory length: 365626   epsilon: 0.47405854001069037    steps: 570    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
            "episode: 1632   score: 3.0   memory length: 365853   epsilon: 0.4736090800106875    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
            "episode: 1633   score: 7.0   memory length: 366242   epsilon: 0.47283886001068265    steps: 389    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
            "episode: 1634   score: 6.0   memory length: 366580   epsilon: 0.4721696200106784    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1635   score: 4.0   memory length: 366854   epsilon: 0.471627100010675    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
            "episode: 1636   score: 5.0   memory length: 367177   epsilon: 0.47098756001067094    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1637   score: 10.0   memory length: 367592   epsilon: 0.47016586001066574    steps: 415    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
            "episode: 1638   score: 3.0   memory length: 367804   epsilon: 0.4697461000106631    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
            "episode: 1639   score: 9.0   memory length: 368288   epsilon: 0.468787780010657    steps: 484    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
            "episode: 1640   score: 5.0   memory length: 368598   epsilon: 0.46817398001065313    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
            "episode: 1641   score: 5.0   memory length: 368941   epsilon: 0.46749484001064884    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
            "episode: 1642   score: 6.0   memory length: 369286   epsilon: 0.4668117400106445    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
            "episode: 1643   score: 6.0   memory length: 369647   epsilon: 0.46609696001064    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
            "episode: 1644   score: 5.0   memory length: 369923   epsilon: 0.46555048001063654    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
            "episode: 1645   score: 6.0   memory length: 370278   epsilon: 0.4648475800106321    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
            "episode: 1646   score: 7.0   memory length: 370652   epsilon: 0.4641070600106274    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
            "episode: 1647   score: 3.0   memory length: 370864   epsilon: 0.46368730001062475    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
            "episode: 1648   score: 6.0   memory length: 371215   epsilon: 0.46299232001062035    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
            "episode: 1649   score: 4.0   memory length: 371497   epsilon: 0.4624339600106168    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
            "episode: 1650   score: 11.0   memory length: 372063   epsilon: 0.4613132800106097    steps: 566    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
            "episode: 1651   score: 12.0   memory length: 372610   epsilon: 0.4602302200106029    steps: 547    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
            "episode: 1652   score: 12.0   memory length: 373064   epsilon: 0.4593313000105972    steps: 454    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1653   score: 3.0   memory length: 373313   epsilon: 0.45883828001059407    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1654   score: 8.0   memory length: 373771   epsilon: 0.45793144001058833    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1655   score: 8.0   memory length: 374219   epsilon: 0.4570444000105827    steps: 448    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
            "episode: 1656   score: 5.0   memory length: 374527   epsilon: 0.45643456001057886    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
            "episode: 1657   score: 4.0   memory length: 374787   epsilon: 0.4559197600105756    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
            "episode: 1658   score: 3.0   memory length: 374999   epsilon: 0.45550000001057295    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1659   score: 7.0   memory length: 375367   epsilon: 0.45477136001056834    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1660   score: 6.0   memory length: 375698   epsilon: 0.4541159800105642    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
            "episode: 1661   score: 3.0   memory length: 375910   epsilon: 0.45369622001056154    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
            "episode: 1662   score: 6.0   memory length: 376250   epsilon: 0.4530230200105573    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
            "episode: 1663   score: 9.0   memory length: 376753   epsilon: 0.452027080010551    steps: 503    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1664   score: 3.0   memory length: 376981   epsilon: 0.4515756400105481    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
            "episode: 1665   score: 4.0   memory length: 377240   epsilon: 0.4510628200105449    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
            "episode: 1666   score: 3.0   memory length: 377451   epsilon: 0.45064504001054223    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
            "episode: 1667   score: 4.0   memory length: 377728   epsilon: 0.45009658001053876    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
            "episode: 1668   score: 9.0   memory length: 378182   epsilon: 0.4491976600105331    steps: 454    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
            "episode: 1669   score: 6.0   memory length: 378547   epsilon: 0.4484749600105285    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1670   score: 4.0   memory length: 378809   epsilon: 0.4479562000105252    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1671   score: 8.0   memory length: 379283   epsilon: 0.4470176800105193    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 5.71\n",
            "episode: 1672   score: 7.0   memory length: 379682   epsilon: 0.4462276600105143    steps: 399    lr: 1.6000000000000003e-05     evaluation reward: 5.72\n",
            "episode: 1673   score: 6.0   memory length: 380038   epsilon: 0.4455227800105098    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
            "episode: 1674   score: 5.0   memory length: 380369   epsilon: 0.4448674000105057    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
            "episode: 1675   score: 6.0   memory length: 380747   epsilon: 0.44411896001050094    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
            "episode: 1676   score: 7.0   memory length: 381091   epsilon: 0.44343784001049663    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1677   score: 6.0   memory length: 381452   epsilon: 0.4427230600104921    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
            "episode: 1678   score: 3.0   memory length: 381698   epsilon: 0.442235980010489    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
            "episode: 1679   score: 7.0   memory length: 382068   epsilon: 0.4415033800104844    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
            "episode: 1680   score: 3.0   memory length: 382282   epsilon: 0.4410796600104817    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
            "episode: 1681   score: 2.0   memory length: 382465   epsilon: 0.4407173200104794    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
            "episode: 1682   score: 4.0   memory length: 382730   epsilon: 0.4401926200104761    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
            "episode: 1683   score: 5.0   memory length: 383061   epsilon: 0.43953724001047195    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1684   score: 5.0   memory length: 383391   epsilon: 0.4388838400104678    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1685   score: 5.0   memory length: 383718   epsilon: 0.4382363800104637    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1686   score: 4.0   memory length: 383981   epsilon: 0.4377156400104604    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1687   score: 9.0   memory length: 384428   epsilon: 0.4368305800104548    steps: 447    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1688   score: 3.0   memory length: 384642   epsilon: 0.43640686001045215    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
            "episode: 1689   score: 3.0   memory length: 384856   epsilon: 0.43598314001044947    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 5.59\n",
            "episode: 1690   score: 7.0   memory length: 385232   epsilon: 0.43523866001044476    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
            "episode: 1691   score: 6.0   memory length: 385577   epsilon: 0.43455556001044043    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
            "episode: 1692   score: 10.0   memory length: 385988   epsilon: 0.4337417800104353    steps: 411    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
            "episode: 1693   score: 4.0   memory length: 386269   epsilon: 0.43318540001043176    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
            "episode: 1694   score: 6.0   memory length: 386626   epsilon: 0.4324785400104273    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
            "episode: 1695   score: 7.0   memory length: 386984   epsilon: 0.4317697000104228    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
            "episode: 1696   score: 11.0   memory length: 387404   epsilon: 0.43093810001041755    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 5.74\n",
            "episode: 1697   score: 7.0   memory length: 387770   epsilon: 0.43021342001041296    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
            "episode: 1698   score: 3.0   memory length: 388002   epsilon: 0.42975406001041005    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 5.74\n",
            "episode: 1699   score: 6.0   memory length: 388314   epsilon: 0.42913630001040615    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
            "episode: 1700   score: 4.0   memory length: 388589   epsilon: 0.4285918000104027    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.78\n",
            "episode: 1701   score: 8.0   memory length: 389017   epsilon: 0.42774436001039734    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
            "episode: 1702   score: 6.0   memory length: 389392   epsilon: 0.42700186001039264    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
            "episode: 1703   score: 8.0   memory length: 389832   epsilon: 0.42613066001038713    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
            "episode: 1704   score: 5.0   memory length: 390163   epsilon: 0.425475280010383    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
            "episode: 1705   score: 6.0   memory length: 390541   epsilon: 0.42472684001037825    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
            "episode: 1706   score: 8.0   memory length: 390941   epsilon: 0.42393484001037324    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
            "episode: 1707   score: 1.0   memory length: 391093   epsilon: 0.42363388001037133    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
            "episode: 1708   score: 3.0   memory length: 391305   epsilon: 0.4232141200103687    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
            "episode: 1709   score: 6.0   memory length: 391654   epsilon: 0.4225231000103643    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 5.75\n",
            "episode: 1710   score: 6.0   memory length: 392032   epsilon: 0.42177466001035957    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
            "episode: 1711   score: 6.0   memory length: 392374   epsilon: 0.4210975000103553    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
            "episode: 1712   score: 4.0   memory length: 392672   epsilon: 0.42050746001035155    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
            "episode: 1713   score: 2.0   memory length: 392855   epsilon: 0.42014512001034926    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
            "episode: 1714   score: 6.0   memory length: 393178   epsilon: 0.4195055800103452    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
            "episode: 1715   score: 6.0   memory length: 393556   epsilon: 0.4187571400103405    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
            "episode: 1716   score: 9.0   memory length: 394048   epsilon: 0.4177829800103343    steps: 492    lr: 1.6000000000000003e-05     evaluation reward: 5.72\n",
            "episode: 1717   score: 4.0   memory length: 394324   epsilon: 0.41723650001033086    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.7\n",
            "episode: 1718   score: 8.0   memory length: 394601   epsilon: 0.4166880400103274    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 5.74\n",
            "episode: 1719   score: 8.0   memory length: 395026   epsilon: 0.41584654001032206    steps: 425    lr: 1.6000000000000003e-05     evaluation reward: 5.75\n",
            "episode: 1720   score: 10.0   memory length: 395572   epsilon: 0.4147654600103152    steps: 546    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
            "episode: 1721   score: 7.0   memory length: 395946   epsilon: 0.41402494001031054    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
            "episode: 1722   score: 3.0   memory length: 396180   epsilon: 0.4135616200103076    steps: 234    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
            "episode: 1723   score: 7.0   memory length: 396587   epsilon: 0.4127557600103025    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
            "episode: 1724   score: 5.0   memory length: 396875   epsilon: 0.4121855200102989    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
            "episode: 1725   score: 5.0   memory length: 397202   epsilon: 0.4115380600102948    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
            "episode: 1726   score: 4.0   memory length: 397445   epsilon: 0.41105692001029176    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
            "episode: 1727   score: 4.0   memory length: 397742   epsilon: 0.41046886001028804    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 5.76\n",
            "episode: 1728   score: 3.0   memory length: 397954   epsilon: 0.4100491000102854    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.76\n",
            "episode: 1729   score: 7.0   memory length: 398331   epsilon: 0.40930264001028066    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
            "episode: 1730   score: 4.0   memory length: 398572   epsilon: 0.40882546001027764    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 5.78\n",
            "episode: 1731   score: 6.0   memory length: 398910   epsilon: 0.4081562200102734    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 5.74\n",
            "episode: 1732   score: 8.0   memory length: 399366   epsilon: 0.4072533400102677    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
            "episode: 1733   score: 9.0   memory length: 399726   epsilon: 0.4065405400102632    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
            "episode: 1734   score: 8.0   memory length: 400162   epsilon: 0.4056772600102577    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
            "episode: 1735   score: 6.0   memory length: 400554   epsilon: 0.4049011000102528    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
            "episode: 1736   score: 3.0   memory length: 400765   epsilon: 0.40448332001025017    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
            "episode: 1737   score: 6.0   memory length: 401123   epsilon: 0.4037744800102457    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 5.79\n",
            "episode: 1738   score: 4.0   memory length: 401386   epsilon: 0.4032537400102424    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 5.8\n",
            "episode: 1739   score: 3.0   memory length: 401614   epsilon: 0.40280230001023953    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
            "episode: 1740   score: 3.0   memory length: 401846   epsilon: 0.4023429400102366    steps: 232    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
            "episode: 1741   score: 4.0   memory length: 402109   epsilon: 0.40182220001023333    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
            "episode: 1742   score: 5.0   memory length: 402434   epsilon: 0.40117870001022926    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
            "episode: 1743   score: 5.0   memory length: 402727   epsilon: 0.4005985600102256    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
            "episode: 1744   score: 4.0   memory length: 403021   epsilon: 0.4000164400102219    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
            "episode: 1745   score: 4.0   memory length: 403281   epsilon: 0.39950164001021865    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 5.66\n",
            "episode: 1746   score: 3.0   memory length: 403495   epsilon: 0.39907792001021597    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 5.62\n",
            "episode: 1747   score: 5.0   memory length: 403826   epsilon: 0.3984225400102118    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
            "episode: 1748   score: 6.0   memory length: 404150   epsilon: 0.39778102001020776    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
            "episode: 1749   score: 4.0   memory length: 404428   epsilon: 0.3972305800102043    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
            "episode: 1750   score: 9.0   memory length: 404880   epsilon: 0.3963356200101986    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 5.62\n",
            "episode: 1751   score: 6.0   memory length: 405221   epsilon: 0.39566044001019435    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
            "episode: 1752   score: 11.0   memory length: 405637   epsilon: 0.39483676001018914    steps: 416    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
            "episode: 1753   score: 8.0   memory length: 406030   epsilon: 0.3940586200101842    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 5.6\n",
            "episode: 1754   score: 11.0   memory length: 406553   epsilon: 0.39302308001017766    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 5.63\n",
            "episode: 1755   score: 7.0   memory length: 406929   epsilon: 0.39227860001017295    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 5.62\n",
            "episode: 1756   score: 7.0   memory length: 407320   epsilon: 0.39150442001016805    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
            "episode: 1757   score: 7.0   memory length: 407740   epsilon: 0.3906728200101628    steps: 420    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
            "episode: 1758   score: 9.0   memory length: 408202   epsilon: 0.389758060010157    steps: 462    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
            "episode: 1759   score: 5.0   memory length: 408528   epsilon: 0.3891125800101529    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
            "episode: 1760   score: 7.0   memory length: 408904   epsilon: 0.3883681000101482    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
            "episode: 1761   score: 5.0   memory length: 409200   epsilon: 0.3877820200101445    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
            "episode: 1762   score: 5.0   memory length: 409473   epsilon: 0.3872414800101411    steps: 273    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
            "episode: 1763   score: 8.0   memory length: 409932   epsilon: 0.38633266001013533    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
            "episode: 1764   score: 4.0   memory length: 410213   epsilon: 0.3857762800101318    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
            "episode: 1765   score: 7.0   memory length: 410621   epsilon: 0.3849684400101267    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
            "episode: 1766   score: 10.0   memory length: 411111   epsilon: 0.38399824001012056    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
            "episode: 1767   score: 8.0   memory length: 411500   epsilon: 0.3832280200101157    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
            "episode: 1768   score: 5.0   memory length: 411811   epsilon: 0.3826122400101118    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
            "episode: 1769   score: 9.0   memory length: 412299   epsilon: 0.3816460000101057    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 5.86\n",
            "episode: 1770   score: 5.0   memory length: 412609   epsilon: 0.3810322000101018    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
            "episode: 1771   score: 9.0   memory length: 413066   epsilon: 0.38012734001009607    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
            "episode: 1772   score: 5.0   memory length: 413380   epsilon: 0.37950562001009214    steps: 314    lr: 6.400000000000001e-06     evaluation reward: 5.86\n",
            "episode: 1773   score: 2.0   memory length: 413584   epsilon: 0.3791017000100896    steps: 204    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
            "episode: 1774   score: 5.0   memory length: 413895   epsilon: 0.3784859200100857    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
            "episode: 1775   score: 7.0   memory length: 414303   epsilon: 0.3776780800100806    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
            "episode: 1776   score: 7.0   memory length: 414663   epsilon: 0.37696528001007606    steps: 360    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
            "episode: 1777   score: 9.0   memory length: 415140   epsilon: 0.3760208200100701    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 5.86\n",
            "episode: 1778   score: 7.0   memory length: 415549   epsilon: 0.37521100001006497    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
            "episode: 1779   score: 5.0   memory length: 415859   epsilon: 0.3745972000100611    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
            "episode: 1780   score: 6.0   memory length: 416196   epsilon: 0.37392994001005686    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
            "episode: 1781   score: 5.0   memory length: 416502   epsilon: 0.373324060010053    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1782   score: 6.0   memory length: 416859   epsilon: 0.37261720001004855    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
            "episode: 1783   score: 5.0   memory length: 417171   epsilon: 0.37199944001004465    steps: 312    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
            "episode: 1784   score: 5.0   memory length: 417459   epsilon: 0.37142920001004104    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
            "episode: 1785   score: 5.0   memory length: 417749   epsilon: 0.3708550000100374    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
            "episode: 1786   score: 3.0   memory length: 417961   epsilon: 0.37043524001003475    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 5.95\n",
            "episode: 1787   score: 3.0   memory length: 418173   epsilon: 0.3700154800100321    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
            "episode: 1788   score: 8.0   memory length: 418647   epsilon: 0.36907696001002616    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1789   score: 7.0   memory length: 419016   epsilon: 0.36834634001002153    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1790   score: 6.0   memory length: 419373   epsilon: 0.36763948001001706    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1791   score: 9.0   memory length: 419861   epsilon: 0.36667324001001095    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 6.0\n",
            "episode: 1792   score: 5.0   memory length: 420188   epsilon: 0.36602578001000685    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.95\n",
            "episode: 1793   score: 5.0   memory length: 420496   epsilon: 0.365415940010003    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
            "episode: 1794   score: 7.0   memory length: 420839   epsilon: 0.3647368000099987    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1795   score: 4.0   memory length: 421100   epsilon: 0.3642200200099954    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1796   score: 10.0   memory length: 421600   epsilon: 0.36323002000998916    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
            "episode: 1797   score: 6.0   memory length: 421962   epsilon: 0.36251326000998463    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 5.92\n",
            "episode: 1798   score: 6.0   memory length: 422285   epsilon: 0.3618737200099806    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 5.95\n",
            "episode: 1799   score: 9.0   memory length: 422736   epsilon: 0.36098074000997493    steps: 451    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1800   score: 7.0   memory length: 423111   epsilon: 0.36023824000997023    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
            "episode: 1801   score: 6.0   memory length: 423467   epsilon: 0.3595333600099658    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
            "episode: 1802   score: 4.0   memory length: 423725   epsilon: 0.35902252000996254    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1803   score: 4.0   memory length: 423988   epsilon: 0.35850178000995925    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
            "episode: 1804   score: 4.0   memory length: 424233   epsilon: 0.3580166800099562    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 5.92\n",
            "episode: 1805   score: 3.0   memory length: 424447   epsilon: 0.3575929600099535    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
            "episode: 1806   score: 6.0   memory length: 424825   epsilon: 0.35684452000994876    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
            "episode: 1807   score: 4.0   memory length: 425105   epsilon: 0.35629012000994525    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
            "episode: 1808   score: 7.0   memory length: 425474   epsilon: 0.35555950000994063    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1809   score: 4.0   memory length: 425735   epsilon: 0.35504272000993736    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 5.92\n",
            "episode: 1810   score: 5.0   memory length: 426046   epsilon: 0.35442694000993347    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
            "episode: 1811   score: 5.0   memory length: 426373   epsilon: 0.35377948000992937    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
            "episode: 1812   score: 8.0   memory length: 426791   epsilon: 0.35295184000992413    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1813   score: 5.0   memory length: 427119   epsilon: 0.35230240000992    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1814   score: 6.0   memory length: 427460   epsilon: 0.35162722000991575    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1815   score: 7.0   memory length: 427839   epsilon: 0.350876800009911    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1816   score: 5.0   memory length: 428168   epsilon: 0.3502253800099069    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1817   score: 8.0   memory length: 428611   epsilon: 0.34934824000990133    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1818   score: 8.0   memory length: 429034   epsilon: 0.34851070000989604    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1819   score: 4.0   memory length: 429292   epsilon: 0.3479998600098928    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1820   score: 5.0   memory length: 429620   epsilon: 0.3473504200098887    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
            "episode: 1821   score: 5.0   memory length: 429907   epsilon: 0.3467821600098851    steps: 287    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
            "episode: 1822   score: 8.0   memory length: 430345   epsilon: 0.3459149200098796    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 5.92\n",
            "episode: 1823   score: 8.0   memory length: 430787   epsilon: 0.3450397600098741    steps: 442    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
            "episode: 1824   score: 6.0   memory length: 431145   epsilon: 0.3443309200098696    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1825   score: 8.0   memory length: 431557   epsilon: 0.34351516000986443    steps: 412    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1826   score: 10.0   memory length: 432062   epsilon: 0.3425152600098581    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 6.03\n",
            "episode: 1827   score: 9.0   memory length: 432493   epsilon: 0.3416618800098527    steps: 431    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
            "episode: 1828   score: 3.0   memory length: 432724   epsilon: 0.3412045000098498    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
            "episode: 1829   score: 4.0   memory length: 433007   epsilon: 0.34064416000984626    steps: 283    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
            "episode: 1830   score: 5.0   memory length: 433296   epsilon: 0.34007194000984264    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
            "episode: 1831   score: 4.0   memory length: 433541   epsilon: 0.3395868400098396    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
            "episode: 1832   score: 6.0   memory length: 433914   epsilon: 0.3388483000098349    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1833   score: 5.0   memory length: 434219   epsilon: 0.3382444000098311    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1834   score: 8.0   memory length: 434612   epsilon: 0.33746626000982616    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1835   score: 7.0   memory length: 434996   epsilon: 0.33670594000982135    steps: 384    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
            "episode: 1836   score: 5.0   memory length: 435301   epsilon: 0.3361020400098175    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
            "episode: 1837   score: 4.0   memory length: 435560   epsilon: 0.3355892200098143    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
            "episode: 1838   score: 5.0   memory length: 435835   epsilon: 0.33504472000981084    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 6.0\n",
            "episode: 1839   score: 5.0   memory length: 436142   epsilon: 0.334436860009807    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1840   score: 3.0   memory length: 436356   epsilon: 0.3340131400098043    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1841   score: 7.0   memory length: 436761   epsilon: 0.33321124000979924    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
            "episode: 1842   score: 4.0   memory length: 437039   epsilon: 0.33266080000979575    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
            "episode: 1843   score: 7.0   memory length: 437438   epsilon: 0.33187078000979076    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
            "episode: 1844   score: 6.0   memory length: 437762   epsilon: 0.3312292600097867    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
            "episode: 1845   score: 9.0   memory length: 438239   epsilon: 0.3302848000097807    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
            "episode: 1846   score: 6.0   memory length: 438584   epsilon: 0.3296017000097764    steps: 345    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
            "episode: 1847   score: 9.0   memory length: 438947   epsilon: 0.32888296000977185    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 6.2\n",
            "episode: 1848   score: 8.0   memory length: 439371   epsilon: 0.32804344000976654    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 6.22\n",
            "episode: 1849   score: 3.0   memory length: 439584   epsilon: 0.32762170000976387    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.21\n",
            "episode: 1850   score: 4.0   memory length: 439880   epsilon: 0.32703562000976016    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
            "episode: 1851   score: 6.0   memory length: 440241   epsilon: 0.32632084000975564    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
            "episode: 1852   score: 6.0   memory length: 440572   epsilon: 0.3256654600097515    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
            "episode: 1853   score: 4.0   memory length: 440835   epsilon: 0.3251447200097482    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 6.07\n",
            "episode: 1854   score: 6.0   memory length: 441180   epsilon: 0.3244616200097439    steps: 345    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1855   score: 7.0   memory length: 441555   epsilon: 0.3237191200097392    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1856   score: 6.0   memory length: 441882   epsilon: 0.3230716600097351    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
            "episode: 1857   score: 10.0   memory length: 442350   epsilon: 0.3221450200097292    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
            "episode: 1858   score: 4.0   memory length: 442608   epsilon: 0.321634180009726    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
            "episode: 1859   score: 3.0   memory length: 442857   epsilon: 0.32114116000972287    steps: 249    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1860   score: 9.0   memory length: 443329   epsilon: 0.32020660000971696    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
            "episode: 1861   score: 3.0   memory length: 443543   epsilon: 0.3197828800097143    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1862   score: 3.0   memory length: 443791   epsilon: 0.31929184000971117    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 5.95\n",
            "episode: 1863   score: 11.0   memory length: 444313   epsilon: 0.31825828000970463    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1864   score: 4.0   memory length: 444611   epsilon: 0.3176682400097009    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
            "episode: 1865   score: 5.0   memory length: 444938   epsilon: 0.3170207800096968    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
            "episode: 1866   score: 7.0   memory length: 445345   epsilon: 0.3162149200096917    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
            "episode: 1867   score: 5.0   memory length: 445636   epsilon: 0.31563874000968806    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
            "episode: 1868   score: 3.0   memory length: 445867   epsilon: 0.31518136000968516    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
            "episode: 1869   score: 11.0   memory length: 446380   epsilon: 0.31416562000967874    steps: 513    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
            "episode: 1870   score: 8.0   memory length: 446790   epsilon: 0.3133538200096736    steps: 410    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
            "episode: 1871   score: 10.0   memory length: 447346   epsilon: 0.31225294000966664    steps: 556    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
            "episode: 1872   score: 8.0   memory length: 447782   epsilon: 0.3113896600096612    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
            "episode: 1873   score: 4.0   memory length: 448025   epsilon: 0.31090852000965813    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
            "episode: 1874   score: 11.0   memory length: 448430   epsilon: 0.31010662000965306    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
            "episode: 1875   score: 5.0   memory length: 448741   epsilon: 0.30949084000964916    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.03\n",
            "episode: 1876   score: 6.0   memory length: 449097   epsilon: 0.3087859600096447    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1877   score: 9.0   memory length: 449565   epsilon: 0.30785932000963884    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1878   score: 9.0   memory length: 450049   epsilon: 0.3069010000096328    steps: 484    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
            "episode: 1879   score: 3.0   memory length: 450280   epsilon: 0.3064436200096299    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1880   score: 4.0   memory length: 450560   epsilon: 0.3058892200096264    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 6.0\n",
            "episode: 1881   score: 7.0   memory length: 450939   epsilon: 0.3051388000096216    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
            "episode: 1882   score: 7.0   memory length: 451342   epsilon: 0.3043408600096166    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 6.03\n",
            "episode: 1883   score: 7.0   memory length: 451754   epsilon: 0.3035251000096114    steps: 412    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
            "episode: 1884   score: 9.0   memory length: 452189   epsilon: 0.30266380000960597    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
            "episode: 1885   score: 9.0   memory length: 452683   epsilon: 0.3016856800095998    steps: 494    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
            "episode: 1886   score: 4.0   memory length: 452964   epsilon: 0.30112930000959626    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 6.14\n",
            "episode: 1887   score: 7.0   memory length: 453211   epsilon: 0.30064024000959316    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 6.18\n",
            "episode: 1888   score: 6.0   memory length: 453572   epsilon: 0.29992546000958864    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
            "episode: 1889   score: 5.0   memory length: 453865   epsilon: 0.29934532000958497    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.14\n",
            "episode: 1890   score: 10.0   memory length: 454329   epsilon: 0.29842660000957916    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 6.18\n",
            "episode: 1891   score: 6.0   memory length: 454692   epsilon: 0.2977078600095746    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
            "episode: 1892   score: 11.0   memory length: 455175   epsilon: 0.29675152000956856    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 6.21\n",
            "episode: 1893   score: 8.0   memory length: 455592   epsilon: 0.29592586000956334    steps: 417    lr: 6.400000000000001e-06     evaluation reward: 6.24\n",
            "episode: 1894   score: 5.0   memory length: 455935   epsilon: 0.29524672000955904    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 6.22\n",
            "episode: 1895   score: 6.0   memory length: 456274   epsilon: 0.2945755000095548    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 6.24\n",
            "episode: 1896   score: 5.0   memory length: 456584   epsilon: 0.2939617000095509    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
            "episode: 1897   score: 7.0   memory length: 456991   epsilon: 0.2931558400095458    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 6.2\n",
            "episode: 1898   score: 6.0   memory length: 457385   epsilon: 0.2923757200095409    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 6.2\n",
            "episode: 1899   score: 8.0   memory length: 457792   epsilon: 0.2915698600095358    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
            "episode: 1900   score: 7.0   memory length: 458214   epsilon: 0.2907343000095305    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
            "episode: 1901   score: 6.0   memory length: 458590   epsilon: 0.2899898200095258    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
            "episode: 1902   score: 7.0   memory length: 458995   epsilon: 0.2891879200095207    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.22\n",
            "episode: 1903   score: 10.0   memory length: 459516   epsilon: 0.2881563400095142    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
            "episode: 1904   score: 10.0   memory length: 460022   epsilon: 0.28715446000950784    steps: 506    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
            "episode: 1905   score: 8.0   memory length: 460452   epsilon: 0.28630306000950245    steps: 430    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
            "episode: 1906   score: 6.0   memory length: 460812   epsilon: 0.28559026000949794    steps: 360    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
            "episode: 1907   score: 3.0   memory length: 461026   epsilon: 0.28516654000949526    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
            "episode: 1908   score: 3.0   memory length: 461240   epsilon: 0.2847428200094926    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
            "episode: 1909   score: 5.0   memory length: 461515   epsilon: 0.28419832000948914    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
            "episode: 1910   score: 7.0   memory length: 461853   epsilon: 0.2835290800094849    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
            "episode: 1911   score: 18.0   memory length: 462542   epsilon: 0.28216486000947627    steps: 689    lr: 6.400000000000001e-06     evaluation reward: 6.5\n",
            "episode: 1912   score: 5.0   memory length: 462854   epsilon: 0.28154710000947236    steps: 312    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
            "episode: 1913   score: 9.0   memory length: 463331   epsilon: 0.2806026400094664    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 6.51\n",
            "episode: 1914   score: 7.0   memory length: 463723   epsilon: 0.2798264800094615    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
            "episode: 1915   score: 5.0   memory length: 464033   epsilon: 0.2792126800094576    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.5\n",
            "episode: 1916   score: 4.0   memory length: 464309   epsilon: 0.27866620000945413    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
            "episode: 1917   score: 5.0   memory length: 464605   epsilon: 0.2780801200094504    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 6.46\n",
            "episode: 1918   score: 9.0   memory length: 465087   epsilon: 0.2771257600094444    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
            "episode: 1919   score: 5.0   memory length: 465414   epsilon: 0.2764783000094403    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.48\n",
            "episode: 1920   score: 7.0   memory length: 465770   epsilon: 0.27577342000943583    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 6.5\n",
            "episode: 1921   score: 7.0   memory length: 466193   epsilon: 0.27493588000943053    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
            "episode: 1922   score: 3.0   memory length: 466405   epsilon: 0.2745161200094279    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
            "episode: 1923   score: 7.0   memory length: 466811   epsilon: 0.2737122400094228    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 6.46\n",
            "episode: 1924   score: 4.0   memory length: 467056   epsilon: 0.2732271400094197    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 6.44\n",
            "episode: 1925   score: 4.0   memory length: 467315   epsilon: 0.2727143200094165    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
            "episode: 1926   score: 7.0   memory length: 467679   epsilon: 0.2719936000094119    steps: 364    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
            "episode: 1927   score: 6.0   memory length: 468057   epsilon: 0.2712451600094072    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
            "episode: 1928   score: 10.0   memory length: 468580   epsilon: 0.27020962000940063    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
            "episode: 1929   score: 6.0   memory length: 468941   epsilon: 0.2694948400093961    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
            "episode: 1930   score: 5.0   memory length: 469269   epsilon: 0.268845400009392    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
            "episode: 1931   score: 10.0   memory length: 469683   epsilon: 0.2680256800093868    steps: 414    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
            "episode: 1932   score: 8.0   memory length: 470109   epsilon: 0.2671822000093815    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 6.51\n",
            "episode: 1933   score: 8.0   memory length: 470485   epsilon: 0.26643772000937677    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 6.54\n",
            "episode: 1934   score: 7.0   memory length: 470820   epsilon: 0.26577442000937257    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
            "episode: 1935   score: 7.0   memory length: 471225   epsilon: 0.2649725200093675    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
            "episode: 1936   score: 6.0   memory length: 471579   epsilon: 0.26427160000936306    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 6.54\n",
            "episode: 1937   score: 4.0   memory length: 471855   epsilon: 0.2637251200093596    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 6.54\n",
            "episode: 1938   score: 9.0   memory length: 472333   epsilon: 0.2627786800093536    steps: 478    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
            "episode: 1939   score: 4.0   memory length: 472593   epsilon: 0.26226388000935036    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
            "episode: 1940   score: 5.0   memory length: 472882   epsilon: 0.26169166000934674    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 6.59\n",
            "episode: 1941   score: 5.0   memory length: 473188   epsilon: 0.2610857800093429    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
            "episode: 1942   score: 4.0   memory length: 473433   epsilon: 0.26060068000933984    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
            "episode: 1943   score: 8.0   memory length: 473847   epsilon: 0.25978096000933465    steps: 414    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
            "episode: 1944   score: 5.0   memory length: 474176   epsilon: 0.25912954000933053    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
            "episode: 1945   score: 6.0   memory length: 474534   epsilon: 0.25842070000932604    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 6.54\n",
            "episode: 1946   score: 5.0   memory length: 474843   epsilon: 0.25780888000932217    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
            "episode: 1947   score: 9.0   memory length: 475277   epsilon: 0.25694956000931674    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
            "episode: 1948   score: 8.0   memory length: 475676   epsilon: 0.25615954000931174    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
            "episode: 1949   score: 8.0   memory length: 476152   epsilon: 0.2552170600093058    steps: 476    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
            "episode: 1950   score: 4.0   memory length: 476413   epsilon: 0.2547002800093025    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
            "episode: 1951   score: 7.0   memory length: 476778   epsilon: 0.25397758000929793    steps: 365    lr: 6.400000000000001e-06     evaluation reward: 6.59\n",
            "episode: 1952   score: 11.0   memory length: 477275   epsilon: 0.2529935200092917    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
            "episode: 1953   score: 6.0   memory length: 477589   epsilon: 0.25237180000928777    steps: 314    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
            "episode: 1954   score: 7.0   memory length: 477982   epsilon: 0.25159366000928285    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 6.67\n",
            "episode: 1955   score: 7.0   memory length: 478368   epsilon: 0.250829380009278    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 6.67\n",
            "episode: 1956   score: 11.0   memory length: 478766   epsilon: 0.250041340009273    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
            "episode: 1957   score: 5.0   memory length: 479060   epsilon: 0.24945922000926934    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 6.67\n",
            "episode: 1958   score: 5.0   memory length: 479368   epsilon: 0.2488493800092655    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 6.68\n",
            "episode: 1959   score: 9.0   memory length: 479678   epsilon: 0.2482355800092616    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
            "episode: 1960   score: 8.0   memory length: 480078   epsilon: 0.2474435800092566    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
            "episode: 1961   score: 12.0   memory length: 480650   epsilon: 0.24631102000924943    steps: 572    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
            "episode: 1962   score: 11.0   memory length: 481164   epsilon: 0.245293300009243    steps: 514    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
            "episode: 1963   score: 7.0   memory length: 481549   epsilon: 0.24453100000923816    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
            "episode: 1964   score: 9.0   memory length: 482058   epsilon: 0.2435231800092318    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
            "episode: 1965   score: 9.0   memory length: 482541   epsilon: 0.24256684000922574    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 6.95\n",
            "episode: 1966   score: 5.0   memory length: 482847   epsilon: 0.2419609600092219    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
            "episode: 1967   score: 5.0   memory length: 483157   epsilon: 0.24134716000921802    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
            "episode: 1968   score: 6.0   memory length: 483505   epsilon: 0.24065812000921366    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 6.96\n",
            "episode: 1969   score: 8.0   memory length: 483896   epsilon: 0.23988394000920876    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
            "episode: 1970   score: 15.0   memory length: 484498   epsilon: 0.23869198000920122    steps: 602    lr: 6.400000000000001e-06     evaluation reward: 7.0\n",
            "episode: 1971   score: 8.0   memory length: 484911   epsilon: 0.23787424000919605    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 6.98\n",
            "episode: 1972   score: 8.0   memory length: 485315   epsilon: 0.237074320009191    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 6.98\n",
            "episode: 1973   score: 4.0   memory length: 485560   epsilon: 0.23658922000918792    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 6.98\n",
            "episode: 1974   score: 7.0   memory length: 485988   epsilon: 0.23574178000918256    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
            "episode: 1975   score: 4.0   memory length: 486270   epsilon: 0.23518342000917902    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
            "episode: 1976   score: 12.0   memory length: 486781   epsilon: 0.23417164000917262    steps: 511    lr: 6.400000000000001e-06     evaluation reward: 6.99\n",
            "episode: 1977   score: 5.0   memory length: 487092   epsilon: 0.23355586000916873    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.95\n",
            "episode: 1978   score: 8.0   memory length: 487462   epsilon: 0.2328232600091641    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
            "episode: 1979   score: 16.0   memory length: 487953   epsilon: 0.23185108000915794    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 7.07\n",
            "episode: 1980   score: 3.0   memory length: 488165   epsilon: 0.23143132000915528    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
            "episode: 1981   score: 7.0   memory length: 488545   epsilon: 0.23067892000915052    steps: 380    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
            "episode: 1982   score: 4.0   memory length: 488808   epsilon: 0.23015818000914723    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
            "episode: 1983   score: 7.0   memory length: 489170   epsilon: 0.2294414200091427    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
            "episode: 1984   score: 7.0   memory length: 489521   epsilon: 0.2287464400091383    steps: 351    lr: 6.400000000000001e-06     evaluation reward: 7.01\n",
            "episode: 1985   score: 8.0   memory length: 489887   epsilon: 0.2280217600091337    steps: 366    lr: 6.400000000000001e-06     evaluation reward: 7.0\n",
            "episode: 1986   score: 6.0   memory length: 490224   epsilon: 0.2273545000091295    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 7.02\n",
            "episode: 1987   score: 8.0   memory length: 490656   epsilon: 0.22649914000912408    steps: 432    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
            "episode: 1988   score: 5.0   memory length: 490967   epsilon: 0.22588336000912018    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 7.02\n",
            "episode: 1989   score: 6.0   memory length: 491307   epsilon: 0.22521016000911592    steps: 340    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
            "episode: 1990   score: 4.0   memory length: 491572   epsilon: 0.2246854600091126    steps: 265    lr: 6.400000000000001e-06     evaluation reward: 6.97\n",
            "episode: 1991   score: 9.0   memory length: 492031   epsilon: 0.22377664000910685    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 7.0\n",
            "episode: 1992   score: 3.0   memory length: 492262   epsilon: 0.22331926000910396    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 6.92\n",
            "episode: 1993   score: 8.0   memory length: 492643   epsilon: 0.2225648800090992    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 6.92\n",
            "episode: 1994   score: 7.0   memory length: 493048   epsilon: 0.2217629800090941    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
            "episode: 1995   score: 8.0   memory length: 493494   epsilon: 0.22087990000908853    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 6.96\n",
            "episode: 1996   score: 6.0   memory length: 493799   epsilon: 0.2202760000090847    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 6.97\n",
            "episode: 1997   score: 7.0   memory length: 494166   epsilon: 0.2195493400090801    steps: 367    lr: 6.400000000000001e-06     evaluation reward: 6.97\n",
            "episode: 1998   score: 8.0   memory length: 494602   epsilon: 0.21868606000907465    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 6.99\n",
            "episode: 1999   score: 5.0   memory length: 494928   epsilon: 0.21804058000907056    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 6.96\n",
            "episode: 2000   score: 5.0   memory length: 495222   epsilon: 0.21745846000906688    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
            "episode: 2001   score: 9.0   memory length: 495590   epsilon: 0.21672982000906227    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 6.97\n",
            "episode: 2002   score: 5.0   memory length: 495900   epsilon: 0.21611602000905838    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.95\n",
            "episode: 2003   score: 6.0   memory length: 496253   epsilon: 0.21541708000905396    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
            "episode: 2004   score: 3.0   memory length: 496467   epsilon: 0.21499336000905128    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 6.84\n",
            "episode: 2005   score: 7.0   memory length: 496894   epsilon: 0.21414790000904593    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
            "episode: 2006   score: 5.0   memory length: 497187   epsilon: 0.21356776000904226    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
            "episode: 2007   score: 9.0   memory length: 497621   epsilon: 0.21270844000903683    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
            "episode: 2008   score: 5.0   memory length: 497948   epsilon: 0.21206098000903273    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
            "episode: 2009   score: 7.0   memory length: 498328   epsilon: 0.21130858000902797    steps: 380    lr: 6.400000000000001e-06     evaluation reward: 6.92\n",
            "episode: 2010   score: 6.0   memory length: 498661   epsilon: 0.2106492400090238    steps: 333    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
            "episode: 2011   score: 5.0   memory length: 498970   epsilon: 0.21003742000901993    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
            "episode: 2012   score: 8.0   memory length: 499404   epsilon: 0.2091781000090145    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
            "episode: 2013   score: 7.0   memory length: 499773   epsilon: 0.20844748000900987    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
            "episode: 2014   score: 9.0   memory length: 500278   epsilon: 0.20744758000900354    steps: 505    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
            "episode: 2015   score: 8.0   memory length: 500724   epsilon: 0.20656450000899795    steps: 446    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
            "episode: 2016   score: 5.0   memory length: 501035   epsilon: 0.20594872000899406    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 6.85\n",
            "episode: 2017   score: 6.0   memory length: 501372   epsilon: 0.20528146000898984    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
            "episode: 2018   score: 10.0   memory length: 501831   epsilon: 0.20437264000898409    steps: 459    lr: 2.560000000000001e-06     evaluation reward: 6.87\n",
            "episode: 2019   score: 11.0   memory length: 502258   epsilon: 0.20352718000897874    steps: 427    lr: 2.560000000000001e-06     evaluation reward: 6.93\n",
            "episode: 2020   score: 9.0   memory length: 502732   epsilon: 0.2025886600089728    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 6.95\n",
            "episode: 2021   score: 4.0   memory length: 502992   epsilon: 0.20207386000896954    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 6.92\n",
            "episode: 2022   score: 7.0   memory length: 503414   epsilon: 0.20123830000896425    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 6.96\n",
            "episode: 2023   score: 8.0   memory length: 503842   epsilon: 0.2003908600089589    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 6.97\n",
            "episode: 2024   score: 6.0   memory length: 504167   epsilon: 0.19974736000895482    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 6.99\n",
            "episode: 2025   score: 6.0   memory length: 504546   epsilon: 0.19899694000895007    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 7.01\n",
            "episode: 2026   score: 8.0   memory length: 504998   epsilon: 0.1981019800089444    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 7.02\n",
            "episode: 2027   score: 8.0   memory length: 505388   epsilon: 0.19732978000893953    steps: 390    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
            "episode: 2028   score: 10.0   memory length: 505898   epsilon: 0.19631998000893314    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
            "episode: 2029   score: 3.0   memory length: 506112   epsilon: 0.19589626000893046    steps: 214    lr: 2.560000000000001e-06     evaluation reward: 7.01\n",
            "episode: 2030   score: 9.0   memory length: 506589   epsilon: 0.19495180000892448    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 7.05\n",
            "episode: 2031   score: 5.0   memory length: 506896   epsilon: 0.19434394000892063    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
            "episode: 2032   score: 6.0   memory length: 507252   epsilon: 0.19363906000891618    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 6.98\n",
            "episode: 2033   score: 7.0   memory length: 507680   epsilon: 0.1927916200089108    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 6.97\n",
            "episode: 2034   score: 6.0   memory length: 508042   epsilon: 0.19207486000890628    steps: 362    lr: 2.560000000000001e-06     evaluation reward: 6.96\n",
            "episode: 2035   score: 10.0   memory length: 508477   epsilon: 0.19121356000890083    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 6.99\n",
            "episode: 2036   score: 5.0   memory length: 508773   epsilon: 0.19062748000889712    steps: 296    lr: 2.560000000000001e-06     evaluation reward: 6.98\n",
            "episode: 2037   score: 8.0   memory length: 509219   epsilon: 0.18974440000889153    steps: 446    lr: 2.560000000000001e-06     evaluation reward: 7.02\n",
            "episode: 2038   score: 4.0   memory length: 509484   epsilon: 0.18921970000888821    steps: 265    lr: 2.560000000000001e-06     evaluation reward: 6.97\n",
            "episode: 2039   score: 5.0   memory length: 509765   epsilon: 0.1886633200088847    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 6.98\n",
            "episode: 2040   score: 8.0   memory length: 510218   epsilon: 0.18776638000887902    steps: 453    lr: 2.560000000000001e-06     evaluation reward: 7.01\n",
            "episode: 2041   score: 7.0   memory length: 510604   epsilon: 0.18700210000887418    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 7.03\n",
            "episode: 2042   score: 10.0   memory length: 511037   epsilon: 0.18614476000886876    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 7.09\n",
            "episode: 2043   score: 12.0   memory length: 511508   epsilon: 0.18521218000886286    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 7.13\n",
            "episode: 2044   score: 8.0   memory length: 511936   epsilon: 0.1843647400088575    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
            "episode: 2045   score: 10.0   memory length: 512400   epsilon: 0.18344602000885168    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
            "episode: 2046   score: 12.0   memory length: 512927   epsilon: 0.18240256000884508    steps: 527    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2047   score: 8.0   memory length: 513381   epsilon: 0.1815036400088394    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.26\n",
            "episode: 2048   score: 9.0   memory length: 513861   epsilon: 0.18055324000883338    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2049   score: 6.0   memory length: 514216   epsilon: 0.17985034000882894    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
            "episode: 2050   score: 7.0   memory length: 514603   epsilon: 0.1790840800088241    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
            "episode: 2051   score: 6.0   memory length: 514974   epsilon: 0.17834950000881944    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2052   score: 8.0   memory length: 515359   epsilon: 0.17758720000881462    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
            "episode: 2053   score: 7.0   memory length: 515747   epsilon: 0.17681896000880976    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
            "episode: 2054   score: 5.0   memory length: 516043   epsilon: 0.17623288000880605    steps: 296    lr: 2.560000000000001e-06     evaluation reward: 7.23\n",
            "episode: 2055   score: 8.0   memory length: 516480   epsilon: 0.17536762000880057    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
            "episode: 2056   score: 9.0   memory length: 516935   epsilon: 0.17446672000879487    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
            "episode: 2057   score: 5.0   memory length: 517246   epsilon: 0.17385094000879098    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
            "episode: 2058   score: 7.0   memory length: 517654   epsilon: 0.17304310000878587    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
            "episode: 2059   score: 5.0   memory length: 517947   epsilon: 0.1724629600087822    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
            "episode: 2060   score: 10.0   memory length: 518325   epsilon: 0.17171452000877746    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
            "episode: 2061   score: 11.0   memory length: 518916   epsilon: 0.17054434000877006    steps: 591    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
            "episode: 2062   score: 6.0   memory length: 519271   epsilon: 0.1698414400087656    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
            "episode: 2063   score: 7.0   memory length: 519677   epsilon: 0.16903756000876052    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
            "episode: 2064   score: 5.0   memory length: 520022   epsilon: 0.1683544600087562    steps: 345    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
            "episode: 2065   score: 9.0   memory length: 520470   epsilon: 0.1674674200087506    steps: 448    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
            "episode: 2066   score: 7.0   memory length: 520846   epsilon: 0.16672294000874588    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
            "episode: 2067   score: 7.0   memory length: 521224   epsilon: 0.16597450000874114    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
            "episode: 2068   score: 9.0   memory length: 521697   epsilon: 0.16503796000873522    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
            "episode: 2069   score: 11.0   memory length: 522256   epsilon: 0.16393114000872822    steps: 559    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
            "episode: 2070   score: 7.0   memory length: 522607   epsilon: 0.16323616000872382    steps: 351    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
            "episode: 2071   score: 8.0   memory length: 523070   epsilon: 0.16231942000871802    steps: 463    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
            "episode: 2072   score: 6.0   memory length: 523450   epsilon: 0.16156702000871326    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
            "episode: 2073   score: 6.0   memory length: 523829   epsilon: 0.1608166000087085    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
            "episode: 2074   score: 8.0   memory length: 524231   epsilon: 0.16002064000870347    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 7.15\n",
            "episode: 2075   score: 16.0   memory length: 524838   epsilon: 0.15881878000869587    steps: 607    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2076   score: 4.0   memory length: 525101   epsilon: 0.15829804000869258    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
            "episode: 2077   score: 11.0   memory length: 525641   epsilon: 0.1572288400086858    steps: 540    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
            "episode: 2078   score: 9.0   memory length: 526093   epsilon: 0.15633388000868015    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 7.26\n",
            "episode: 2079   score: 9.0   memory length: 526540   epsilon: 0.15544882000867455    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
            "episode: 2080   score: 12.0   memory length: 527097   epsilon: 0.15434596000866757    steps: 557    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
            "episode: 2081   score: 9.0   memory length: 527542   epsilon: 0.153464860008662    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.3\n",
            "episode: 2082   score: 5.0   memory length: 527886   epsilon: 0.1527837400086577    steps: 344    lr: 2.560000000000001e-06     evaluation reward: 7.31\n",
            "episode: 2083   score: 7.0   memory length: 528292   epsilon: 0.1519798600086526    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 7.31\n",
            "episode: 2084   score: 10.0   memory length: 528751   epsilon: 0.15107104000864685    steps: 459    lr: 2.560000000000001e-06     evaluation reward: 7.34\n",
            "episode: 2085   score: 4.0   memory length: 529047   epsilon: 0.15048496000864314    steps: 296    lr: 2.560000000000001e-06     evaluation reward: 7.3\n",
            "episode: 2086   score: 4.0   memory length: 529308   epsilon: 0.14996818000863987    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
            "episode: 2087   score: 8.0   memory length: 529713   epsilon: 0.1491662800086348    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
            "episode: 2088   score: 5.0   memory length: 530024   epsilon: 0.1485505000086309    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
            "episode: 2089   score: 5.0   memory length: 530315   epsilon: 0.14797432000862726    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2090   score: 11.0   memory length: 530817   epsilon: 0.14698036000862097    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 7.34\n",
            "episode: 2091   score: 14.0   memory length: 531407   epsilon: 0.14581216000861358    steps: 590    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
            "episode: 2092   score: 7.0   memory length: 531852   epsilon: 0.144931060008608    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
            "episode: 2093   score: 7.0   memory length: 532228   epsilon: 0.1441865800086033    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 7.42\n",
            "episode: 2094   score: 8.0   memory length: 532669   epsilon: 0.14331340000859777    steps: 441    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
            "episode: 2095   score: 10.0   memory length: 533123   epsilon: 0.14241448000859208    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
            "episode: 2096   score: 7.0   memory length: 533511   epsilon: 0.14164624000858722    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
            "episode: 2097   score: 6.0   memory length: 533913   epsilon: 0.14085028000858218    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
            "episode: 2098   score: 5.0   memory length: 534224   epsilon: 0.1402345000085783    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 7.42\n",
            "episode: 2099   score: 7.0   memory length: 534625   epsilon: 0.13944052000857327    steps: 401    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
            "episode: 2100   score: 10.0   memory length: 535091   epsilon: 0.13851784000856743    steps: 466    lr: 2.560000000000001e-06     evaluation reward: 7.49\n",
            "episode: 2101   score: 7.0   memory length: 535440   epsilon: 0.13782682000856306    steps: 349    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
            "episode: 2102   score: 7.0   memory length: 535793   epsilon: 0.13712788000855863    steps: 353    lr: 2.560000000000001e-06     evaluation reward: 7.49\n",
            "episode: 2103   score: 8.0   memory length: 536216   epsilon: 0.13629034000855333    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 7.51\n",
            "episode: 2104   score: 6.0   memory length: 536556   epsilon: 0.13561714000854908    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 7.54\n",
            "episode: 2105   score: 6.0   memory length: 536885   epsilon: 0.13496572000854495    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 7.53\n",
            "episode: 2106   score: 11.0   memory length: 537338   epsilon: 0.13406878000853928    steps: 453    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
            "episode: 2107   score: 10.0   memory length: 537716   epsilon: 0.13332034000853454    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
            "episode: 2108   score: 9.0   memory length: 538192   epsilon: 0.13237786000852858    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 7.64\n",
            "episode: 2109   score: 7.0   memory length: 538581   epsilon: 0.1316076400085237    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 7.64\n",
            "episode: 2110   score: 9.0   memory length: 539030   epsilon: 0.13071862000851808    steps: 449    lr: 2.560000000000001e-06     evaluation reward: 7.67\n",
            "episode: 2111   score: 7.0   memory length: 539405   epsilon: 0.12997612000851338    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 7.69\n",
            "episode: 2112   score: 8.0   memory length: 539844   epsilon: 0.12910690000850789    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 7.69\n",
            "episode: 2113   score: 5.0   memory length: 540137   epsilon: 0.12852676000850422    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 7.67\n",
            "episode: 2114   score: 8.0   memory length: 540554   epsilon: 0.127701100008499    steps: 417    lr: 2.560000000000001e-06     evaluation reward: 7.66\n",
            "episode: 2115   score: 7.0   memory length: 540957   epsilon: 0.12690316000849394    steps: 403    lr: 2.560000000000001e-06     evaluation reward: 7.65\n",
            "episode: 2116   score: 8.0   memory length: 541362   epsilon: 0.12610126000848887    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 7.68\n",
            "episode: 2117   score: 13.0   memory length: 541835   epsilon: 0.12516472000848294    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
            "episode: 2118   score: 7.0   memory length: 542082   epsilon: 0.12467566000848213    steps: 247    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
            "episode: 2119   score: 9.0   memory length: 542532   epsilon: 0.12378466000848273    steps: 450    lr: 2.560000000000001e-06     evaluation reward: 7.7\n",
            "episode: 2120   score: 9.0   memory length: 543009   epsilon: 0.12284020000848338    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 7.7\n",
            "episode: 2121   score: 9.0   memory length: 543470   epsilon: 0.121927420008484    steps: 461    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
            "episode: 2122   score: 6.0   memory length: 543811   epsilon: 0.12125224000848446    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
            "episode: 2123   score: 10.0   memory length: 544303   epsilon: 0.12027808000848512    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2124   score: 6.0   memory length: 544674   epsilon: 0.11954350000848563    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2125   score: 8.0   memory length: 545067   epsilon: 0.11876536000848616    steps: 393    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
            "episode: 2126   score: 10.0   memory length: 545541   epsilon: 0.1178268400084868    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
            "episode: 2127   score: 8.0   memory length: 545979   epsilon: 0.11695960000848739    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
            "episode: 2128   score: 4.0   memory length: 546224   epsilon: 0.11647450000848772    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
            "episode: 2129   score: 8.0   memory length: 546669   epsilon: 0.11559340000848832    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
            "episode: 2130   score: 8.0   memory length: 547095   epsilon: 0.1147499200084889    steps: 426    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
            "episode: 2131   score: 9.0   memory length: 547584   epsilon: 0.11378170000848956    steps: 489    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
            "episode: 2132   score: 8.0   memory length: 548003   epsilon: 0.11295208000849012    steps: 419    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
            "episode: 2133   score: 13.0   memory length: 548542   epsilon: 0.11188486000849085    steps: 539    lr: 2.560000000000001e-06     evaluation reward: 7.9\n",
            "episode: 2134   score: 7.0   memory length: 548934   epsilon: 0.11110870000849138    steps: 392    lr: 2.560000000000001e-06     evaluation reward: 7.91\n",
            "episode: 2135   score: 7.0   memory length: 549323   epsilon: 0.1103384800084919    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
            "episode: 2136   score: 11.0   memory length: 549832   epsilon: 0.10933066000849259    steps: 509    lr: 2.560000000000001e-06     evaluation reward: 7.94\n",
            "episode: 2137   score: 5.0   memory length: 550159   epsilon: 0.10868320000849303    steps: 327    lr: 2.560000000000001e-06     evaluation reward: 7.91\n",
            "episode: 2138   score: 10.0   memory length: 550597   epsilon: 0.10781596000849362    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
            "episode: 2139   score: 8.0   memory length: 551016   epsilon: 0.10698634000849419    steps: 419    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
            "episode: 2140   score: 5.0   memory length: 551328   epsilon: 0.10636858000849461    steps: 312    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
            "episode: 2141   score: 5.0   memory length: 551643   epsilon: 0.10574488000849504    steps: 315    lr: 2.560000000000001e-06     evaluation reward: 7.95\n",
            "episode: 2142   score: 7.0   memory length: 552023   epsilon: 0.10499248000849555    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 7.92\n",
            "episode: 2143   score: 7.0   memory length: 552409   epsilon: 0.10422820000849607    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 7.87\n",
            "episode: 2144   score: 4.0   memory length: 552672   epsilon: 0.10370746000849643    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
            "episode: 2145   score: 5.0   memory length: 552983   epsilon: 0.10309168000849685    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
            "episode: 2146   score: 8.0   memory length: 553410   epsilon: 0.10224622000849742    steps: 427    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
            "episode: 2147   score: 7.0   memory length: 553768   epsilon: 0.10153738000849791    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
            "episode: 2148   score: 8.0   memory length: 554175   epsilon: 0.10073152000849846    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
            "episode: 2149   score: 9.0   memory length: 554661   epsilon: 0.09976924000849911    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
            "episode: 2150   score: 4.0   memory length: 554908   epsilon: 0.09928018000849945    steps: 247    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
            "episode: 2151   score: 5.0   memory length: 555201   epsilon: 0.09870004000849984    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
            "episode: 2152   score: 12.0   memory length: 555672   epsilon: 0.09776746000850048    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
            "episode: 2153   score: 8.0   memory length: 556082   epsilon: 0.09695566000850103    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2154   score: 5.0   memory length: 556374   epsilon: 0.09637750000850143    steps: 292    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2155   score: 8.0   memory length: 556814   epsilon: 0.09550630000850202    steps: 440    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2156   score: 6.0   memory length: 557173   epsilon: 0.0947954800085025    steps: 359    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
            "episode: 2157   score: 8.0   memory length: 557599   epsilon: 0.09395200000850308    steps: 426    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2158   score: 15.0   memory length: 558144   epsilon: 0.09287290000850382    steps: 545    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
            "episode: 2159   score: 9.0   memory length: 558640   epsilon: 0.09189082000850449    steps: 496    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
            "episode: 2160   score: 7.0   memory length: 559022   epsilon: 0.091134460008505    steps: 382    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
            "episode: 2161   score: 5.0   memory length: 559311   epsilon: 0.09056224000850539    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
            "episode: 2162   score: 6.0   memory length: 559652   epsilon: 0.08988706000850585    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
            "episode: 2163   score: 4.0   memory length: 559896   epsilon: 0.08940394000850618    steps: 244    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2164   score: 7.0   memory length: 560233   epsilon: 0.08873668000850664    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
            "episode: 2165   score: 7.0   memory length: 560570   epsilon: 0.0880694200085071    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
            "episode: 2166   score: 8.0   memory length: 560975   epsilon: 0.08726752000850764    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
            "episode: 2167   score: 7.0   memory length: 561354   epsilon: 0.08651710000850815    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
            "episode: 2168   score: 7.0   memory length: 561702   epsilon: 0.08582806000850862    steps: 348    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
            "episode: 2169   score: 10.0   memory length: 562196   epsilon: 0.08484994000850929    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
            "episode: 2170   score: 6.0   memory length: 562535   epsilon: 0.08417872000850975    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
            "episode: 2171   score: 7.0   memory length: 562884   epsilon: 0.08348770000851022    steps: 349    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
            "episode: 2172   score: 6.0   memory length: 563246   epsilon: 0.08277094000851071    steps: 362    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
            "episode: 2173   score: 15.0   memory length: 563875   epsilon: 0.08152552000851156    steps: 629    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
            "episode: 2174   score: 8.0   memory length: 564310   epsilon: 0.08066422000851214    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
            "episode: 2175   score: 8.0   memory length: 564759   epsilon: 0.07977520000851275    steps: 449    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
            "episode: 2176   score: 6.0   memory length: 565156   epsilon: 0.07898914000851329    steps: 397    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
            "episode: 2177   score: 7.0   memory length: 565543   epsilon: 0.07822288000851381    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
            "episode: 2178   score: 9.0   memory length: 565966   epsilon: 0.07738534000851438    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
            "episode: 2179   score: 7.0   memory length: 566364   epsilon: 0.07659730000851492    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 7.69\n",
            "episode: 2180   score: 6.0   memory length: 566706   epsilon: 0.07592014000851538    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
            "episode: 2181   score: 7.0   memory length: 567114   epsilon: 0.07511230000851593    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
            "episode: 2182   score: 8.0   memory length: 567551   epsilon: 0.07424704000851652    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 7.64\n",
            "episode: 2183   score: 7.0   memory length: 567960   epsilon: 0.07343722000851707    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 7.64\n",
            "episode: 2184   score: 7.0   memory length: 568329   epsilon: 0.07270660000851757    steps: 369    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
            "episode: 2185   score: 4.0   memory length: 568592   epsilon: 0.07218586000851793    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
            "episode: 2186   score: 9.0   memory length: 569066   epsilon: 0.07124734000851857    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 7.66\n",
            "episode: 2187   score: 5.0   memory length: 569359   epsilon: 0.07066720000851896    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
            "episode: 2188   score: 10.0   memory length: 569791   epsilon: 0.06981184000851955    steps: 432    lr: 2.560000000000001e-06     evaluation reward: 7.68\n",
            "episode: 2189   score: 10.0   memory length: 570248   epsilon: 0.06890698000852016    steps: 457    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
            "episode: 2190   score: 7.0   memory length: 570623   epsilon: 0.06816448000852067    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 7.69\n",
            "episode: 2191   score: 9.0   memory length: 571122   epsilon: 0.06717646000852134    steps: 499    lr: 2.560000000000001e-06     evaluation reward: 7.64\n",
            "episode: 2192   score: 6.0   memory length: 571464   epsilon: 0.0664993000085218    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
            "episode: 2193   score: 9.0   memory length: 571883   epsilon: 0.06566968000852237    steps: 419    lr: 2.560000000000001e-06     evaluation reward: 7.65\n",
            "episode: 2194   score: 6.0   memory length: 572193   epsilon: 0.06505588000852279    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
            "episode: 2195   score: 5.0   memory length: 572522   epsilon: 0.06440446000852323    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 7.58\n",
            "episode: 2196   score: 9.0   memory length: 572985   epsilon: 0.06348772000852386    steps: 463    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
            "episode: 2197   score: 12.0   memory length: 573451   epsilon: 0.06256504000852449    steps: 466    lr: 2.560000000000001e-06     evaluation reward: 7.66\n",
            "episode: 2198   score: 8.0   memory length: 573882   epsilon: 0.06171166000852507    steps: 431    lr: 2.560000000000001e-06     evaluation reward: 7.69\n",
            "episode: 2199   score: 9.0   memory length: 574327   epsilon: 0.06083056000852567    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
            "episode: 2200   score: 7.0   memory length: 574686   epsilon: 0.06011974000852616    steps: 359    lr: 2.560000000000001e-06     evaluation reward: 7.68\n",
            "episode: 2201   score: 11.0   memory length: 575211   epsilon: 0.059080240008526866    steps: 525    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
            "episode: 2202   score: 4.0   memory length: 575454   epsilon: 0.058599100008527194    steps: 243    lr: 2.560000000000001e-06     evaluation reward: 7.69\n",
            "episode: 2203   score: 4.0   memory length: 575715   epsilon: 0.058082320008527547    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 7.65\n",
            "episode: 2204   score: 10.0   memory length: 576197   epsilon: 0.0571279600085282    steps: 482    lr: 2.560000000000001e-06     evaluation reward: 7.69\n",
            "episode: 2205   score: 8.0   memory length: 576615   epsilon: 0.05630032000852876    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
            "episode: 2206   score: 5.0   memory length: 576897   epsilon: 0.05574196000852914    steps: 282    lr: 2.560000000000001e-06     evaluation reward: 7.65\n",
            "episode: 2207   score: 8.0   memory length: 577267   epsilon: 0.05500936000852964    steps: 370    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
            "episode: 2208   score: 9.0   memory length: 577758   epsilon: 0.054037180008530306    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
            "episode: 2209   score: 4.0   memory length: 578003   epsilon: 0.053552080008530636    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
            "episode: 2210   score: 7.0   memory length: 578396   epsilon: 0.05277394000853117    steps: 393    lr: 2.560000000000001e-06     evaluation reward: 7.58\n",
            "episode: 2211   score: 6.0   memory length: 578715   epsilon: 0.0521423200085316    steps: 319    lr: 2.560000000000001e-06     evaluation reward: 7.57\n",
            "episode: 2212   score: 8.0   memory length: 579171   epsilon: 0.051239440008532214    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 7.57\n",
            "episode: 2213   score: 7.0   memory length: 579517   epsilon: 0.05055436000853268    steps: 346    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
            "episode: 2214   score: 5.0   memory length: 579813   epsilon: 0.04996828000853308    steps: 296    lr: 2.560000000000001e-06     evaluation reward: 7.56\n",
            "episode: 2215   score: 6.0   memory length: 580155   epsilon: 0.04929112000853354    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 7.55\n",
            "episode: 2216   score: 4.0   memory length: 580413   epsilon: 0.04878028000853389    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 7.51\n",
            "episode: 2217   score: 7.0   memory length: 580789   epsilon: 0.0480358000085344    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
            "episode: 2218   score: 9.0   memory length: 581251   epsilon: 0.04712104000853502    steps: 462    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
            "episode: 2219   score: 9.0   memory length: 581675   epsilon: 0.046281520008535595    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
            "episode: 2220   score: 12.0   memory length: 582111   epsilon: 0.045418240008536184    steps: 436    lr: 2.560000000000001e-06     evaluation reward: 7.5\n",
            "episode: 2221   score: 10.0   memory length: 582619   epsilon: 0.04441240000853687    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 7.51\n",
            "episode: 2222   score: 7.0   memory length: 583023   epsilon: 0.043612480008537416    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 7.52\n",
            "episode: 2223   score: 5.0   memory length: 583347   epsilon: 0.042970960008537853    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
            "episode: 2224   score: 4.0   memory length: 583628   epsilon: 0.04241458000853823    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
            "episode: 2225   score: 9.0   memory length: 584133   epsilon: 0.041414680008538915    steps: 505    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
            "episode: 2226   score: 7.0   memory length: 584541   epsilon: 0.040606840008539466    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
            "episode: 2227   score: 10.0   memory length: 585053   epsilon: 0.03959308000854016    steps: 512    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
            "episode: 2228   score: 4.0   memory length: 585298   epsilon: 0.03910798000854049    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
            "episode: 2229   score: 8.0   memory length: 585776   epsilon: 0.038161540008541134    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
            "episode: 2230   score: 6.0   memory length: 586083   epsilon: 0.03755368000854155    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
            "episode: 2231   score: 5.0   memory length: 586365   epsilon: 0.03699532000854193    steps: 282    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
            "episode: 2232   score: 7.0   memory length: 586729   epsilon: 0.03627460000854242    steps: 364    lr: 2.560000000000001e-06     evaluation reward: 7.38\n",
            "episode: 2233   score: 4.0   memory length: 586974   epsilon: 0.03578950000854275    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.29\n",
            "episode: 2234   score: 4.0   memory length: 587219   epsilon: 0.03530440000854308    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.26\n",
            "episode: 2235   score: 5.0   memory length: 587545   epsilon: 0.03465892000854352    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
            "episode: 2236   score: 4.0   memory length: 587803   epsilon: 0.03414808000854387    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
            "episode: 2237   score: 9.0   memory length: 588248   epsilon: 0.03326698000854447    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
            "episode: 2238   score: 5.0   memory length: 588555   epsilon: 0.03265912000854489    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
            "episode: 2239   score: 4.0   memory length: 588800   epsilon: 0.03217402000854522    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
            "episode: 2240   score: 8.0   memory length: 589245   epsilon: 0.03129292000854582    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.15\n",
            "episode: 2241   score: 10.0   memory length: 589703   epsilon: 0.030386080008546437    steps: 458    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
            "episode: 2242   score: 10.0   memory length: 590205   epsilon: 0.029392120008547115    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 7.23\n",
            "episode: 2243   score: 7.0   memory length: 590580   epsilon: 0.02864962000854762    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 7.23\n",
            "episode: 2244   score: 6.0   memory length: 590937   epsilon: 0.027942760008548104    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
            "episode: 2245   score: 8.0   memory length: 591393   epsilon: 0.02703988000854872    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
            "episode: 2246   score: 7.0   memory length: 591781   epsilon: 0.026271640008549244    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2247   score: 6.0   memory length: 592105   epsilon: 0.02563012000854968    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 7.26\n",
            "episode: 2248   score: 5.0   memory length: 592398   epsilon: 0.025049980008550077    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 7.23\n",
            "episode: 2249   score: 7.0   memory length: 592776   epsilon: 0.024301540008550587    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
            "episode: 2250   score: 10.0   memory length: 593237   epsilon: 0.02338876000855121    steps: 461    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2251   score: 5.0   memory length: 593526   epsilon: 0.0228165400085516    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
            "episode: 2252   score: 10.0   memory length: 593984   epsilon: 0.02190970000855222    steps: 458    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
            "episode: 2253   score: 7.0   memory length: 594359   epsilon: 0.021167200008552725    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
            "episode: 2254   score: 5.0   memory length: 594648   epsilon: 0.020594980008553115    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
            "episode: 2255   score: 8.0   memory length: 595083   epsilon: 0.019733680008553703    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
            "episode: 2256   score: 4.0   memory length: 595328   epsilon: 0.019248580008554034    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
            "episode: 2257   score: 9.0   memory length: 595795   epsilon: 0.018323920008554664    steps: 467    lr: 2.560000000000001e-06     evaluation reward: 7.23\n",
            "episode: 2258   score: 9.0   memory length: 596303   epsilon: 0.01731808000855535    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
            "episode: 2259   score: 5.0   memory length: 596608   epsilon: 0.016714180008555762    steps: 305    lr: 2.560000000000001e-06     evaluation reward: 7.13\n",
            "episode: 2260   score: 9.0   memory length: 597081   epsilon: 0.0157776400085564    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.15\n",
            "episode: 2261   score: 4.0   memory length: 597326   epsilon: 0.01529254000855644    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
            "episode: 2262   score: 4.0   memory length: 597571   epsilon: 0.014807440008556346    steps: 245    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
            "episode: 2263   score: 8.0   memory length: 598027   epsilon: 0.013904560008556171    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
            "episode: 2264   score: 10.0   memory length: 598539   epsilon: 0.012890800008555975    steps: 512    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
            "episode: 2265   score: 8.0   memory length: 598997   epsilon: 0.011983960008555799    steps: 458    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
            "episode: 2266   score: 7.0   memory length: 599371   epsilon: 0.011243440008555655    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
            "episode: 2267   score: 9.0   memory length: 599879   epsilon: 0.01023760000855546    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
            "episode: 2268   score: 9.0   memory length: 600369   epsilon: 0.009998020008555413    steps: 490    lr: 1.0240000000000005e-06     evaluation reward: 7.23\n",
            "episode: 2269   score: 9.0   memory length: 600877   epsilon: 0.009998020008555413    steps: 508    lr: 1.0240000000000005e-06     evaluation reward: 7.22\n",
            "episode: 2270   score: 12.0   memory length: 601348   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.28\n",
            "episode: 2271   score: 12.0   memory length: 601819   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.33\n",
            "episode: 2272   score: 11.0   memory length: 602353   epsilon: 0.009998020008555413    steps: 534    lr: 1.0240000000000005e-06     evaluation reward: 7.38\n",
            "episode: 2273   score: 9.0   memory length: 602861   epsilon: 0.009998020008555413    steps: 508    lr: 1.0240000000000005e-06     evaluation reward: 7.32\n",
            "episode: 2274   score: 8.0   memory length: 603317   epsilon: 0.009998020008555413    steps: 456    lr: 1.0240000000000005e-06     evaluation reward: 7.32\n",
            "episode: 2275   score: 7.0   memory length: 603684   epsilon: 0.009998020008555413    steps: 367    lr: 1.0240000000000005e-06     evaluation reward: 7.31\n",
            "episode: 2276   score: 12.0   memory length: 604155   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.37\n",
            "episode: 2277   score: 7.0   memory length: 604542   epsilon: 0.009998020008555413    steps: 387    lr: 1.0240000000000005e-06     evaluation reward: 7.37\n",
            "episode: 2278   score: 13.0   memory length: 605060   epsilon: 0.009998020008555413    steps: 518    lr: 1.0240000000000005e-06     evaluation reward: 7.41\n",
            "episode: 2279   score: 13.0   memory length: 605582   epsilon: 0.009998020008555413    steps: 522    lr: 1.0240000000000005e-06     evaluation reward: 7.47\n",
            "episode: 2280   score: 7.0   memory length: 605974   epsilon: 0.009998020008555413    steps: 392    lr: 1.0240000000000005e-06     evaluation reward: 7.48\n",
            "episode: 2281   score: 8.0   memory length: 606432   epsilon: 0.009998020008555413    steps: 458    lr: 1.0240000000000005e-06     evaluation reward: 7.49\n",
            "episode: 2282   score: 9.0   memory length: 606901   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 7.5\n",
            "episode: 2283   score: 9.0   memory length: 607370   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 7.52\n",
            "episode: 2284   score: 9.0   memory length: 607839   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 7.54\n",
            "episode: 2285   score: 9.0   memory length: 608308   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 7.59\n",
            "episode: 2286   score: 8.0   memory length: 608746   epsilon: 0.009998020008555413    steps: 438    lr: 1.0240000000000005e-06     evaluation reward: 7.58\n",
            "episode: 2287   score: 8.0   memory length: 609204   epsilon: 0.009998020008555413    steps: 458    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2288   score: 12.0   memory length: 609675   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.63\n",
            "episode: 2289   score: 6.0   memory length: 610072   epsilon: 0.009998020008555413    steps: 397    lr: 1.0240000000000005e-06     evaluation reward: 7.59\n",
            "episode: 2290   score: 6.0   memory length: 610391   epsilon: 0.009998020008555413    steps: 319    lr: 1.0240000000000005e-06     evaluation reward: 7.58\n",
            "episode: 2291   score: 9.0   memory length: 610854   epsilon: 0.009998020008555413    steps: 463    lr: 1.0240000000000005e-06     evaluation reward: 7.58\n",
            "episode: 2292   score: 10.0   memory length: 611361   epsilon: 0.009998020008555413    steps: 507    lr: 1.0240000000000005e-06     evaluation reward: 7.62\n",
            "episode: 2293   score: 12.0   memory length: 611832   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.65\n",
            "episode: 2294   score: 6.0   memory length: 612169   epsilon: 0.009998020008555413    steps: 337    lr: 1.0240000000000005e-06     evaluation reward: 7.65\n",
            "episode: 2295   score: 6.0   memory length: 612507   epsilon: 0.009998020008555413    steps: 338    lr: 1.0240000000000005e-06     evaluation reward: 7.66\n",
            "episode: 2296   score: 8.0   memory length: 612946   epsilon: 0.009998020008555413    steps: 439    lr: 1.0240000000000005e-06     evaluation reward: 7.65\n",
            "episode: 2297   score: 7.0   memory length: 613344   epsilon: 0.009998020008555413    steps: 398    lr: 1.0240000000000005e-06     evaluation reward: 7.6\n",
            "episode: 2298   score: 9.0   memory length: 613793   epsilon: 0.009998020008555413    steps: 449    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2299   score: 8.0   memory length: 614254   epsilon: 0.009998020008555413    steps: 461    lr: 1.0240000000000005e-06     evaluation reward: 7.6\n",
            "episode: 2300   score: 9.0   memory length: 614731   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 7.62\n",
            "episode: 2301   score: 6.0   memory length: 615034   epsilon: 0.009998020008555413    steps: 303    lr: 1.0240000000000005e-06     evaluation reward: 7.57\n",
            "episode: 2302   score: 6.0   memory length: 615337   epsilon: 0.009998020008555413    steps: 303    lr: 1.0240000000000005e-06     evaluation reward: 7.59\n",
            "episode: 2303   score: 8.0   memory length: 615721   epsilon: 0.009998020008555413    steps: 384    lr: 1.0240000000000005e-06     evaluation reward: 7.63\n",
            "episode: 2304   score: 8.0   memory length: 616157   epsilon: 0.009998020008555413    steps: 436    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2305   score: 8.0   memory length: 616632   epsilon: 0.009998020008555413    steps: 475    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2306   score: 5.0   memory length: 616938   epsilon: 0.009998020008555413    steps: 306    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2307   score: 11.0   memory length: 617484   epsilon: 0.009998020008555413    steps: 546    lr: 1.0240000000000005e-06     evaluation reward: 7.64\n",
            "episode: 2308   score: 9.0   memory length: 617936   epsilon: 0.009998020008555413    steps: 452    lr: 1.0240000000000005e-06     evaluation reward: 7.64\n",
            "episode: 2309   score: 8.0   memory length: 618355   epsilon: 0.009998020008555413    steps: 419    lr: 1.0240000000000005e-06     evaluation reward: 7.68\n",
            "episode: 2310   score: 5.0   memory length: 618647   epsilon: 0.009998020008555413    steps: 292    lr: 1.0240000000000005e-06     evaluation reward: 7.66\n",
            "episode: 2311   score: 4.0   memory length: 618892   epsilon: 0.009998020008555413    steps: 245    lr: 1.0240000000000005e-06     evaluation reward: 7.64\n",
            "episode: 2312   score: 5.0   memory length: 619215   epsilon: 0.009998020008555413    steps: 323    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2313   score: 6.0   memory length: 619522   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 7.6\n",
            "episode: 2314   score: 6.0   memory length: 619852   epsilon: 0.009998020008555413    steps: 330    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2315   score: 6.0   memory length: 620159   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 7.61\n",
            "episode: 2316   score: 7.0   memory length: 620568   epsilon: 0.009998020008555413    steps: 409    lr: 1.0240000000000005e-06     evaluation reward: 7.64\n",
            "episode: 2317   score: 8.0   memory length: 621004   epsilon: 0.009998020008555413    steps: 436    lr: 1.0240000000000005e-06     evaluation reward: 7.65\n",
            "episode: 2318   score: 9.0   memory length: 621456   epsilon: 0.009998020008555413    steps: 452    lr: 1.0240000000000005e-06     evaluation reward: 7.65\n",
            "episode: 2319   score: 12.0   memory length: 621927   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.68\n",
            "episode: 2320   score: 12.0   memory length: 622398   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.68\n",
            "episode: 2321   score: 8.0   memory length: 622856   epsilon: 0.009998020008555413    steps: 458    lr: 1.0240000000000005e-06     evaluation reward: 7.66\n",
            "episode: 2322   score: 4.0   memory length: 623101   epsilon: 0.009998020008555413    steps: 245    lr: 1.0240000000000005e-06     evaluation reward: 7.63\n",
            "episode: 2323   score: 7.0   memory length: 623515   epsilon: 0.009998020008555413    steps: 414    lr: 1.0240000000000005e-06     evaluation reward: 7.65\n",
            "episode: 2324   score: 9.0   memory length: 623989   epsilon: 0.009998020008555413    steps: 474    lr: 1.0240000000000005e-06     evaluation reward: 7.7\n",
            "episode: 2325   score: 11.0   memory length: 624523   epsilon: 0.009998020008555413    steps: 534    lr: 1.0240000000000005e-06     evaluation reward: 7.72\n",
            "episode: 2326   score: 6.0   memory length: 624826   epsilon: 0.009998020008555413    steps: 303    lr: 1.0240000000000005e-06     evaluation reward: 7.71\n",
            "episode: 2327   score: 10.0   memory length: 625319   epsilon: 0.009998020008555413    steps: 493    lr: 1.0240000000000005e-06     evaluation reward: 7.71\n",
            "episode: 2328   score: 12.0   memory length: 625790   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.79\n",
            "episode: 2329   score: 8.0   memory length: 626267   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 7.79\n",
            "episode: 2330   score: 12.0   memory length: 626738   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.85\n",
            "episode: 2331   score: 12.0   memory length: 627209   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.92\n",
            "episode: 2332   score: 11.0   memory length: 627760   epsilon: 0.009998020008555413    steps: 551    lr: 1.0240000000000005e-06     evaluation reward: 7.96\n",
            "episode: 2333   score: 12.0   memory length: 628231   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 8.04\n",
            "episode: 2334   score: 15.0   memory length: 628795   epsilon: 0.009998020008555413    steps: 564    lr: 1.0240000000000005e-06     evaluation reward: 8.15\n",
            "episode: 2335   score: 11.0   memory length: 629329   epsilon: 0.009998020008555413    steps: 534    lr: 1.0240000000000005e-06     evaluation reward: 8.21\n",
            "episode: 2336   score: 9.0   memory length: 629742   epsilon: 0.009998020008555413    steps: 413    lr: 1.0240000000000005e-06     evaluation reward: 8.26\n",
            "episode: 2337   score: 8.0   memory length: 630168   epsilon: 0.009998020008555413    steps: 426    lr: 1.0240000000000005e-06     evaluation reward: 8.25\n",
            "episode: 2338   score: 11.0   memory length: 630677   epsilon: 0.009998020008555413    steps: 509    lr: 1.0240000000000005e-06     evaluation reward: 8.31\n",
            "episode: 2339   score: 12.0   memory length: 631148   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 8.39\n",
            "episode: 2340   score: 12.0   memory length: 631619   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 8.43\n",
            "episode: 2341   score: 9.0   memory length: 632115   epsilon: 0.009998020008555413    steps: 496    lr: 1.0240000000000005e-06     evaluation reward: 8.42\n",
            "episode: 2342   score: 12.0   memory length: 632586   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 8.44\n",
            "episode: 2343   score: 9.0   memory length: 633055   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 8.46\n",
            "episode: 2344   score: 10.0   memory length: 633507   epsilon: 0.009998020008555413    steps: 452    lr: 1.0240000000000005e-06     evaluation reward: 8.5\n",
            "episode: 2345   score: 11.0   memory length: 633928   epsilon: 0.009998020008555413    steps: 421    lr: 1.0240000000000005e-06     evaluation reward: 8.53\n",
            "episode: 2346   score: 8.0   memory length: 634384   epsilon: 0.009998020008555413    steps: 456    lr: 1.0240000000000005e-06     evaluation reward: 8.54\n",
            "episode: 2347   score: 7.0   memory length: 634813   epsilon: 0.009998020008555413    steps: 429    lr: 1.0240000000000005e-06     evaluation reward: 8.55\n",
            "episode: 2348   score: 10.0   memory length: 635271   epsilon: 0.009998020008555413    steps: 458    lr: 1.0240000000000005e-06     evaluation reward: 8.6\n",
            "episode: 2349   score: 8.0   memory length: 635727   epsilon: 0.009998020008555413    steps: 456    lr: 1.0240000000000005e-06     evaluation reward: 8.61\n",
            "episode: 2350   score: 8.0   memory length: 636183   epsilon: 0.009998020008555413    steps: 456    lr: 1.0240000000000005e-06     evaluation reward: 8.59\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-3b074d992556>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# You can set train_frame to a lower value while testing your starts training earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Update the target network only for Double DQN only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-d6f628fa7142>\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3d6b2b261fdc>\u001b[0m in \u001b[0;36msample_mini_batch\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mmini_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state, HISTORY_SIZE)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = torch.tensor([[0]]).cuda()\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "\n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['lives'])\n",
        "\n",
        "        life = info['lives']\n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory\n",
        "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "\n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards')\n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "\n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyk0T8UOgPt5"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm2Qsua8gPt5"
      },
      "source": [
        "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
        "\n",
        "Please save your model before running this portion of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Tc6AI3LIgPt6"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install xvfb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9YTeC6zHaf8",
        "outputId": "3a634110-c442-4003-b928-e86b0dad1ba6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n",
            "Fetched 7,813 kB in 4s (2,156 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 121920 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G5k7HJD3gPt6"
      },
      "outputs": [],
      "source": [
        "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = RecordVideo(env, './video')\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DOSBqiHEgPt6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "outputId": "76e14143-d59a-41b7-da7c-d494bc1d59ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/assignment5/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BreakoutDeterministic-v4 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay\n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7907e3c3e6b0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state, HISTORY_SIZE)\n",
        "\n",
        "while not done:\n",
        "\n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "    state = next_state\n",
        "\n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "\n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['lives'])\n",
        "\n",
        "    life = info['lives']\n",
        "    r = np.clip(reward, -1, 1)\n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory\n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "\n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehqX7LSuDcvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}