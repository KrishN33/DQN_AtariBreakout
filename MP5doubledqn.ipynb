{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m180.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.24.1)\n",
      "Collecting scipy>=1.9 (from scikit-image)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.0)\n",
      "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.3.0)\n",
      "Collecting imageio>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2024.4.24-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2024.4.24-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tifffile, scipy, lazy-loader, imageio, scikit-image\n",
      "Successfully installed imageio-2.34.1 lazy-loader-0.4 scikit-image-0.23.2 scipy-1.13.0 tifffile-2024.4.24\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyvirtualdisplay\n",
      "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.24.1)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827620 sha256=ed4b1c0c98a27854272f5c7237e6d9bd1ac4bf63b70705fcb5c5616f8b55c9c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: pyvirtualdisplay, gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8 pyvirtualdisplay-3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "/bin/bash: line 1: sudo: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (68.2.2)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
      "Installing collected packages: setuptools\n",
      "Successfully installed setuptools-69.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting ez_setup\n",
      "  Downloading ez_setup-0.9.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ez_setup\n",
      "  Building wheel for ez_setup (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ez_setup: filename=ez_setup-0.9-py3-none-any.whl size=10994 sha256=9edb6422100316ca0202898ebbbf78086e4d30cd7136ecf8d60b41d5f728c8bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/d6/77/8f495e85fb7df23d41c328b9ea3cf0d9e83631b20bba479293\n",
      "Successfully built ez_setup\n",
      "Installing collected packages: ez_setup\n",
      "Successfully installed ez_setup-0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
      "Collecting ale-py~=0.8.0 (from gym[atari])\n",
      "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting importlib-resources (from ale-py~=0.8.0->gym[atari])\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (4.4.0)\n",
      "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: importlib-resources, ale-py\n",
      "Successfully installed ale-py-0.8.1 importlib-resources-6.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
      "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license])\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license])\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.31.0)\n",
      "Collecting tqdm (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license])\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license])\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2022.12.7)\n",
      "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=1dc7b7498445905b2493a988eb4336ceec693f8c7ea9224fdc93f424f331e705\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: tqdm, click, AutoROM.accept-rom-license, autorom\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2 click-8.1.7 tqdm-4.66.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 4.0   memory length: 293   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 4.0\n",
      "episode: 1   score: 1.0   memory length: 462   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 2   score: 2.0   memory length: 678   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 2.3333333333333335\n",
      "episode: 3   score: 2.0   memory length: 895   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 4   score: 2.0   memory length: 1093   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 5   score: 5.0   memory length: 1422   epsilon: 1.0    steps: 329    lr: 0.0001     evaluation reward: 2.6666666666666665\n",
      "episode: 6   score: 2.0   memory length: 1639   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 2.5714285714285716\n",
      "episode: 7   score: 2.0   memory length: 1857   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 8   score: 0.0   memory length: 1980   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.2222222222222223\n",
      "episode: 9   score: 0.0   memory length: 2102   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 10   score: 0.0   memory length: 2225   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8181818181818181\n",
      "episode: 11   score: 0.0   memory length: 2348   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 12   score: 4.0   memory length: 2643   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.8461538461538463\n",
      "episode: 13   score: 2.0   memory length: 2866   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.8571428571428572\n",
      "episode: 14   score: 2.0   memory length: 3064   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8666666666666667\n",
      "episode: 15   score: 1.0   memory length: 3234   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.8125\n",
      "episode: 16   score: 2.0   memory length: 3450   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.8235294117647058\n",
      "episode: 17   score: 0.0   memory length: 3573   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7222222222222223\n",
      "episode: 18   score: 1.0   memory length: 3724   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6842105263157894\n",
      "episode: 19   score: 4.0   memory length: 4038   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 20   score: 2.0   memory length: 4235   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8095238095238095\n",
      "episode: 21   score: 3.0   memory length: 4481   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.8636363636363635\n",
      "episode: 22   score: 2.0   memory length: 4679   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8695652173913044\n",
      "episode: 23   score: 2.0   memory length: 4876   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.875\n",
      "episode: 24   score: 0.0   memory length: 4999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 25   score: 0.0   memory length: 5122   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7307692307692308\n",
      "episode: 26   score: 4.0   memory length: 5415   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.8148148148148149\n",
      "episode: 27   score: 2.0   memory length: 5612   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8214285714285714\n",
      "episode: 28   score: 2.0   memory length: 5811   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.8275862068965518\n",
      "episode: 29   score: 0.0   memory length: 5934   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7666666666666666\n",
      "episode: 30   score: 0.0   memory length: 6057   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7096774193548387\n",
      "episode: 31   score: 2.0   memory length: 6256   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.71875\n",
      "episode: 32   score: 0.0   memory length: 6378   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 33   score: 2.0   memory length: 6576   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6764705882352942\n",
      "episode: 34   score: 0.0   memory length: 6699   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6285714285714286\n",
      "episode: 35   score: 3.0   memory length: 6929   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 36   score: 1.0   memory length: 7080   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6486486486486487\n",
      "episode: 37   score: 1.0   memory length: 7249   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.631578947368421\n",
      "episode: 38   score: 2.0   memory length: 7447   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.641025641025641\n",
      "episode: 39   score: 1.0   memory length: 7616   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 40   score: 1.0   memory length: 7786   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6097560975609757\n",
      "episode: 41   score: 1.0   memory length: 7936   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.5952380952380953\n",
      "episode: 42   score: 0.0   memory length: 8059   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.558139534883721\n",
      "episode: 43   score: 0.0   memory length: 8181   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5227272727272727\n",
      "episode: 44   score: 1.0   memory length: 8351   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.511111111111111\n",
      "episode: 45   score: 3.0   memory length: 8596   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5434782608695652\n",
      "episode: 46   score: 1.0   memory length: 8766   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5319148936170213\n",
      "episode: 47   score: 0.0   memory length: 8888   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 48   score: 3.0   memory length: 9156   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.530612244897959\n",
      "episode: 49   score: 1.0   memory length: 9325   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 50   score: 3.0   memory length: 9591   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.5490196078431373\n",
      "episode: 51   score: 0.0   memory length: 9714   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5192307692307692\n",
      "episode: 52   score: 2.0   memory length: 9912   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.528301886792453\n",
      "episode: 53   score: 2.0   memory length: 10130   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.537037037037037\n",
      "episode: 54   score: 0.0   memory length: 10253   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.509090909090909\n",
      "episode: 55   score: 2.0   memory length: 10451   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5178571428571428\n",
      "episode: 56   score: 0.0   memory length: 10573   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4912280701754386\n",
      "episode: 57   score: 1.0   memory length: 10723   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4827586206896552\n",
      "episode: 58   score: 2.0   memory length: 10920   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4915254237288136\n",
      "episode: 59   score: 2.0   memory length: 11119   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 60   score: 2.0   memory length: 11317   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5081967213114753\n",
      "episode: 61   score: 0.0   memory length: 11439   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4838709677419355\n",
      "episode: 62   score: 0.0   memory length: 11562   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4603174603174602\n",
      "episode: 63   score: 1.0   memory length: 11731   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.453125\n",
      "episode: 64   score: 1.0   memory length: 11882   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4461538461538461\n",
      "episode: 65   score: 2.0   memory length: 12080   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 66   score: 0.0   memory length: 12203   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4328358208955223\n",
      "episode: 67   score: 1.0   memory length: 12372   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4264705882352942\n",
      "episode: 68   score: 1.0   memory length: 12522   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4202898550724639\n",
      "episode: 69   score: 0.0   memory length: 12644   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 70   score: 2.0   memory length: 12842   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.408450704225352\n",
      "episode: 71   score: 2.0   memory length: 13061   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 72   score: 2.0   memory length: 13259   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4246575342465753\n",
      "episode: 73   score: 3.0   memory length: 13506   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.445945945945946\n",
      "episode: 74   score: 1.0   memory length: 13657   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 75   score: 3.0   memory length: 13904   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4605263157894737\n",
      "episode: 76   score: 0.0   memory length: 14026   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4415584415584415\n",
      "episode: 77   score: 1.0   memory length: 14195   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.435897435897436\n",
      "episode: 78   score: 0.0   memory length: 14318   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4177215189873418\n",
      "episode: 79   score: 2.0   memory length: 14515   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.425\n",
      "episode: 80   score: 2.0   memory length: 14713   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4320987654320987\n",
      "episode: 81   score: 4.0   memory length: 15009   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.4634146341463414\n",
      "episode: 82   score: 0.0   memory length: 15132   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4457831325301205\n",
      "episode: 83   score: 1.0   memory length: 15300   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4404761904761905\n",
      "episode: 84   score: 1.0   memory length: 15450   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4352941176470588\n",
      "episode: 85   score: 1.0   memory length: 15619   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.430232558139535\n",
      "episode: 86   score: 0.0   memory length: 15742   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4137931034482758\n",
      "episode: 87   score: 0.0   memory length: 15865   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3977272727272727\n",
      "episode: 88   score: 2.0   memory length: 16083   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.404494382022472\n",
      "episode: 89   score: 0.0   memory length: 16206   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3888888888888888\n",
      "episode: 90   score: 1.0   memory length: 16356   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.3846153846153846\n",
      "episode: 91   score: 1.0   memory length: 16507   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3804347826086956\n",
      "episode: 92   score: 0.0   memory length: 16629   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3655913978494623\n",
      "episode: 93   score: 2.0   memory length: 16827   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.372340425531915\n",
      "episode: 94   score: 4.0   memory length: 17120   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 95   score: 0.0   memory length: 17243   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3854166666666667\n",
      "episode: 96   score: 8.0   memory length: 17687   epsilon: 1.0    steps: 444    lr: 0.0001     evaluation reward: 1.4536082474226804\n",
      "episode: 97   score: 1.0   memory length: 17857   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4489795918367347\n",
      "episode: 98   score: 2.0   memory length: 18059   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 99   score: 4.0   memory length: 18353   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 100   score: 5.0   memory length: 18686   epsilon: 1.0    steps: 333    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 101   score: 3.0   memory length: 18950   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 102   score: 0.0   memory length: 19073   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 103   score: 2.0   memory length: 19273   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 104   score: 3.0   memory length: 19544   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 105   score: 0.0   memory length: 19667   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 106   score: 3.0   memory length: 19913   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 107   score: 1.0   memory length: 20064   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 108   score: 0.0   memory length: 20187   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 109   score: 2.0   memory length: 20407   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 110   score: 1.0   memory length: 20558   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 111   score: 2.0   memory length: 20776   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 112   score: 0.0   memory length: 20899   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 113   score: 0.0   memory length: 21022   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 114   score: 1.0   memory length: 21191   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 115   score: 2.0   memory length: 21388   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 116   score: 2.0   memory length: 21586   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 117   score: 2.0   memory length: 21784   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 118   score: 4.0   memory length: 22035   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 119   score: 0.0   memory length: 22158   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 120   score: 4.0   memory length: 22415   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 121   score: 2.0   memory length: 22613   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 122   score: 2.0   memory length: 22810   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 123   score: 3.0   memory length: 23038   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 124   score: 0.0   memory length: 23160   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 125   score: 0.0   memory length: 23282   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 126   score: 3.0   memory length: 23529   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 127   score: 0.0   memory length: 23651   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 128   score: 3.0   memory length: 23877   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 129   score: 0.0   memory length: 24000   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 130   score: 2.0   memory length: 24198   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 131   score: 1.0   memory length: 24367   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 132   score: 2.0   memory length: 24565   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 133   score: 0.0   memory length: 24688   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 134   score: 2.0   memory length: 24886   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 135   score: 2.0   memory length: 25101   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 136   score: 0.0   memory length: 25224   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 137   score: 1.0   memory length: 25393   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 138   score: 0.0   memory length: 25516   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 139   score: 0.0   memory length: 25638   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 140   score: 0.0   memory length: 25761   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 141   score: 0.0   memory length: 25884   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 142   score: 1.0   memory length: 26034   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 143   score: 0.0   memory length: 26157   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 144   score: 2.0   memory length: 26355   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 145   score: 1.0   memory length: 26524   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 146   score: 5.0   memory length: 26833   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 147   score: 2.0   memory length: 27031   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 148   score: 0.0   memory length: 27154   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 149   score: 1.0   memory length: 27305   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 150   score: 0.0   memory length: 27428   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 151   score: 2.0   memory length: 27648   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 152   score: 2.0   memory length: 27846   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 153   score: 3.0   memory length: 28059   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 154   score: 2.0   memory length: 28274   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 155   score: 0.0   memory length: 28397   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 156   score: 6.0   memory length: 28774   epsilon: 1.0    steps: 377    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 157   score: 1.0   memory length: 28925   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 158   score: 4.0   memory length: 29205   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 159   score: 1.0   memory length: 29374   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 160   score: 2.0   memory length: 29592   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 161   score: 1.0   memory length: 29763   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 162   score: 1.0   memory length: 29914   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 163   score: 2.0   memory length: 30112   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 164   score: 3.0   memory length: 30357   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 165   score: 1.0   memory length: 30528   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 166   score: 0.0   memory length: 30650   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 167   score: 1.0   memory length: 30822   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 168   score: 5.0   memory length: 31121   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 169   score: 2.0   memory length: 31339   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 170   score: 1.0   memory length: 31508   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 171   score: 0.0   memory length: 31630   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 172   score: 1.0   memory length: 31798   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 173   score: 2.0   memory length: 32015   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 174   score: 2.0   memory length: 32232   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 175   score: 4.0   memory length: 32508   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 176   score: 0.0   memory length: 32631   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 177   score: 2.0   memory length: 32829   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 178   score: 5.0   memory length: 33172   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 179   score: 2.0   memory length: 33352   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 180   score: 1.0   memory length: 33521   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 181   score: 1.0   memory length: 33693   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 182   score: 2.0   memory length: 33890   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 183   score: 3.0   memory length: 34159   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 184   score: 2.0   memory length: 34378   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 185   score: 0.0   memory length: 34501   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 186   score: 0.0   memory length: 34624   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 187   score: 0.0   memory length: 34747   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 188   score: 0.0   memory length: 34870   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 189   score: 0.0   memory length: 34993   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 190   score: 0.0   memory length: 35116   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 191   score: 4.0   memory length: 35430   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 192   score: 1.0   memory length: 35599   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 193   score: 0.0   memory length: 35722   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 194   score: 3.0   memory length: 35972   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 195   score: 3.0   memory length: 36205   epsilon: 1.0    steps: 233    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 196   score: 3.0   memory length: 36434   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 197   score: 3.0   memory length: 36680   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 198   score: 2.0   memory length: 36883   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 199   score: 1.0   memory length: 37053   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 200   score: 0.0   memory length: 37176   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 201   score: 2.0   memory length: 37373   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 202   score: 1.0   memory length: 37542   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 203   score: 1.0   memory length: 37714   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 204   score: 0.0   memory length: 37836   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 205   score: 2.0   memory length: 38033   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 206   score: 2.0   memory length: 38231   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 207   score: 3.0   memory length: 38495   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 208   score: 3.0   memory length: 38721   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 209   score: 4.0   memory length: 39036   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 210   score: 1.0   memory length: 39205   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 211   score: 3.0   memory length: 39452   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 212   score: 0.0   memory length: 39574   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 213   score: 1.0   memory length: 39742   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 214   score: 0.0   memory length: 39864   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 215   score: 0.0   memory length: 39986   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 216   score: 1.0   memory length: 40137   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 217   score: 6.0   memory length: 40494   epsilon: 1.0    steps: 357    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 218   score: 0.0   memory length: 40617   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 219   score: 2.0   memory length: 40820   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 220   score: 2.0   memory length: 41038   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 221   score: 2.0   memory length: 41257   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 222   score: 2.0   memory length: 41475   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 223   score: 4.0   memory length: 41753   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 224   score: 3.0   memory length: 42001   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 225   score: 0.0   memory length: 42124   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 226   score: 1.0   memory length: 42293   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 227   score: 1.0   memory length: 42461   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 228   score: 0.0   memory length: 42584   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 229   score: 3.0   memory length: 42832   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 230   score: 0.0   memory length: 42954   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 231   score: 3.0   memory length: 43200   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 232   score: 0.0   memory length: 43322   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 233   score: 0.0   memory length: 43445   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 234   score: 4.0   memory length: 43742   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 235   score: 3.0   memory length: 43969   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 236   score: 0.0   memory length: 44092   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 237   score: 5.0   memory length: 44438   epsilon: 1.0    steps: 346    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 238   score: 3.0   memory length: 44668   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 239   score: 2.0   memory length: 44866   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 240   score: 1.0   memory length: 45016   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 241   score: 1.0   memory length: 45186   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 242   score: 1.0   memory length: 45336   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 243   score: 1.0   memory length: 45504   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 244   score: 0.0   memory length: 45626   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 245   score: 2.0   memory length: 45844   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 246   score: 1.0   memory length: 45994   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 247   score: 2.0   memory length: 46192   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 248   score: 0.0   memory length: 46315   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 249   score: 3.0   memory length: 46541   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 250   score: 1.0   memory length: 46692   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 251   score: 2.0   memory length: 46890   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 252   score: 3.0   memory length: 47136   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 253   score: 0.0   memory length: 47259   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 254   score: 3.0   memory length: 47525   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 255   score: 2.0   memory length: 47743   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 256   score: 2.0   memory length: 47940   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 257   score: 1.0   memory length: 48109   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 258   score: 0.0   memory length: 48232   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 259   score: 2.0   memory length: 48431   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 260   score: 1.0   memory length: 48582   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 261   score: 0.0   memory length: 48705   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 262   score: 0.0   memory length: 48827   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 263   score: 0.0   memory length: 48949   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 264   score: 1.0   memory length: 49100   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 265   score: 2.0   memory length: 49318   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 266   score: 2.0   memory length: 49516   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 267   score: 1.0   memory length: 49667   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 268   score: 1.0   memory length: 49836   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 269   score: 3.0   memory length: 50065   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 270   score: 3.0   memory length: 50329   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 271   score: 2.0   memory length: 50546   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 272   score: 1.0   memory length: 50714   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 273   score: 2.0   memory length: 50914   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 274   score: 2.0   memory length: 51112   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 275   score: 4.0   memory length: 51406   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 276   score: 5.0   memory length: 51742   epsilon: 1.0    steps: 336    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 277   score: 0.0   memory length: 51865   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 278   score: 2.0   memory length: 52081   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 279   score: 3.0   memory length: 52344   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 280   score: 0.0   memory length: 52467   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 281   score: 1.0   memory length: 52636   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 282   score: 3.0   memory length: 52861   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 283   score: 1.0   memory length: 53032   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 284   score: 1.0   memory length: 53203   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 285   score: 2.0   memory length: 53421   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 286   score: 3.0   memory length: 53669   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 287   score: 1.0   memory length: 53837   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 288   score: 2.0   memory length: 54055   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 289   score: 1.0   memory length: 54225   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 290   score: 4.0   memory length: 54519   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 291   score: 1.0   memory length: 54688   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 292   score: 1.0   memory length: 54857   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 293   score: 1.0   memory length: 55008   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 294   score: 0.0   memory length: 55131   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 295   score: 1.0   memory length: 55282   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 296   score: 2.0   memory length: 55501   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 297   score: 1.0   memory length: 55671   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 298   score: 2.0   memory length: 55851   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 299   score: 3.0   memory length: 56079   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 300   score: 1.0   memory length: 56248   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 301   score: 2.0   memory length: 56448   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 302   score: 1.0   memory length: 56619   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 303   score: 0.0   memory length: 56741   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 304   score: 5.0   memory length: 57084   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 305   score: 1.0   memory length: 57253   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 306   score: 2.0   memory length: 57451   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 307   score: 2.0   memory length: 57651   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 308   score: 0.0   memory length: 57774   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 309   score: 2.0   memory length: 57971   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 310   score: 2.0   memory length: 58188   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 311   score: 0.0   memory length: 58311   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 312   score: 0.0   memory length: 58433   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 313   score: 3.0   memory length: 58662   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 314   score: 1.0   memory length: 58813   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 315   score: 0.0   memory length: 58936   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 316   score: 2.0   memory length: 59133   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 317   score: 0.0   memory length: 59255   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 318   score: 1.0   memory length: 59405   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 319   score: 2.0   memory length: 59602   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 320   score: 0.0   memory length: 59725   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 321   score: 1.0   memory length: 59894   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 322   score: 1.0   memory length: 60045   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 323   score: 1.0   memory length: 60196   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 324   score: 0.0   memory length: 60319   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 325   score: 0.0   memory length: 60441   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 326   score: 1.0   memory length: 60612   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 327   score: 2.0   memory length: 60811   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 328   score: 2.0   memory length: 61032   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 329   score: 1.0   memory length: 61201   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 330   score: 2.0   memory length: 61398   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 331   score: 2.0   memory length: 61595   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 332   score: 2.0   memory length: 61810   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 333   score: 1.0   memory length: 61979   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 334   score: 0.0   memory length: 62102   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 335   score: 3.0   memory length: 62349   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 336   score: 3.0   memory length: 62596   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 337   score: 0.0   memory length: 62719   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 338   score: 2.0   memory length: 62938   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 339   score: 0.0   memory length: 63060   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 340   score: 3.0   memory length: 63289   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 341   score: 0.0   memory length: 63412   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 342   score: 2.0   memory length: 63612   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 343   score: 0.0   memory length: 63734   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 344   score: 0.0   memory length: 63857   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 345   score: 2.0   memory length: 64055   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 346   score: 0.0   memory length: 64177   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 347   score: 4.0   memory length: 64492   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 348   score: 2.0   memory length: 64710   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 349   score: 2.0   memory length: 64908   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 350   score: 0.0   memory length: 65031   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 351   score: 0.0   memory length: 65154   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 352   score: 0.0   memory length: 65276   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 353   score: 1.0   memory length: 65445   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 354   score: 0.0   memory length: 65568   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 355   score: 0.0   memory length: 65690   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 356   score: 0.0   memory length: 65812   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 357   score: 1.0   memory length: 65982   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 358   score: 0.0   memory length: 66105   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 359   score: 1.0   memory length: 66256   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 360   score: 0.0   memory length: 66379   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 361   score: 8.0   memory length: 66724   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 362   score: 3.0   memory length: 66950   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 363   score: 2.0   memory length: 67131   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 364   score: 0.0   memory length: 67254   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 365   score: 0.0   memory length: 67377   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 366   score: 3.0   memory length: 67647   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 367   score: 1.0   memory length: 67798   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 368   score: 1.0   memory length: 67967   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 369   score: 0.0   memory length: 68089   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 370   score: 3.0   memory length: 68355   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 371   score: 0.0   memory length: 68478   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 372   score: 5.0   memory length: 68800   epsilon: 1.0    steps: 322    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 373   score: 2.0   memory length: 68998   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 374   score: 3.0   memory length: 69260   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 375   score: 0.0   memory length: 69382   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 376   score: 0.0   memory length: 69505   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 377   score: 0.0   memory length: 69628   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 378   score: 0.0   memory length: 69750   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 379   score: 4.0   memory length: 70027   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 380   score: 0.0   memory length: 70150   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 381   score: 1.0   memory length: 70318   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 382   score: 1.0   memory length: 70469   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 383   score: 0.0   memory length: 70591   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 384   score: 4.0   memory length: 70868   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 385   score: 2.0   memory length: 71086   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 386   score: 4.0   memory length: 71384   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 387   score: 3.0   memory length: 71613   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 388   score: 0.0   memory length: 71736   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 389   score: 2.0   memory length: 71936   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 390   score: 2.0   memory length: 72151   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 391   score: 1.0   memory length: 72302   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 392   score: 1.0   memory length: 72453   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 393   score: 0.0   memory length: 72575   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 394   score: 2.0   memory length: 72793   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 395   score: 3.0   memory length: 73021   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 396   score: 1.0   memory length: 73171   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 397   score: 0.0   memory length: 73293   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 398   score: 1.0   memory length: 73461   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 399   score: 3.0   memory length: 73709   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 400   score: 2.0   memory length: 73909   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 401   score: 0.0   memory length: 74032   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 402   score: 3.0   memory length: 74278   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 403   score: 0.0   memory length: 74400   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 404   score: 0.0   memory length: 74522   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 405   score: 2.0   memory length: 74720   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 406   score: 0.0   memory length: 74842   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 407   score: 1.0   memory length: 74992   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 408   score: 1.0   memory length: 75163   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 409   score: 2.0   memory length: 75361   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 410   score: 2.0   memory length: 75582   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 411   score: 0.0   memory length: 75704   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 412   score: 3.0   memory length: 75933   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 413   score: 1.0   memory length: 76101   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 414   score: 2.0   memory length: 76319   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 415   score: 0.0   memory length: 76441   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 416   score: 0.0   memory length: 76564   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 417   score: 3.0   memory length: 76813   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 418   score: 3.0   memory length: 77039   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 419   score: 2.0   memory length: 77236   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 420   score: 1.0   memory length: 77386   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 421   score: 2.0   memory length: 77605   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 422   score: 1.0   memory length: 77774   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 423   score: 3.0   memory length: 78020   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 424   score: 5.0   memory length: 78384   epsilon: 1.0    steps: 364    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 425   score: 2.0   memory length: 78599   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 426   score: 0.0   memory length: 78722   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 427   score: 1.0   memory length: 78891   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 428   score: 1.0   memory length: 79041   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 429   score: 3.0   memory length: 79266   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 430   score: 1.0   memory length: 79416   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 431   score: 0.0   memory length: 79539   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 432   score: 3.0   memory length: 79765   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 433   score: 6.0   memory length: 80131   epsilon: 1.0    steps: 366    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 434   score: 5.0   memory length: 80463   epsilon: 1.0    steps: 332    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 435   score: 3.0   memory length: 80707   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 436   score: 1.0   memory length: 80877   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 437   score: 1.0   memory length: 81047   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 438   score: 1.0   memory length: 81219   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 439   score: 0.0   memory length: 81342   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 440   score: 5.0   memory length: 81669   epsilon: 1.0    steps: 327    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 441   score: 4.0   memory length: 81966   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 442   score: 1.0   memory length: 82135   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 443   score: 2.0   memory length: 82353   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 444   score: 0.0   memory length: 82475   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 445   score: 0.0   memory length: 82597   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 446   score: 3.0   memory length: 82844   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 447   score: 0.0   memory length: 82966   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 448   score: 1.0   memory length: 83135   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 449   score: 0.0   memory length: 83258   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 450   score: 2.0   memory length: 83456   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 451   score: 1.0   memory length: 83625   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 452   score: 2.0   memory length: 83842   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 453   score: 1.0   memory length: 84012   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 454   score: 0.0   memory length: 84134   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 455   score: 1.0   memory length: 84304   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 456   score: 5.0   memory length: 84627   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 457   score: 2.0   memory length: 84846   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 458   score: 2.0   memory length: 85064   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 459   score: 3.0   memory length: 85329   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 460   score: 3.0   memory length: 85576   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 461   score: 1.0   memory length: 85727   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 462   score: 3.0   memory length: 85955   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 463   score: 2.0   memory length: 86153   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 464   score: 1.0   memory length: 86322   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 465   score: 3.0   memory length: 86549   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 466   score: 3.0   memory length: 86780   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 467   score: 0.0   memory length: 86903   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 468   score: 1.0   memory length: 87073   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 469   score: 0.0   memory length: 87196   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 470   score: 1.0   memory length: 87365   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 471   score: 1.0   memory length: 87516   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 472   score: 3.0   memory length: 87781   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 473   score: 0.0   memory length: 87904   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 474   score: 0.0   memory length: 88026   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 475   score: 0.0   memory length: 88149   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 476   score: 2.0   memory length: 88349   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 477   score: 4.0   memory length: 88644   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 478   score: 2.0   memory length: 88826   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 479   score: 3.0   memory length: 89052   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 480   score: 0.0   memory length: 89175   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 481   score: 0.0   memory length: 89298   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 482   score: 2.0   memory length: 89496   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 483   score: 0.0   memory length: 89619   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 484   score: 4.0   memory length: 89894   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 485   score: 1.0   memory length: 90063   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 486   score: 0.0   memory length: 90185   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 487   score: 0.0   memory length: 90308   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 488   score: 0.0   memory length: 90431   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 489   score: 3.0   memory length: 90658   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 490   score: 2.0   memory length: 90840   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 491   score: 0.0   memory length: 90963   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 492   score: 0.0   memory length: 91086   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 493   score: 2.0   memory length: 91285   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 494   score: 2.0   memory length: 91487   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 495   score: 2.0   memory length: 91685   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 496   score: 0.0   memory length: 91807   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 497   score: 0.0   memory length: 91930   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 498   score: 3.0   memory length: 92161   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 499   score: 1.0   memory length: 92330   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 500   score: 1.0   memory length: 92499   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 501   score: 1.0   memory length: 92649   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 502   score: 1.0   memory length: 92817   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 503   score: 2.0   memory length: 93037   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 504   score: 4.0   memory length: 93356   epsilon: 1.0    steps: 319    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 505   score: 2.0   memory length: 93578   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 506   score: 0.0   memory length: 93701   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 507   score: 3.0   memory length: 93927   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 508   score: 5.0   memory length: 94245   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 509   score: 0.0   memory length: 94367   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 510   score: 1.0   memory length: 94535   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 511   score: 0.0   memory length: 94657   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 512   score: 0.0   memory length: 94779   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 513   score: 1.0   memory length: 94929   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 514   score: 1.0   memory length: 95100   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 515   score: 1.0   memory length: 95268   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 516   score: 1.0   memory length: 95438   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 517   score: 1.0   memory length: 95608   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 518   score: 0.0   memory length: 95731   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 519   score: 0.0   memory length: 95854   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 520   score: 2.0   memory length: 96055   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 521   score: 2.0   memory length: 96253   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 522   score: 0.0   memory length: 96376   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 523   score: 0.0   memory length: 96499   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 524   score: 2.0   memory length: 96696   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 525   score: 1.0   memory length: 96868   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 526   score: 3.0   memory length: 97115   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 527   score: 0.0   memory length: 97238   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 528   score: 0.0   memory length: 97361   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 529   score: 1.0   memory length: 97532   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 530   score: 3.0   memory length: 97778   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 531   score: 0.0   memory length: 97901   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 532   score: 0.0   memory length: 98024   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 533   score: 1.0   memory length: 98176   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 534   score: 2.0   memory length: 98393   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 535   score: 0.0   memory length: 98516   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 536   score: 1.0   memory length: 98688   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 537   score: 0.0   memory length: 98811   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 538   score: 3.0   memory length: 99056   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 539   score: 4.0   memory length: 99313   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 540   score: 2.0   memory length: 99514   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 541   score: 1.0   memory length: 99683   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/assignment5_materials/memory.py:30: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sample = np.array(sample, dtype=object)\n",
      "/workspace/assignment5_materials/agent_double.py:70: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  mini_batch = np.array(mini_batch, dtype=object).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 542   score: 5.0   memory length: 100007   epsilon: 0.9999841600000003    steps: 324    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 543   score: 2.0   memory length: 100205   epsilon: 0.9995921200000089    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 544   score: 0.0   memory length: 100327   epsilon: 0.9993505600000141    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 545   score: 2.0   memory length: 100544   epsilon: 0.9989209000000234    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 546   score: 0.0   memory length: 100666   epsilon: 0.9986793400000287    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 547   score: 1.0   memory length: 100835   epsilon: 0.9983447200000359    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 548   score: 1.0   memory length: 100986   epsilon: 0.9980457400000424    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 549   score: 3.0   memory length: 101234   epsilon: 0.9975547000000531    steps: 248    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 550   score: 4.0   memory length: 101510   epsilon: 0.997008220000065    steps: 276    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 551   score: 1.0   memory length: 101660   epsilon: 0.9967112200000714    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 552   score: 0.0   memory length: 101782   epsilon: 0.9964696600000766    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 553   score: 1.0   memory length: 101953   epsilon: 0.996131080000084    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 554   score: 0.0   memory length: 102076   epsilon: 0.9958875400000893    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 555   score: 1.0   memory length: 102226   epsilon: 0.9955905400000957    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 556   score: 1.0   memory length: 102396   epsilon: 0.995253940000103    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 557   score: 3.0   memory length: 102623   epsilon: 0.9948044800001128    steps: 227    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 558   score: 2.0   memory length: 102821   epsilon: 0.9944124400001213    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 559   score: 2.0   memory length: 103040   epsilon: 0.9939788200001307    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 560   score: 0.0   memory length: 103163   epsilon: 0.993735280000136    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 561   score: 3.0   memory length: 103413   epsilon: 0.9932402800001467    steps: 250    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 562   score: 2.0   memory length: 103594   epsilon: 0.9928819000001545    steps: 181    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 563   score: 2.0   memory length: 103775   epsilon: 0.9925235200001623    steps: 181    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 564   score: 1.0   memory length: 103943   epsilon: 0.9921908800001695    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 565   score: 1.0   memory length: 104094   epsilon: 0.991891900000176    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 566   score: 2.0   memory length: 104292   epsilon: 0.9914998600001845    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 567   score: 1.0   memory length: 104443   epsilon: 0.991200880000191    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 568   score: 0.0   memory length: 104565   epsilon: 0.9909593200001963    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 569   score: 3.0   memory length: 104811   epsilon: 0.9904722400002068    steps: 246    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 570   score: 1.0   memory length: 104979   epsilon: 0.9901396000002141    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 571   score: 0.0   memory length: 105101   epsilon: 0.9898980400002193    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 572   score: 3.0   memory length: 105329   epsilon: 0.9894466000002291    steps: 228    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 573   score: 0.0   memory length: 105451   epsilon: 0.9892050400002343    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 574   score: 2.0   memory length: 105670   epsilon: 0.9887714200002438    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 575   score: 1.0   memory length: 105820   epsilon: 0.9884744200002502    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 576   score: 2.0   memory length: 106041   epsilon: 0.9880368400002597    steps: 221    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 577   score: 0.0   memory length: 106163   epsilon: 0.987795280000265    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 578   score: 3.0   memory length: 106410   epsilon: 0.9873062200002756    steps: 247    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 579   score: 2.0   memory length: 106590   epsilon: 0.9869498200002833    steps: 180    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 580   score: 0.0   memory length: 106713   epsilon: 0.9867062800002886    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 581   score: 2.0   memory length: 106911   epsilon: 0.9863142400002971    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 582   score: 1.0   memory length: 107080   epsilon: 0.9859796200003044    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 583   score: 2.0   memory length: 107278   epsilon: 0.9855875800003129    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 584   score: 1.0   memory length: 107428   epsilon: 0.9852905800003193    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 585   score: 2.0   memory length: 107626   epsilon: 0.9848985400003278    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 586   score: 0.0   memory length: 107749   epsilon: 0.9846550000003331    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 587   score: 3.0   memory length: 107975   epsilon: 0.9842075200003428    steps: 226    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 588   score: 1.0   memory length: 108147   epsilon: 0.9838669600003502    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 589   score: 0.0   memory length: 108270   epsilon: 0.9836234200003555    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 590   score: 2.0   memory length: 108489   epsilon: 0.9831898000003649    steps: 219    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 591   score: 1.0   memory length: 108639   epsilon: 0.9828928000003714    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 592   score: 1.0   memory length: 108807   epsilon: 0.9825601600003786    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 593   score: 0.0   memory length: 108929   epsilon: 0.9823186000003838    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 594   score: 0.0   memory length: 109051   epsilon: 0.9820770400003891    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 595   score: 2.0   memory length: 109248   epsilon: 0.9816869800003976    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 596   score: 2.0   memory length: 109469   epsilon: 0.9812494000004071    steps: 221    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 597   score: 2.0   memory length: 109687   epsilon: 0.9808177600004164    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 598   score: 2.0   memory length: 109903   epsilon: 0.9803900800004257    steps: 216    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 599   score: 0.0   memory length: 110026   epsilon: 0.980146540000431    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 600   score: 3.0   memory length: 110275   epsilon: 0.9796535200004417    steps: 249    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 601   score: 0.0   memory length: 110398   epsilon: 0.979409980000447    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 602   score: 0.0   memory length: 110520   epsilon: 0.9791684200004522    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 603   score: 4.0   memory length: 110808   epsilon: 0.9785981800004646    steps: 288    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 604   score: 1.0   memory length: 110976   epsilon: 0.9782655400004718    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 605   score: 3.0   memory length: 111243   epsilon: 0.9777368800004833    steps: 267    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 606   score: 2.0   memory length: 111459   epsilon: 0.9773092000004926    steps: 216    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 607   score: 0.0   memory length: 111581   epsilon: 0.9770676400004978    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 608   score: 0.0   memory length: 111703   epsilon: 0.9768260800005031    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 609   score: 1.0   memory length: 111875   epsilon: 0.9764855200005105    steps: 172    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 610   score: 0.0   memory length: 111997   epsilon: 0.9762439600005157    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 611   score: 5.0   memory length: 112322   epsilon: 0.9756004600005297    steps: 325    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 612   score: 1.0   memory length: 112473   epsilon: 0.9753014800005362    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 613   score: 1.0   memory length: 112643   epsilon: 0.9749648800005435    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 614   score: 3.0   memory length: 112913   epsilon: 0.9744302800005551    steps: 270    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 615   score: 0.0   memory length: 113035   epsilon: 0.9741887200005603    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 616   score: 0.0   memory length: 113158   epsilon: 0.9739451800005656    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 617   score: 0.0   memory length: 113280   epsilon: 0.9737036200005709    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 618   score: 1.0   memory length: 113431   epsilon: 0.9734046400005774    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 619   score: 0.0   memory length: 113553   epsilon: 0.9731630800005826    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 620   score: 1.0   memory length: 113705   epsilon: 0.9728621200005891    steps: 152    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 621   score: 1.0   memory length: 113874   epsilon: 0.9725275000005964    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 622   score: 2.0   memory length: 114092   epsilon: 0.9720958600006058    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 623   score: 0.0   memory length: 114215   epsilon: 0.9718523200006111    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 624   score: 0.0   memory length: 114338   epsilon: 0.9716087800006163    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 625   score: 5.0   memory length: 114660   epsilon: 0.9709712200006302    steps: 322    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 626   score: 1.0   memory length: 114829   epsilon: 0.9706366000006375    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 627   score: 2.0   memory length: 115027   epsilon: 0.970244560000646    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 628   score: 1.0   memory length: 115198   epsilon: 0.9699059800006533    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 629   score: 1.0   memory length: 115368   epsilon: 0.9695693800006606    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 630   score: 0.0   memory length: 115491   epsilon: 0.9693258400006659    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 631   score: 0.0   memory length: 115613   epsilon: 0.9690842800006711    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 632   score: 0.0   memory length: 115735   epsilon: 0.9688427200006764    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 633   score: 3.0   memory length: 115983   epsilon: 0.968351680000687    steps: 248    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 634   score: 0.0   memory length: 116106   epsilon: 0.9681081400006923    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 635   score: 4.0   memory length: 116377   epsilon: 0.967571560000704    steps: 271    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 636   score: 1.0   memory length: 116528   epsilon: 0.9672725800007105    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 637   score: 3.0   memory length: 116777   epsilon: 0.9667795600007212    steps: 249    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 638   score: 0.0   memory length: 116900   epsilon: 0.9665360200007265    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 639   score: 2.0   memory length: 117118   epsilon: 0.9661043800007358    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 640   score: 2.0   memory length: 117316   epsilon: 0.9657123400007444    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 641   score: 1.0   memory length: 117467   epsilon: 0.9654133600007508    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 642   score: 2.0   memory length: 117685   epsilon: 0.9649817200007602    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 643   score: 1.0   memory length: 117854   epsilon: 0.9646471000007675    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 644   score: 2.0   memory length: 118057   epsilon: 0.9642451600007762    steps: 203    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 645   score: 0.0   memory length: 118179   epsilon: 0.9640036000007814    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 646   score: 2.0   memory length: 118377   epsilon: 0.96361156000079    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 647   score: 0.0   memory length: 118499   epsilon: 0.9633700000007952    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 648   score: 3.0   memory length: 118708   epsilon: 0.9629561800008042    steps: 209    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 649   score: 4.0   memory length: 118983   epsilon: 0.962411680000816    steps: 275    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 650   score: 6.0   memory length: 119315   epsilon: 0.9617543200008303    steps: 332    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 651   score: 1.0   memory length: 119466   epsilon: 0.9614553400008368    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 652   score: 1.0   memory length: 119617   epsilon: 0.9611563600008433    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 653   score: 2.0   memory length: 119814   epsilon: 0.9607663000008517    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 654   score: 3.0   memory length: 120063   epsilon: 0.9602732800008624    steps: 249    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 655   score: 3.0   memory length: 120289   epsilon: 0.9598258000008721    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 656   score: 0.0   memory length: 120412   epsilon: 0.9595822600008774    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 657   score: 1.0   memory length: 120581   epsilon: 0.9592476400008847    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 658   score: 2.0   memory length: 120799   epsilon: 0.9588160000008941    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 659   score: 0.0   memory length: 120921   epsilon: 0.9585744400008993    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 660   score: 0.0   memory length: 121044   epsilon: 0.9583309000009046    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 661   score: 4.0   memory length: 121316   epsilon: 0.9577923400009163    steps: 272    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 662   score: 2.0   memory length: 121516   epsilon: 0.9573963400009249    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 663   score: 1.0   memory length: 121667   epsilon: 0.9570973600009314    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 664   score: 1.0   memory length: 121836   epsilon: 0.9567627400009386    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 665   score: 4.0   memory length: 122133   epsilon: 0.9561746800009514    steps: 297    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 666   score: 0.0   memory length: 122255   epsilon: 0.9559331200009566    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 667   score: 1.0   memory length: 122405   epsilon: 0.9556361200009631    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 668   score: 1.0   memory length: 122575   epsilon: 0.9552995200009704    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 669   score: 2.0   memory length: 122773   epsilon: 0.9549074800009789    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 670   score: 0.0   memory length: 122896   epsilon: 0.9546639400009842    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 671   score: 1.0   memory length: 123065   epsilon: 0.9543293200009915    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 672   score: 2.0   memory length: 123263   epsilon: 0.953937280001    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 673   score: 0.0   memory length: 123386   epsilon: 0.9536937400010053    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 674   score: 2.0   memory length: 123605   epsilon: 0.9532601200010147    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 675   score: 0.0   memory length: 123727   epsilon: 0.9530185600010199    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 676   score: 2.0   memory length: 123907   epsilon: 0.9526621600010277    steps: 180    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 677   score: 1.0   memory length: 124077   epsilon: 0.952325560001035    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 678   score: 0.0   memory length: 124199   epsilon: 0.9520840000010402    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 679   score: 1.0   memory length: 124371   epsilon: 0.9517434400010476    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 680   score: 1.0   memory length: 124540   epsilon: 0.9514088200010549    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 681   score: 3.0   memory length: 124789   epsilon: 0.9509158000010656    steps: 249    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 682   score: 1.0   memory length: 124959   epsilon: 0.9505792000010729    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 683   score: 0.0   memory length: 125082   epsilon: 0.9503356600010782    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 684   score: 2.0   memory length: 125281   epsilon: 0.9499416400010867    steps: 199    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 685   score: 0.0   memory length: 125404   epsilon: 0.949698100001092    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 686   score: 0.0   memory length: 125527   epsilon: 0.9494545600010973    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 687   score: 2.0   memory length: 125724   epsilon: 0.9490645000011058    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 688   score: 3.0   memory length: 125970   epsilon: 0.9485774200011163    steps: 246    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 689   score: 0.0   memory length: 126093   epsilon: 0.9483338800011216    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 690   score: 0.0   memory length: 126216   epsilon: 0.9480903400011269    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 691   score: 0.0   memory length: 126339   epsilon: 0.9478468000011322    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 692   score: 0.0   memory length: 126462   epsilon: 0.9476032600011375    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 693   score: 1.0   memory length: 126613   epsilon: 0.947304280001144    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 694   score: 1.0   memory length: 126763   epsilon: 0.9470072800011504    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 695   score: 2.0   memory length: 126962   epsilon: 0.946613260001159    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 696   score: 1.0   memory length: 127113   epsilon: 0.9463142800011655    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 697   score: 1.0   memory length: 127281   epsilon: 0.9459816400011727    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 698   score: 4.0   memory length: 127594   epsilon: 0.9453619000011861    steps: 313    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 699   score: 0.0   memory length: 127716   epsilon: 0.9451203400011914    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 700   score: 2.0   memory length: 127913   epsilon: 0.9447302800011999    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 701   score: 3.0   memory length: 128158   epsilon: 0.9442451800012104    steps: 245    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 702   score: 3.0   memory length: 128409   epsilon: 0.9437482000012212    steps: 251    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 703   score: 1.0   memory length: 128578   epsilon: 0.9434135800012284    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 704   score: 3.0   memory length: 128843   epsilon: 0.9428888800012398    steps: 265    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 705   score: 0.0   memory length: 128966   epsilon: 0.9426453400012451    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 706   score: 4.0   memory length: 129264   epsilon: 0.9420553000012579    steps: 298    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 707   score: 1.0   memory length: 129432   epsilon: 0.9417226600012651    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 708   score: 1.0   memory length: 129582   epsilon: 0.9414256600012716    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 709   score: 2.0   memory length: 129780   epsilon: 0.9410336200012801    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 710   score: 2.0   memory length: 129978   epsilon: 0.9406415800012886    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 711   score: 3.0   memory length: 130227   epsilon: 0.9401485600012993    steps: 249    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 712   score: 2.0   memory length: 130445   epsilon: 0.9397169200013087    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 713   score: 0.0   memory length: 130568   epsilon: 0.939473380001314    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 714   score: 0.0   memory length: 130691   epsilon: 0.9392298400013193    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 715   score: 2.0   memory length: 130888   epsilon: 0.9388397800013277    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 716   score: 1.0   memory length: 131057   epsilon: 0.938505160001335    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 717   score: 0.0   memory length: 131180   epsilon: 0.9382616200013403    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 718   score: 1.0   memory length: 131331   epsilon: 0.9379626400013468    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 719   score: 0.0   memory length: 131453   epsilon: 0.937721080001352    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 720   score: 1.0   memory length: 131622   epsilon: 0.9373864600013593    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 721   score: 3.0   memory length: 131869   epsilon: 0.9368974000013699    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 722   score: 4.0   memory length: 132143   epsilon: 0.9363548800013817    steps: 274    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 723   score: 2.0   memory length: 132323   epsilon: 0.9359984800013894    steps: 180    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 724   score: 2.0   memory length: 132522   epsilon: 0.935604460001398    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 725   score: 0.0   memory length: 132644   epsilon: 0.9353629000014032    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 726   score: 1.0   memory length: 132794   epsilon: 0.9350659000014097    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 727   score: 0.0   memory length: 132916   epsilon: 0.9348243400014149    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 728   score: 0.0   memory length: 133039   epsilon: 0.9345808000014202    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 729   score: 3.0   memory length: 133288   epsilon: 0.9340877800014309    steps: 249    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 730   score: 2.0   memory length: 133486   epsilon: 0.9336957400014394    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 731   score: 1.0   memory length: 133638   epsilon: 0.9333947800014459    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 732   score: 1.0   memory length: 133809   epsilon: 0.9330562000014533    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 733   score: 0.0   memory length: 133932   epsilon: 0.9328126600014586    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 734   score: 2.0   memory length: 134150   epsilon: 0.9323810200014679    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 735   score: 1.0   memory length: 134318   epsilon: 0.9320483800014752    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 736   score: 2.0   memory length: 134515   epsilon: 0.9316583200014836    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 737   score: 2.0   memory length: 134713   epsilon: 0.9312662800014921    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 738   score: 3.0   memory length: 134939   epsilon: 0.9308188000015019    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 739   score: 3.0   memory length: 135204   epsilon: 0.9302941000015132    steps: 265    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 740   score: 1.0   memory length: 135355   epsilon: 0.9299951200015197    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 741   score: 4.0   memory length: 135671   epsilon: 0.9293694400015333    steps: 316    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 742   score: 1.0   memory length: 135840   epsilon: 0.9290348200015406    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 743   score: 2.0   memory length: 136058   epsilon: 0.92860318000155    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 744   score: 2.0   memory length: 136256   epsilon: 0.9282111400015585    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 745   score: 2.0   memory length: 136474   epsilon: 0.9277795000015678    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 746   score: 3.0   memory length: 136700   epsilon: 0.9273320200015776    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 747   score: 2.0   memory length: 136879   epsilon: 0.9269776000015852    steps: 179    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 748   score: 0.0   memory length: 137002   epsilon: 0.9267340600015905    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 749   score: 1.0   memory length: 137171   epsilon: 0.9263994400015978    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 750   score: 1.0   memory length: 137321   epsilon: 0.9261024400016042    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 751   score: 8.0   memory length: 137615   epsilon: 0.9255203200016169    steps: 294    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 752   score: 2.0   memory length: 137833   epsilon: 0.9250886800016263    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 753   score: 1.0   memory length: 137983   epsilon: 0.9247916800016327    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 754   score: 2.0   memory length: 138180   epsilon: 0.9244016200016412    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 755   score: 0.0   memory length: 138303   epsilon: 0.9241580800016465    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 756   score: 1.0   memory length: 138454   epsilon: 0.9238591000016529    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 757   score: 5.0   memory length: 138782   epsilon: 0.923209660001667    steps: 328    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 758   score: 0.0   memory length: 138904   epsilon: 0.9229681000016723    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 759   score: 0.0   memory length: 139027   epsilon: 0.9227245600016776    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 760   score: 3.0   memory length: 139271   epsilon: 0.9222414400016881    steps: 244    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 761   score: 3.0   memory length: 139519   epsilon: 0.9217504000016987    steps: 248    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 762   score: 4.0   memory length: 139816   epsilon: 0.9211623400017115    steps: 297    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 763   score: 4.0   memory length: 140073   epsilon: 0.9206534800017225    steps: 257    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 764   score: 2.0   memory length: 140291   epsilon: 0.9202218400017319    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 765   score: 0.0   memory length: 140413   epsilon: 0.9199802800017371    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 766   score: 3.0   memory length: 140661   epsilon: 0.9194892400017478    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 767   score: 2.0   memory length: 140877   epsilon: 0.9190615600017571    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 768   score: 4.0   memory length: 141176   epsilon: 0.91846954000177    steps: 299    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 769   score: 2.0   memory length: 141374   epsilon: 0.9180775000017785    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 770   score: 3.0   memory length: 141602   epsilon: 0.9176260600017883    steps: 228    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 771   score: 2.0   memory length: 141800   epsilon: 0.9172340200017968    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 772   score: 0.0   memory length: 141922   epsilon: 0.916992460001802    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 773   score: 2.0   memory length: 142138   epsilon: 0.9165647800018113    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 774   score: 2.0   memory length: 142355   epsilon: 0.9161351200018206    steps: 217    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 775   score: 1.0   memory length: 142505   epsilon: 0.9158381200018271    steps: 150    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 776   score: 2.0   memory length: 142724   epsilon: 0.9154045000018365    steps: 219    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 777   score: 3.0   memory length: 142991   epsilon: 0.914875840001848    steps: 267    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 778   score: 0.0   memory length: 143113   epsilon: 0.9146342800018532    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 779   score: 4.0   memory length: 143409   epsilon: 0.9140482000018659    steps: 296    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 780   score: 1.0   memory length: 143559   epsilon: 0.9137512000018724    steps: 150    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 781   score: 1.0   memory length: 143730   epsilon: 0.9134126200018797    steps: 171    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 782   score: 0.0   memory length: 143853   epsilon: 0.913169080001885    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 783   score: 2.0   memory length: 144050   epsilon: 0.9127790200018935    steps: 197    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 784   score: 0.0   memory length: 144172   epsilon: 0.9125374600018987    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 785   score: 4.0   memory length: 144448   epsilon: 0.9119909800019106    steps: 276    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 786   score: 1.0   memory length: 144617   epsilon: 0.9116563600019179    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 787   score: 3.0   memory length: 144884   epsilon: 0.9111277000019293    steps: 267    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 788   score: 2.0   memory length: 145082   epsilon: 0.9107356600019378    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 789   score: 4.0   memory length: 145377   epsilon: 0.9101515600019505    steps: 295    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 790   score: 2.0   memory length: 145575   epsilon: 0.909759520001959    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 791   score: 4.0   memory length: 145891   epsilon: 0.9091338400019726    steps: 316    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 792   score: 2.0   memory length: 146109   epsilon: 0.908702200001982    steps: 218    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 793   score: 2.0   memory length: 146325   epsilon: 0.9082745200019913    steps: 216    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 794   score: 0.0   memory length: 146447   epsilon: 0.9080329600019965    steps: 122    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 795   score: 1.0   memory length: 146598   epsilon: 0.907733980002003    steps: 151    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 796   score: 2.0   memory length: 146778   epsilon: 0.9073775800020107    steps: 180    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 797   score: 2.0   memory length: 146996   epsilon: 0.9069459400020201    steps: 218    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 798   score: 1.0   memory length: 147165   epsilon: 0.9066113200020274    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 799   score: 3.0   memory length: 147376   epsilon: 0.9061935400020364    steps: 211    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 800   score: 1.0   memory length: 147548   epsilon: 0.9058529800020438    steps: 172    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 801   score: 1.0   memory length: 147719   epsilon: 0.9055144000020512    steps: 171    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 802   score: 1.0   memory length: 147870   epsilon: 0.9052154200020577    steps: 151    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 803   score: 0.0   memory length: 147993   epsilon: 0.904971880002063    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 804   score: 3.0   memory length: 148219   epsilon: 0.9045244000020727    steps: 226    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 805   score: 2.0   memory length: 148419   epsilon: 0.9041284000020813    steps: 200    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 806   score: 1.0   memory length: 148591   epsilon: 0.9037878400020887    steps: 172    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 807   score: 3.0   memory length: 148802   epsilon: 0.9033700600020977    steps: 211    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 808   score: 3.0   memory length: 149048   epsilon: 0.9028829800021083    steps: 246    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 809   score: 0.0   memory length: 149171   epsilon: 0.9026394400021136    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 810   score: 3.0   memory length: 149417   epsilon: 0.9021523600021242    steps: 246    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 811   score: 0.0   memory length: 149539   epsilon: 0.9019108000021294    steps: 122    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 812   score: 0.0   memory length: 149662   epsilon: 0.9016672600021347    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 813   score: 3.0   memory length: 149887   epsilon: 0.9012217600021444    steps: 225    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 814   score: 3.0   memory length: 150100   epsilon: 0.9008000200021535    steps: 213    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 815   score: 1.0   memory length: 150251   epsilon: 0.90050104000216    steps: 151    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 816   score: 1.0   memory length: 150420   epsilon: 0.9001664200021673    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 817   score: 2.0   memory length: 150637   epsilon: 0.8997367600021766    steps: 217    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 818   score: 3.0   memory length: 150883   epsilon: 0.8992496800021872    steps: 246    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 819   score: 1.0   memory length: 151055   epsilon: 0.8989091200021946    steps: 172    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 820   score: 5.0   memory length: 151386   epsilon: 0.8982537400022088    steps: 331    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 821   score: 2.0   memory length: 151605   epsilon: 0.8978201200022182    steps: 219    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 822   score: 1.0   memory length: 151756   epsilon: 0.8975211400022247    steps: 151    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 823   score: 3.0   memory length: 151982   epsilon: 0.8970736600022344    steps: 226    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 824   score: 1.0   memory length: 152151   epsilon: 0.8967390400022417    steps: 169    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 825   score: 5.0   memory length: 152475   epsilon: 0.8960975200022556    steps: 324    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 826   score: 1.0   memory length: 152626   epsilon: 0.8957985400022621    steps: 151    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 827   score: 2.0   memory length: 152828   epsilon: 0.8953985800022708    steps: 202    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 828   score: 0.0   memory length: 152951   epsilon: 0.8951550400022761    steps: 123    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 829   score: 2.0   memory length: 153132   epsilon: 0.8947966600022839    steps: 181    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 830   score: 2.0   memory length: 153349   epsilon: 0.8943670000022932    steps: 217    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 831   score: 1.0   memory length: 153499   epsilon: 0.8940700000022996    steps: 150    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 832   score: 4.0   memory length: 153813   epsilon: 0.8934482800023131    steps: 314    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 833   score: 2.0   memory length: 153995   epsilon: 0.893087920002321    steps: 182    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 834   score: 4.0   memory length: 154269   epsilon: 0.8925454000023327    steps: 274    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 835   score: 0.0   memory length: 154392   epsilon: 0.892301860002338    steps: 123    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 836   score: 0.0   memory length: 154515   epsilon: 0.8920583200023433    steps: 123    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 837   score: 0.0   memory length: 154638   epsilon: 0.8918147800023486    steps: 123    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 838   score: 1.0   memory length: 154810   epsilon: 0.891474220002356    steps: 172    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 839   score: 1.0   memory length: 154961   epsilon: 0.8911752400023625    steps: 151    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 840   score: 2.0   memory length: 155143   epsilon: 0.8908148800023703    steps: 182    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 841   score: 1.0   memory length: 155294   epsilon: 0.8905159000023768    steps: 151    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 842   score: 2.0   memory length: 155511   epsilon: 0.8900862400023861    steps: 217    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 843   score: 1.0   memory length: 155680   epsilon: 0.8897516200023934    steps: 169    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 844   score: 1.0   memory length: 155831   epsilon: 0.8894526400023999    steps: 151    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 845   score: 4.0   memory length: 156150   epsilon: 0.8888210200024136    steps: 319    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 846   score: 3.0   memory length: 156397   epsilon: 0.8883319600024242    steps: 247    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 847   score: 2.0   memory length: 156595   epsilon: 0.8879399200024327    steps: 198    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 848   score: 2.0   memory length: 156775   epsilon: 0.8875835200024405    steps: 180    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 849   score: 2.0   memory length: 156992   epsilon: 0.8871538600024498    steps: 217    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 850   score: 4.0   memory length: 157308   epsilon: 0.8865281800024634    steps: 316    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 851   score: 3.0   memory length: 157537   epsilon: 0.8860747600024732    steps: 229    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 852   score: 0.0   memory length: 157659   epsilon: 0.8858332000024784    steps: 122    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 853   score: 2.0   memory length: 157857   epsilon: 0.885441160002487    steps: 198    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 854   score: 1.0   memory length: 158027   epsilon: 0.8851045600024943    steps: 170    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 855   score: 1.0   memory length: 158199   epsilon: 0.8847640000025017    steps: 172    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 856   score: 0.0   memory length: 158322   epsilon: 0.884520460002507    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 857   score: 2.0   memory length: 158520   epsilon: 0.8841284200025155    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 858   score: 3.0   memory length: 158766   epsilon: 0.883641340002526    steps: 246    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 859   score: 2.0   memory length: 158967   epsilon: 0.8832433600025347    steps: 201    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 860   score: 1.0   memory length: 159118   epsilon: 0.8829443800025412    steps: 151    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 861   score: 4.0   memory length: 159393   epsilon: 0.882399880002553    steps: 275    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 862   score: 1.0   memory length: 159564   epsilon: 0.8820613000025603    steps: 171    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 863   score: 2.0   memory length: 159762   epsilon: 0.8816692600025688    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 864   score: 2.0   memory length: 159960   epsilon: 0.8812772200025774    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 865   score: 2.0   memory length: 160158   epsilon: 0.8808851800025859    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 866   score: 3.0   memory length: 160384   epsilon: 0.8804377000025956    steps: 226    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 867   score: 0.0   memory length: 160506   epsilon: 0.8801961400026008    steps: 122    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 868   score: 1.0   memory length: 160657   epsilon: 0.8798971600026073    steps: 151    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 869   score: 1.0   memory length: 160807   epsilon: 0.8796001600026138    steps: 150    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 870   score: 1.0   memory length: 160975   epsilon: 0.879267520002621    steps: 168    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 871   score: 3.0   memory length: 161203   epsilon: 0.8788160800026308    steps: 228    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 872   score: 0.0   memory length: 161326   epsilon: 0.8785725400026361    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 873   score: 3.0   memory length: 161555   epsilon: 0.8781191200026459    steps: 229    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 874   score: 3.0   memory length: 161805   epsilon: 0.8776241200026567    steps: 250    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 875   score: 5.0   memory length: 162150   epsilon: 0.8769410200026715    steps: 345    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 876   score: 2.0   memory length: 162348   epsilon: 0.87654898000268    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 877   score: 6.0   memory length: 162742   epsilon: 0.8757688600026969    steps: 394    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 878   score: 4.0   memory length: 163031   epsilon: 0.8751966400027094    steps: 289    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 879   score: 3.0   memory length: 163257   epsilon: 0.8747491600027191    steps: 226    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 880   score: 5.0   memory length: 163566   epsilon: 0.8741373400027324    steps: 309    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 881   score: 3.0   memory length: 163792   epsilon: 0.8736898600027421    steps: 226    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 882   score: 2.0   memory length: 164010   epsilon: 0.8732582200027514    steps: 218    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 883   score: 3.0   memory length: 164258   epsilon: 0.8727671800027621    steps: 248    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 884   score: 1.0   memory length: 164430   epsilon: 0.8724266200027695    steps: 172    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 885   score: 0.0   memory length: 164553   epsilon: 0.8721830800027748    steps: 123    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 886   score: 1.0   memory length: 164721   epsilon: 0.871850440002782    steps: 168    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 887   score: 5.0   memory length: 165046   epsilon: 0.871206940002796    steps: 325    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 888   score: 0.0   memory length: 165169   epsilon: 0.8709634000028013    steps: 123    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 889   score: 2.0   memory length: 165367   epsilon: 0.8705713600028098    steps: 198    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 890   score: 3.0   memory length: 165593   epsilon: 0.8701238800028195    steps: 226    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 891   score: 1.0   memory length: 165744   epsilon: 0.869824900002826    steps: 151    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 892   score: 0.0   memory length: 165867   epsilon: 0.8695813600028313    steps: 123    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 893   score: 2.0   memory length: 166065   epsilon: 0.8691893200028398    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 894   score: 1.0   memory length: 166216   epsilon: 0.8688903400028463    steps: 151    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 895   score: 2.0   memory length: 166396   epsilon: 0.868533940002854    steps: 180    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 896   score: 2.0   memory length: 166593   epsilon: 0.8681438800028625    steps: 197    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 897   score: 7.0   memory length: 166990   epsilon: 0.8673578200028795    steps: 397    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 898   score: 4.0   memory length: 167272   epsilon: 0.8667994600028917    steps: 282    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 899   score: 1.0   memory length: 167423   epsilon: 0.8665004800028981    steps: 151    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 900   score: 2.0   memory length: 167621   epsilon: 0.8661084400029067    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 901   score: 2.0   memory length: 167801   epsilon: 0.8657520400029144    steps: 180    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 902   score: 4.0   memory length: 168096   epsilon: 0.8651679400029271    steps: 295    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 903   score: 2.0   memory length: 168311   epsilon: 0.8647422400029363    steps: 215    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 904   score: 5.0   memory length: 168633   epsilon: 0.8641046800029502    steps: 322    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 905   score: 3.0   memory length: 168877   epsilon: 0.8636215600029606    steps: 244    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 906   score: 2.0   memory length: 169057   epsilon: 0.8632651600029684    steps: 180    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 907   score: 3.0   memory length: 169286   epsilon: 0.8628117400029782    steps: 229    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 908   score: 0.0   memory length: 169409   epsilon: 0.8625682000029835    steps: 123    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 909   score: 2.0   memory length: 169609   epsilon: 0.8621722000029921    steps: 200    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 910   score: 2.0   memory length: 169807   epsilon: 0.8617801600030006    steps: 198    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 911   score: 4.0   memory length: 170083   epsilon: 0.8612336800030125    steps: 276    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 912   score: 4.0   memory length: 170379   epsilon: 0.8606476000030252    steps: 296    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 913   score: 7.0   memory length: 170743   epsilon: 0.8599268800030408    steps: 364    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 914   score: 3.0   memory length: 170952   epsilon: 0.8595130600030498    steps: 209    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 915   score: 3.0   memory length: 171177   epsilon: 0.8590675600030595    steps: 225    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 916   score: 2.0   memory length: 171395   epsilon: 0.8586359200030689    steps: 218    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 917   score: 1.0   memory length: 171545   epsilon: 0.8583389200030753    steps: 150    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 918   score: 1.0   memory length: 171717   epsilon: 0.8579983600030827    steps: 172    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 919   score: 1.0   memory length: 171888   epsilon: 0.8576597800030901    steps: 171    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 920   score: 0.0   memory length: 172011   epsilon: 0.8574162400030954    steps: 123    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 921   score: 4.0   memory length: 172306   epsilon: 0.856832140003108    steps: 295    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 922   score: 4.0   memory length: 172582   epsilon: 0.8562856600031199    steps: 276    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 923   score: 0.0   memory length: 172704   epsilon: 0.8560441000031251    steps: 122    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 924   score: 2.0   memory length: 172902   epsilon: 0.8556520600031337    steps: 198    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 925   score: 0.0   memory length: 173024   epsilon: 0.8554105000031389    steps: 122    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 926   score: 0.0   memory length: 173147   epsilon: 0.8551669600031442    steps: 123    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 927   score: 1.0   memory length: 173298   epsilon: 0.8548679800031507    steps: 151    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 928   score: 2.0   memory length: 173498   epsilon: 0.8544719800031593    steps: 200    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 929   score: 2.0   memory length: 173679   epsilon: 0.854113600003167    steps: 181    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 930   score: 1.0   memory length: 173851   epsilon: 0.8537730400031744    steps: 172    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 931   score: 3.0   memory length: 174099   epsilon: 0.8532820000031851    steps: 248    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 932   score: 0.0   memory length: 174222   epsilon: 0.8530384600031904    steps: 123    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 933   score: 3.0   memory length: 174492   epsilon: 0.852503860003202    steps: 270    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 934   score: 0.0   memory length: 174615   epsilon: 0.8522603200032073    steps: 123    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 935   score: 0.0   memory length: 174737   epsilon: 0.8520187600032125    steps: 122    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 936   score: 2.0   memory length: 174917   epsilon: 0.8516623600032203    steps: 180    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 937   score: 0.0   memory length: 175039   epsilon: 0.8514208000032255    steps: 122    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 938   score: 2.0   memory length: 175238   epsilon: 0.8510267800032341    steps: 199    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 939   score: 0.0   memory length: 175361   epsilon: 0.8507832400032393    steps: 123    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 940   score: 3.0   memory length: 175607   epsilon: 0.8502961600032499    steps: 246    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 941   score: 4.0   memory length: 175904   epsilon: 0.8497081000032627    steps: 297    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 942   score: 4.0   memory length: 176198   epsilon: 0.8491259800032753    steps: 294    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 943   score: 2.0   memory length: 176380   epsilon: 0.8487656200032831    steps: 182    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 944   score: 6.0   memory length: 176736   epsilon: 0.8480607400032985    steps: 356    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 945   score: 2.0   memory length: 176954   epsilon: 0.8476291000033078    steps: 218    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 946   score: 2.0   memory length: 177172   epsilon: 0.8471974600033172    steps: 218    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 947   score: 2.0   memory length: 177390   epsilon: 0.8467658200033266    steps: 218    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 948   score: 5.0   memory length: 177670   epsilon: 0.8462114200033386    steps: 280    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 949   score: 3.0   memory length: 177897   epsilon: 0.8457619600033484    steps: 227    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 950   score: 2.0   memory length: 178097   epsilon: 0.845365960003357    steps: 200    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 951   score: 1.0   memory length: 178266   epsilon: 0.8450313400033642    steps: 169    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 952   score: 1.0   memory length: 178416   epsilon: 0.8447343400033707    steps: 150    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 953   score: 2.0   memory length: 178613   epsilon: 0.8443442800033791    steps: 197    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 954   score: 1.0   memory length: 178784   epsilon: 0.8440057000033865    steps: 171    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 955   score: 2.0   memory length: 178984   epsilon: 0.8436097000033951    steps: 200    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 956   score: 0.0   memory length: 179106   epsilon: 0.8433681400034003    steps: 122    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 957   score: 2.0   memory length: 179324   epsilon: 0.8429365000034097    steps: 218    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 958   score: 4.0   memory length: 179593   epsilon: 0.8424038800034213    steps: 269    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 959   score: 6.0   memory length: 179979   epsilon: 0.8416396000034378    steps: 386    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 960   score: 4.0   memory length: 180254   epsilon: 0.8410951000034497    steps: 275    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 961   score: 5.0   memory length: 180601   epsilon: 0.8404080400034646    steps: 347    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 962   score: 2.0   memory length: 180820   epsilon: 0.839974420003474    steps: 219    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 963   score: 1.0   memory length: 180971   epsilon: 0.8396754400034805    steps: 151    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 964   score: 1.0   memory length: 181142   epsilon: 0.8393368600034878    steps: 171    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 965   score: 3.0   memory length: 181390   epsilon: 0.8388458200034985    steps: 248    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 966   score: 2.0   memory length: 181607   epsilon: 0.8384161600035078    steps: 217    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 967   score: 3.0   memory length: 181833   epsilon: 0.8379686800035175    steps: 226    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 968   score: 5.0   memory length: 182156   epsilon: 0.8373291400035314    steps: 323    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 969   score: 0.0   memory length: 182279   epsilon: 0.8370856000035367    steps: 123    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 970   score: 1.0   memory length: 182447   epsilon: 0.8367529600035439    steps: 168    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 971   score: 2.0   memory length: 182627   epsilon: 0.8363965600035517    steps: 180    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 972   score: 1.0   memory length: 182799   epsilon: 0.8360560000035591    steps: 172    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 973   score: 1.0   memory length: 182950   epsilon: 0.8357570200035656    steps: 151    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 974   score: 3.0   memory length: 183176   epsilon: 0.8353095400035753    steps: 226    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 975   score: 1.0   memory length: 183346   epsilon: 0.8349729400035826    steps: 170    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 976   score: 0.0   memory length: 183469   epsilon: 0.8347294000035879    steps: 123    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 977   score: 3.0   memory length: 183718   epsilon: 0.8342363800035986    steps: 249    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 978   score: 5.0   memory length: 184044   epsilon: 0.8335909000036126    steps: 326    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 979   score: 4.0   memory length: 184338   epsilon: 0.8330087800036252    steps: 294    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 980   score: 3.0   memory length: 184586   epsilon: 0.8325177400036359    steps: 248    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 981   score: 2.0   memory length: 184784   epsilon: 0.8321257000036444    steps: 198    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 982   score: 0.0   memory length: 184907   epsilon: 0.8318821600036497    steps: 123    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 983   score: 1.0   memory length: 185058   epsilon: 0.8315831800036562    steps: 151    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 984   score: 1.0   memory length: 185209   epsilon: 0.8312842000036627    steps: 151    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 985   score: 2.0   memory length: 185426   epsilon: 0.830854540003672    steps: 217    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 986   score: 1.0   memory length: 185597   epsilon: 0.8305159600036793    steps: 171    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 987   score: 6.0   memory length: 185957   epsilon: 0.8298031600036948    steps: 360    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 988   score: 0.0   memory length: 186079   epsilon: 0.8295616000037    steps: 122    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 989   score: 0.0   memory length: 186202   epsilon: 0.8293180600037053    steps: 123    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 990   score: 2.0   memory length: 186420   epsilon: 0.8288864200037147    steps: 218    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 991   score: 1.0   memory length: 186588   epsilon: 0.8285537800037219    steps: 168    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 992   score: 5.0   memory length: 186916   epsilon: 0.827904340003736    steps: 328    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 993   score: 3.0   memory length: 187165   epsilon: 0.8274113200037467    steps: 249    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 994   score: 3.0   memory length: 187411   epsilon: 0.8269242400037573    steps: 246    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 995   score: 2.0   memory length: 187630   epsilon: 0.8264906200037667    steps: 219    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 996   score: 2.0   memory length: 187847   epsilon: 0.826060960003776    steps: 217    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 997   score: 1.0   memory length: 187997   epsilon: 0.8257639600037825    steps: 150    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 998   score: 2.0   memory length: 188179   epsilon: 0.8254036000037903    steps: 182    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 999   score: 2.0   memory length: 188376   epsilon: 0.8250135400037988    steps: 197    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1000   score: 4.0   memory length: 188652   epsilon: 0.8244670600038106    steps: 276    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1001   score: 2.0   memory length: 188871   epsilon: 0.8240334400038201    steps: 219    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1002   score: 2.0   memory length: 189069   epsilon: 0.8236414000038286    steps: 198    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1003   score: 1.0   memory length: 189220   epsilon: 0.8233424200038351    steps: 151    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1004   score: 2.0   memory length: 189421   epsilon: 0.8229444400038437    steps: 201    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1005   score: 5.0   memory length: 189725   epsilon: 0.8223425200038568    steps: 304    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1006   score: 2.0   memory length: 189941   epsilon: 0.821914840003866    steps: 216    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1007   score: 2.0   memory length: 190139   epsilon: 0.8215228000038746    steps: 198    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1008   score: 1.0   memory length: 190310   epsilon: 0.8211842200038819    steps: 171    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1009   score: 2.0   memory length: 190529   epsilon: 0.8207506000038913    steps: 219    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1010   score: 2.0   memory length: 190727   epsilon: 0.8203585600038998    steps: 198    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1011   score: 0.0   memory length: 190850   epsilon: 0.8201150200039051    steps: 123    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1012   score: 4.0   memory length: 191105   epsilon: 0.8196101200039161    steps: 255    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1013   score: 2.0   memory length: 191323   epsilon: 0.8191784800039255    steps: 218    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1014   score: 2.0   memory length: 191522   epsilon: 0.818784460003934    steps: 199    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1015   score: 2.0   memory length: 191722   epsilon: 0.8183884600039426    steps: 200    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1016   score: 4.0   memory length: 191978   epsilon: 0.8178815800039536    steps: 256    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1017   score: 4.0   memory length: 192274   epsilon: 0.8172955000039663    steps: 296    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 1018   score: 3.0   memory length: 192522   epsilon: 0.816804460003977    steps: 248    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1019   score: 4.0   memory length: 192799   epsilon: 0.8162560000039889    steps: 277    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1020   score: 3.0   memory length: 193025   epsilon: 0.8158085200039986    steps: 226    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1021   score: 1.0   memory length: 193197   epsilon: 0.815467960004006    steps: 172    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1022   score: 1.0   memory length: 193347   epsilon: 0.8151709600040125    steps: 150    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1023   score: 3.0   memory length: 193577   epsilon: 0.8147155600040223    steps: 230    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1024   score: 8.0   memory length: 194023   epsilon: 0.8138324800040415    steps: 446    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1025   score: 3.0   memory length: 194266   epsilon: 0.813351340004052    steps: 243    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1026   score: 1.0   memory length: 194417   epsilon: 0.8130523600040584    steps: 151    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 1027   score: 4.0   memory length: 194675   epsilon: 0.8125415200040695    steps: 258    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1028   score: 7.0   memory length: 195064   epsilon: 0.8117713000040863    steps: 389    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1029   score: 5.0   memory length: 195370   epsilon: 0.8111654200040994    steps: 306    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1030   score: 7.0   memory length: 195801   epsilon: 0.8103120400041179    steps: 431    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 1031   score: 5.0   memory length: 196150   epsilon: 0.8096210200041329    steps: 349    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1032   score: 2.0   memory length: 196366   epsilon: 0.8091933400041422    steps: 216    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 1033   score: 1.0   memory length: 196517   epsilon: 0.8088943600041487    steps: 151    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1034   score: 5.0   memory length: 196859   epsilon: 0.8082172000041634    steps: 342    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1035   score: 0.0   memory length: 196981   epsilon: 0.8079756400041687    steps: 122    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1036   score: 5.0   memory length: 197288   epsilon: 0.8073677800041819    steps: 307    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1037   score: 3.0   memory length: 197532   epsilon: 0.8068846600041923    steps: 244    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1038   score: 3.0   memory length: 197760   epsilon: 0.8064332200042021    steps: 228    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1039   score: 2.0   memory length: 197959   epsilon: 0.8060392000042107    steps: 199    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1040   score: 4.0   memory length: 198239   epsilon: 0.8054848000042227    steps: 280    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1041   score: 4.0   memory length: 198516   epsilon: 0.8049363400042346    steps: 277    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1042   score: 2.0   memory length: 198732   epsilon: 0.8045086600042439    steps: 216    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1043   score: 3.0   memory length: 198962   epsilon: 0.8040532600042538    steps: 230    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1044   score: 2.0   memory length: 199159   epsilon: 0.8036632000042623    steps: 197    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1045   score: 4.0   memory length: 199453   epsilon: 0.8030810800042749    steps: 294    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1046   score: 4.0   memory length: 199748   epsilon: 0.8024969800042876    steps: 295    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1047   score: 3.0   memory length: 199994   epsilon: 0.8020099000042982    steps: 246    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1048   score: 5.0   memory length: 200305   epsilon: 0.8013941200043115    steps: 311    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1049   score: 6.0   memory length: 200699   epsilon: 0.8006140000043285    steps: 394    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1050   score: 2.0   memory length: 200899   epsilon: 0.8002180000043371    steps: 200    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1051   score: 5.0   memory length: 201221   epsilon: 0.7995804400043509    steps: 322    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1052   score: 5.0   memory length: 201546   epsilon: 0.7989369400043649    steps: 325    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1053   score: 2.0   memory length: 201744   epsilon: 0.7985449000043734    steps: 198    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1054   score: 2.0   memory length: 201942   epsilon: 0.7981528600043819    steps: 198    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1055   score: 4.0   memory length: 202235   epsilon: 0.7975727200043945    steps: 293    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1056   score: 1.0   memory length: 202386   epsilon: 0.797273740004401    steps: 151    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1057   score: 1.0   memory length: 202537   epsilon: 0.7969747600044075    steps: 151    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1058   score: 3.0   memory length: 202765   epsilon: 0.7965233200044173    steps: 228    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1059   score: 4.0   memory length: 203061   epsilon: 0.79593724000443    steps: 296    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1060   score: 3.0   memory length: 203332   epsilon: 0.7954006600044417    steps: 271    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1061   score: 4.0   memory length: 203626   epsilon: 0.7948185400044543    steps: 294    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1062   score: 1.0   memory length: 203797   epsilon: 0.7944799600044616    steps: 171    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1063   score: 2.0   memory length: 204014   epsilon: 0.794050300004471    steps: 217    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1064   score: 5.0   memory length: 204338   epsilon: 0.7934087800044849    steps: 324    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1065   score: 3.0   memory length: 204564   epsilon: 0.7929613000044946    steps: 226    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1066   score: 1.0   memory length: 204733   epsilon: 0.7926266800045019    steps: 169    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1067   score: 6.0   memory length: 205143   epsilon: 0.7918148800045195    steps: 410    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1068   score: 1.0   memory length: 205294   epsilon: 0.791515900004526    steps: 151    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1069   score: 1.0   memory length: 205445   epsilon: 0.7912169200045325    steps: 151    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1070   score: 1.0   memory length: 205616   epsilon: 0.7908783400045398    steps: 171    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1071   score: 6.0   memory length: 206013   epsilon: 0.7900922800045569    steps: 397    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1072   score: 3.0   memory length: 206257   epsilon: 0.7896091600045674    steps: 244    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1073   score: 8.0   memory length: 206756   epsilon: 0.7886211400045888    steps: 499    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1074   score: 2.0   memory length: 206936   epsilon: 0.7882647400045966    steps: 180    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1075   score: 2.0   memory length: 207154   epsilon: 0.7878331000046059    steps: 218    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1076   score: 2.0   memory length: 207352   epsilon: 0.7874410600046144    steps: 198    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1077   score: 6.0   memory length: 207730   epsilon: 0.7866926200046307    steps: 378    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1078   score: 6.0   memory length: 208106   epsilon: 0.7859481400046469    steps: 376    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1079   score: 3.0   memory length: 208352   epsilon: 0.7854610600046574    steps: 246    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1080   score: 2.0   memory length: 208568   epsilon: 0.7850333800046667    steps: 216    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1081   score: 3.0   memory length: 208817   epsilon: 0.7845403600046774    steps: 249    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1082   score: 1.0   memory length: 208968   epsilon: 0.7842413800046839    steps: 151    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1083   score: 4.0   memory length: 209225   epsilon: 0.783732520004695    steps: 257    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1084   score: 5.0   memory length: 209535   epsilon: 0.7831187200047083    steps: 310    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1085   score: 2.0   memory length: 209732   epsilon: 0.7827286600047167    steps: 197    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1086   score: 6.0   memory length: 210073   epsilon: 0.7820534800047314    steps: 341    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1087   score: 2.0   memory length: 210270   epsilon: 0.7816634200047399    steps: 197    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1088   score: 3.0   memory length: 210481   epsilon: 0.7812456400047489    steps: 211    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1089   score: 3.0   memory length: 210750   epsilon: 0.7807130200047605    steps: 269    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1090   score: 4.0   memory length: 211037   epsilon: 0.7801447600047728    steps: 287    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1091   score: 1.0   memory length: 211205   epsilon: 0.7798121200047801    steps: 168    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1092   score: 3.0   memory length: 211451   epsilon: 0.7793250400047906    steps: 246    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1093   score: 2.0   memory length: 211649   epsilon: 0.7789330000047991    steps: 198    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1094   score: 0.0   memory length: 211772   epsilon: 0.7786894600048044    steps: 123    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1095   score: 2.0   memory length: 211952   epsilon: 0.7783330600048122    steps: 180    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1096   score: 3.0   memory length: 212217   epsilon: 0.7778083600048236    steps: 265    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1097   score: 4.0   memory length: 212530   epsilon: 0.777188620004837    steps: 313    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1098   score: 6.0   memory length: 212926   epsilon: 0.776404540004854    steps: 396    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1099   score: 2.0   memory length: 213124   epsilon: 0.7760125000048625    steps: 198    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1100   score: 4.0   memory length: 213378   epsilon: 0.7755095800048735    steps: 254    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1101   score: 1.0   memory length: 213547   epsilon: 0.7751749600048807    steps: 169    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1102   score: 6.0   memory length: 213943   epsilon: 0.7743908800048978    steps: 396    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1103   score: 4.0   memory length: 214258   epsilon: 0.7737671800049113    steps: 315    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1104   score: 3.0   memory length: 214525   epsilon: 0.7732385200049228    steps: 267    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1105   score: 1.0   memory length: 214675   epsilon: 0.7729415200049292    steps: 150    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1106   score: 2.0   memory length: 214855   epsilon: 0.772585120004937    steps: 180    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1107   score: 2.0   memory length: 215052   epsilon: 0.7721950600049454    steps: 197    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1108   score: 0.0   memory length: 215174   epsilon: 0.7719535000049507    steps: 122    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1109   score: 1.0   memory length: 215325   epsilon: 0.7716545200049572    steps: 151    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1110   score: 3.0   memory length: 215551   epsilon: 0.7712070400049669    steps: 226    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1111   score: 3.0   memory length: 215801   epsilon: 0.7707120400049776    steps: 250    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1112   score: 5.0   memory length: 216147   epsilon: 0.7700269600049925    steps: 346    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1113   score: 4.0   memory length: 216461   epsilon: 0.769405240005006    steps: 314    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1114   score: 3.0   memory length: 216708   epsilon: 0.7689161800050166    steps: 247    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1115   score: 3.0   memory length: 216954   epsilon: 0.7684291000050272    steps: 246    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1116   score: 2.0   memory length: 217169   epsilon: 0.7680034000050364    steps: 215    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1117   score: 2.0   memory length: 217366   epsilon: 0.7676133400050449    steps: 197    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1118   score: 1.0   memory length: 217518   epsilon: 0.7673123800050514    steps: 152    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1119   score: 1.0   memory length: 217688   epsilon: 0.7669757800050587    steps: 170    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1120   score: 5.0   memory length: 218012   epsilon: 0.7663342600050727    steps: 324    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1121   score: 4.0   memory length: 218288   epsilon: 0.7657877800050845    steps: 276    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1122   score: 4.0   memory length: 218565   epsilon: 0.7652393200050964    steps: 277    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1123   score: 1.0   memory length: 218733   epsilon: 0.7649066800051036    steps: 168    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1124   score: 1.0   memory length: 218884   epsilon: 0.7646077000051101    steps: 151    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1125   score: 3.0   memory length: 219109   epsilon: 0.7641622000051198    steps: 225    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1126   score: 4.0   memory length: 219404   epsilon: 0.7635781000051325    steps: 295    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1127   score: 2.0   memory length: 219602   epsilon: 0.763186060005141    steps: 198    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1128   score: 2.0   memory length: 219821   epsilon: 0.7627524400051504    steps: 219    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1129   score: 4.0   memory length: 220136   epsilon: 0.762128740005164    steps: 315    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1130   score: 4.0   memory length: 220410   epsilon: 0.7615862200051757    steps: 274    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1131   score: 2.0   memory length: 220628   epsilon: 0.7611545800051851    steps: 218    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1132   score: 7.0   memory length: 221019   epsilon: 0.7603804000052019    steps: 391    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1133   score: 3.0   memory length: 221248   epsilon: 0.7599269800052117    steps: 229    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1134   score: 3.0   memory length: 221478   epsilon: 0.7594715800052216    steps: 230    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1135   score: 3.0   memory length: 221706   epsilon: 0.7590201400052314    steps: 228    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1136   score: 3.0   memory length: 221955   epsilon: 0.7585271200052421    steps: 249    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1137   score: 6.0   memory length: 222345   epsilon: 0.7577549200052589    steps: 390    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1138   score: 5.0   memory length: 222694   epsilon: 0.7570639000052739    steps: 349    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1139   score: 1.0   memory length: 222845   epsilon: 0.7567649200052804    steps: 151    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1140   score: 0.0   memory length: 222968   epsilon: 0.7565213800052857    steps: 123    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1141   score: 1.0   memory length: 223118   epsilon: 0.7562243800052921    steps: 150    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1142   score: 2.0   memory length: 223297   epsilon: 0.7558699600052998    steps: 179    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1143   score: 2.0   memory length: 223495   epsilon: 0.7554779200053083    steps: 198    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1144   score: 3.0   memory length: 223743   epsilon: 0.754986880005319    steps: 248    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1145   score: 3.0   memory length: 223989   epsilon: 0.7544998000053296    steps: 246    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1146   score: 1.0   memory length: 224158   epsilon: 0.7541651800053368    steps: 169    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1147   score: 0.0   memory length: 224281   epsilon: 0.7539216400053421    steps: 123    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1148   score: 3.0   memory length: 224507   epsilon: 0.7534741600053518    steps: 226    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1149   score: 5.0   memory length: 224800   epsilon: 0.7528940200053644    steps: 293    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1150   score: 5.0   memory length: 225127   epsilon: 0.7522465600053785    steps: 327    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1151   score: 4.0   memory length: 225402   epsilon: 0.7517020600053903    steps: 275    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1152   score: 2.0   memory length: 225619   epsilon: 0.7512724000053996    steps: 217    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1153   score: 2.0   memory length: 225816   epsilon: 0.7508823400054081    steps: 197    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1154   score: 3.0   memory length: 226042   epsilon: 0.7504348600054178    steps: 226    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1155   score: 6.0   memory length: 226413   epsilon: 0.7497002800054338    steps: 371    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1156   score: 3.0   memory length: 226657   epsilon: 0.7492171600054442    steps: 244    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1157   score: 0.0   memory length: 226779   epsilon: 0.7489756000054495    steps: 122    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1158   score: 4.0   memory length: 227035   epsilon: 0.7484687200054605    steps: 256    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1159   score: 3.0   memory length: 227248   epsilon: 0.7480469800054697    steps: 213    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1160   score: 2.0   memory length: 227446   epsilon: 0.7476549400054782    steps: 198    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1161   score: 9.0   memory length: 227970   epsilon: 0.7466174200055007    steps: 524    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1162   score: 3.0   memory length: 228200   epsilon: 0.7461620200055106    steps: 230    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1163   score: 3.0   memory length: 228430   epsilon: 0.7457066200055205    steps: 230    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1164   score: 6.0   memory length: 228803   epsilon: 0.7449680800055365    steps: 373    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1165   score: 7.0   memory length: 229204   epsilon: 0.7441741000055537    steps: 401    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1166   score: 5.0   memory length: 229508   epsilon: 0.7435721800055668    steps: 304    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1167   score: 3.0   memory length: 229756   epsilon: 0.7430811400055775    steps: 248    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1168   score: 3.0   memory length: 229988   epsilon: 0.7426217800055874    steps: 232    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1169   score: 2.0   memory length: 230186   epsilon: 0.7422297400055959    steps: 198    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1170   score: 2.0   memory length: 230366   epsilon: 0.7418733400056037    steps: 180    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1171   score: 2.0   memory length: 230546   epsilon: 0.7415169400056114    steps: 180    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1172   score: 5.0   memory length: 230892   epsilon: 0.7408318600056263    steps: 346    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1173   score: 4.0   memory length: 231151   epsilon: 0.7403190400056374    steps: 259    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1174   score: 0.0   memory length: 231274   epsilon: 0.7400755000056427    steps: 123    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1175   score: 5.0   memory length: 231594   epsilon: 0.7394419000056565    steps: 320    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1176   score: 3.0   memory length: 231843   epsilon: 0.7389488800056672    steps: 249    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1177   score: 7.0   memory length: 232246   epsilon: 0.7381509400056845    steps: 403    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1178   score: 4.0   memory length: 232538   epsilon: 0.737572780005697    steps: 292    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1179   score: 4.0   memory length: 232815   epsilon: 0.7370243200057089    steps: 277    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1180   score: 5.0   memory length: 233134   epsilon: 0.7363927000057227    steps: 319    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1181   score: 2.0   memory length: 233333   epsilon: 0.7359986800057312    steps: 199    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1182   score: 5.0   memory length: 233657   epsilon: 0.7353571600057451    steps: 324    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1183   score: 5.0   memory length: 233964   epsilon: 0.7347493000057583    steps: 307    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1184   score: 5.0   memory length: 234267   epsilon: 0.7341493600057714    steps: 303    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1185   score: 4.0   memory length: 234541   epsilon: 0.7336068400057831    steps: 274    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1186   score: 2.0   memory length: 234739   epsilon: 0.7332148000057916    steps: 198    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1187   score: 3.0   memory length: 234949   epsilon: 0.7327990000058007    steps: 210    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1188   score: 3.0   memory length: 235178   epsilon: 0.7323455800058105    steps: 229    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1189   score: 7.0   memory length: 235581   epsilon: 0.7315476400058278    steps: 403    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1190   score: 1.0   memory length: 235732   epsilon: 0.7312486600058343    steps: 151    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1191   score: 4.0   memory length: 235976   epsilon: 0.7307655400058448    steps: 244    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1192   score: 1.0   memory length: 236145   epsilon: 0.7304309200058521    steps: 169    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1193   score: 6.0   memory length: 236510   epsilon: 0.7297082200058678    steps: 365    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1194   score: 4.0   memory length: 236790   epsilon: 0.7291538200058798    steps: 280    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1195   score: 3.0   memory length: 237016   epsilon: 0.7287063400058895    steps: 226    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1196   score: 5.0   memory length: 237301   epsilon: 0.7281420400059018    steps: 285    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1197   score: 1.0   memory length: 237451   epsilon: 0.7278450400059082    steps: 150    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1198   score: 2.0   memory length: 237649   epsilon: 0.7274530000059167    steps: 198    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1199   score: 3.0   memory length: 237919   epsilon: 0.7269184000059283    steps: 270    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1200   score: 2.0   memory length: 238099   epsilon: 0.7265620000059361    steps: 180    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1201   score: 3.0   memory length: 238369   epsilon: 0.7260274000059477    steps: 270    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1202   score: 2.0   memory length: 238588   epsilon: 0.7255937800059571    steps: 219    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1203   score: 4.0   memory length: 238862   epsilon: 0.7250512600059689    steps: 274    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1204   score: 5.0   memory length: 239189   epsilon: 0.7244038000059829    steps: 327    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1205   score: 3.0   memory length: 239414   epsilon: 0.7239583000059926    steps: 225    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1206   score: 4.0   memory length: 239708   epsilon: 0.7233761800060052    steps: 294    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1207   score: 2.0   memory length: 239907   epsilon: 0.7229821600060138    steps: 199    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1208   score: 5.0   memory length: 240212   epsilon: 0.7223782600060269    steps: 305    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1209   score: 0.0   memory length: 240335   epsilon: 0.7221347200060322    steps: 123    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1210   score: 2.0   memory length: 240517   epsilon: 0.72177436000604    steps: 182    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1211   score: 1.0   memory length: 240667   epsilon: 0.7214773600060465    steps: 150    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1212   score: 3.0   memory length: 240892   epsilon: 0.7210318600060561    steps: 225    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1213   score: 2.0   memory length: 241091   epsilon: 0.7206378400060647    steps: 199    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1214   score: 4.0   memory length: 241370   epsilon: 0.7200854200060767    steps: 279    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1215   score: 5.0   memory length: 241715   epsilon: 0.7194023200060915    steps: 345    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1216   score: 3.0   memory length: 241981   epsilon: 0.7188756400061029    steps: 266    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1217   score: 1.0   memory length: 242152   epsilon: 0.7185370600061103    steps: 171    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1218   score: 1.0   memory length: 242302   epsilon: 0.7182400600061167    steps: 150    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1219   score: 5.0   memory length: 242606   epsilon: 0.7176381400061298    steps: 304    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1220   score: 3.0   memory length: 242870   epsilon: 0.7171154200061411    steps: 264    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1221   score: 3.0   memory length: 243115   epsilon: 0.7166303200061517    steps: 245    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1222   score: 4.0   memory length: 243413   epsilon: 0.7160402800061645    steps: 298    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1223   score: 6.0   memory length: 243808   epsilon: 0.7152581800061815    steps: 395    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1224   score: 2.0   memory length: 244006   epsilon: 0.71486614000619    steps: 198    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1225   score: 1.0   memory length: 244157   epsilon: 0.7145671600061965    steps: 151    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1226   score: 6.0   memory length: 244547   epsilon: 0.7137949600062132    steps: 390    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1227   score: 3.0   memory length: 244777   epsilon: 0.7133395600062231    steps: 230    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1228   score: 3.0   memory length: 245044   epsilon: 0.7128109000062346    steps: 267    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1229   score: 6.0   memory length: 245403   epsilon: 0.71210008000625    steps: 359    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1230   score: 2.0   memory length: 245623   epsilon: 0.7116644800062595    steps: 220    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1231   score: 6.0   memory length: 245963   epsilon: 0.7109912800062741    steps: 340    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1232   score: 4.0   memory length: 246240   epsilon: 0.710442820006286    steps: 277    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1233   score: 2.0   memory length: 246420   epsilon: 0.7100864200062937    steps: 180    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1234   score: 5.0   memory length: 246712   epsilon: 0.7095082600063063    steps: 292    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1235   score: 3.0   memory length: 246959   epsilon: 0.7090192000063169    steps: 247    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1236   score: 2.0   memory length: 247158   epsilon: 0.7086251800063255    steps: 199    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1237   score: 3.0   memory length: 247384   epsilon: 0.7081777000063352    steps: 226    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1238   score: 7.0   memory length: 247785   epsilon: 0.7073837200063524    steps: 401    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1239   score: 4.0   memory length: 248026   epsilon: 0.7069065400063628    steps: 241    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1240   score: 5.0   memory length: 248367   epsilon: 0.7062313600063774    steps: 341    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1241   score: 2.0   memory length: 248548   epsilon: 0.7058729800063852    steps: 181    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1242   score: 4.0   memory length: 248841   epsilon: 0.7052928400063978    steps: 293    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1243   score: 3.0   memory length: 249068   epsilon: 0.7048433800064076    steps: 227    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1244   score: 1.0   memory length: 249219   epsilon: 0.704544400006414    steps: 151    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1245   score: 12.0   memory length: 249647   epsilon: 0.7036969600064324    steps: 428    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1246   score: 3.0   memory length: 249856   epsilon: 0.7032831400064414    steps: 209    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1247   score: 1.0   memory length: 250006   epsilon: 0.7029861400064479    steps: 150    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1248   score: 4.0   memory length: 250264   epsilon: 0.702475300006459    steps: 258    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1249   score: 5.0   memory length: 250554   epsilon: 0.7019011000064714    steps: 290    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1250   score: 2.0   memory length: 250751   epsilon: 0.7015110400064799    steps: 197    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1251   score: 5.0   memory length: 251076   epsilon: 0.7008675400064939    steps: 325    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1252   score: 9.0   memory length: 251566   epsilon: 0.6998973400065149    steps: 490    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1253   score: 5.0   memory length: 251910   epsilon: 0.6992162200065297    steps: 344    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1254   score: 1.0   memory length: 252060   epsilon: 0.6989192200065362    steps: 150    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1255   score: 5.0   memory length: 252374   epsilon: 0.6982975000065497    steps: 314    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1256   score: 4.0   memory length: 252616   epsilon: 0.6978183400065601    steps: 242    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1257   score: 2.0   memory length: 252816   epsilon: 0.6974223400065687    steps: 200    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1258   score: 1.0   memory length: 252966   epsilon: 0.6971253400065751    steps: 150    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1259   score: 5.0   memory length: 253289   epsilon: 0.696485800006589    steps: 323    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1260   score: 2.0   memory length: 253487   epsilon: 0.6960937600065975    steps: 198    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1261   score: 6.0   memory length: 253879   epsilon: 0.6953176000066144    steps: 392    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1262   score: 3.0   memory length: 254126   epsilon: 0.694828540006625    steps: 247    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1263   score: 3.0   memory length: 254353   epsilon: 0.6943790800066347    steps: 227    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1264   score: 0.0   memory length: 254476   epsilon: 0.69413554000664    steps: 123    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1265   score: 7.0   memory length: 254724   epsilon: 0.6936445000066507    steps: 248    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1266   score: 5.0   memory length: 255035   epsilon: 0.693028720006664    steps: 311    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1267   score: 1.0   memory length: 255186   epsilon: 0.6927297400066705    steps: 151    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1268   score: 1.0   memory length: 255337   epsilon: 0.692430760006677    steps: 151    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1269   score: 1.0   memory length: 255487   epsilon: 0.6921337600066835    steps: 150    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1270   score: 0.0   memory length: 255610   epsilon: 0.6918902200066888    steps: 123    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1271   score: 1.0   memory length: 255782   epsilon: 0.6915496600066962    steps: 172    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1272   score: 2.0   memory length: 255963   epsilon: 0.6911912800067039    steps: 181    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1273   score: 14.0   memory length: 256433   epsilon: 0.6902606800067241    steps: 470    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1274   score: 4.0   memory length: 256690   epsilon: 0.6897518200067352    steps: 257    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1275   score: 3.0   memory length: 256918   epsilon: 0.689300380006745    steps: 228    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1276   score: 7.0   memory length: 257305   epsilon: 0.6885341200067616    steps: 387    lr: 4e-05     evaluation reward: 3.57\n",
      "episode: 1277   score: 2.0   memory length: 257486   epsilon: 0.6881757400067694    steps: 181    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1278   score: 6.0   memory length: 257834   epsilon: 0.6874867000067844    steps: 348    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1279   score: 2.0   memory length: 258014   epsilon: 0.6871303000067921    steps: 180    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1280   score: 2.0   memory length: 258212   epsilon: 0.6867382600068006    steps: 198    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1281   score: 5.0   memory length: 258538   epsilon: 0.6860927800068146    steps: 326    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1282   score: 4.0   memory length: 258836   epsilon: 0.6855027400068274    steps: 298    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1283   score: 2.0   memory length: 259052   epsilon: 0.6850750600068367    steps: 216    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1284   score: 5.0   memory length: 259360   epsilon: 0.68446522000685    steps: 308    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1285   score: 5.0   memory length: 259682   epsilon: 0.6838276600068638    steps: 322    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1286   score: 3.0   memory length: 259931   epsilon: 0.6833346400068745    steps: 249    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1287   score: 2.0   memory length: 260131   epsilon: 0.6829386400068831    steps: 200    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1288   score: 3.0   memory length: 260396   epsilon: 0.6824139400068945    steps: 265    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1289   score: 3.0   memory length: 260646   epsilon: 0.6819189400069052    steps: 250    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1290   score: 6.0   memory length: 260984   epsilon: 0.6812497000069198    steps: 338    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1291   score: 3.0   memory length: 261213   epsilon: 0.6807962800069296    steps: 229    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1292   score: 4.0   memory length: 261475   epsilon: 0.6802775200069409    steps: 262    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1293   score: 4.0   memory length: 261753   epsilon: 0.6797270800069528    steps: 278    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1294   score: 1.0   memory length: 261904   epsilon: 0.6794281000069593    steps: 151    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1295   score: 7.0   memory length: 262311   epsilon: 0.6786222400069768    steps: 407    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1296   score: 6.0   memory length: 262671   epsilon: 0.6779094400069923    steps: 360    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1297   score: 2.0   memory length: 262871   epsilon: 0.6775134400070009    steps: 200    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1298   score: 5.0   memory length: 263185   epsilon: 0.6768917200070144    steps: 314    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1299   score: 3.0   memory length: 263411   epsilon: 0.6764442400070241    steps: 226    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1300   score: 6.0   memory length: 263785   epsilon: 0.6757037200070402    steps: 374    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1301   score: 0.0   memory length: 263908   epsilon: 0.6754601800070454    steps: 123    lr: 4e-05     evaluation reward: 3.57\n",
      "episode: 1302   score: 3.0   memory length: 264179   epsilon: 0.6749236000070571    steps: 271    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1303   score: 2.0   memory length: 264377   epsilon: 0.6745315600070656    steps: 198    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1304   score: 5.0   memory length: 264681   epsilon: 0.6739296400070787    steps: 304    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1305   score: 0.0   memory length: 264804   epsilon: 0.673686100007084    steps: 123    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1306   score: 1.0   memory length: 264955   epsilon: 0.6733871200070904    steps: 151    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1307   score: 0.0   memory length: 265078   epsilon: 0.6731435800070957    steps: 123    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1308   score: 2.0   memory length: 265276   epsilon: 0.6727515400071042    steps: 198    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1309   score: 3.0   memory length: 265522   epsilon: 0.6722644600071148    steps: 246    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1310   score: 5.0   memory length: 265846   epsilon: 0.6716229400071287    steps: 324    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1311   score: 1.0   memory length: 266015   epsilon: 0.671288320007136    steps: 169    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1312   score: 4.0   memory length: 266310   epsilon: 0.6707042200071487    steps: 295    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1313   score: 5.0   memory length: 266601   epsilon: 0.6701280400071612    steps: 291    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1314   score: 2.0   memory length: 266784   epsilon: 0.6697657000071691    steps: 183    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1315   score: 7.0   memory length: 267174   epsilon: 0.6689935000071858    steps: 390    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1316   score: 3.0   memory length: 267403   epsilon: 0.6685400800071957    steps: 229    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1317   score: 5.0   memory length: 267690   epsilon: 0.667971820007208    steps: 287    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1318   score: 6.0   memory length: 268045   epsilon: 0.6672689200072233    steps: 355    lr: 4e-05     evaluation reward: 3.64\n",
      "episode: 1319   score: 6.0   memory length: 268297   epsilon: 0.6667699600072341    steps: 252    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1320   score: 6.0   memory length: 268655   epsilon: 0.6660611200072495    steps: 358    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1321   score: 4.0   memory length: 268910   epsilon: 0.6655562200072604    steps: 255    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1322   score: 4.0   memory length: 269167   epsilon: 0.6650473600072715    steps: 257    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1323   score: 3.0   memory length: 269415   epsilon: 0.6645563200072822    steps: 248    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1324   score: 5.0   memory length: 269718   epsilon: 0.6639563800072952    steps: 303    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1325   score: 2.0   memory length: 269899   epsilon: 0.663598000007303    steps: 181    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1326   score: 7.0   memory length: 270261   epsilon: 0.6628812400073185    steps: 362    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1327   score: 1.0   memory length: 270432   epsilon: 0.6625426600073259    steps: 171    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1328   score: 5.0   memory length: 270755   epsilon: 0.6619031200073398    steps: 323    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1329   score: 1.0   memory length: 270924   epsilon: 0.661568500007347    steps: 169    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1330   score: 7.0   memory length: 271318   epsilon: 0.660788380007364    steps: 394    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1331   score: 4.0   memory length: 271576   epsilon: 0.660277540007375    steps: 258    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1332   score: 6.0   memory length: 271929   epsilon: 0.6595786000073902    steps: 353    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1333   score: 4.0   memory length: 272209   epsilon: 0.6590242000074022    steps: 280    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1334   score: 7.0   memory length: 272613   epsilon: 0.6582242800074196    steps: 404    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1335   score: 9.0   memory length: 273109   epsilon: 0.6572422000074409    steps: 496    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1336   score: 2.0   memory length: 273326   epsilon: 0.6568125400074503    steps: 217    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1337   score: 3.0   memory length: 273576   epsilon: 0.656317540007461    steps: 250    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1338   score: 10.0   memory length: 274090   epsilon: 0.6552998200074831    steps: 514    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1339   score: 0.0   memory length: 274212   epsilon: 0.6550582600074883    steps: 122    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1340   score: 3.0   memory length: 274458   epsilon: 0.6545711800074989    steps: 246    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1341   score: 3.0   memory length: 274684   epsilon: 0.6541237000075086    steps: 226    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1342   score: 3.0   memory length: 274911   epsilon: 0.6536742400075184    steps: 227    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1343   score: 3.0   memory length: 275161   epsilon: 0.6531792400075291    steps: 250    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1344   score: 2.0   memory length: 275359   epsilon: 0.6527872000075376    steps: 198    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1345   score: 0.0   memory length: 275481   epsilon: 0.6525456400075429    steps: 122    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1346   score: 4.0   memory length: 275756   epsilon: 0.6520011400075547    steps: 275    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1347   score: 8.0   memory length: 276232   epsilon: 0.6510586600075752    steps: 476    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1348   score: 4.0   memory length: 276524   epsilon: 0.6504805000075877    steps: 292    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1349   score: 5.0   memory length: 276865   epsilon: 0.6498053200076024    steps: 341    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1350   score: 1.0   memory length: 277035   epsilon: 0.6494687200076097    steps: 170    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1351   score: 6.0   memory length: 277391   epsilon: 0.648763840007625    steps: 356    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1352   score: 5.0   memory length: 277714   epsilon: 0.6481243000076389    steps: 323    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1353   score: 5.0   memory length: 278028   epsilon: 0.6475025800076524    steps: 314    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1354   score: 4.0   memory length: 278288   epsilon: 0.6469877800076635    steps: 260    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1355   score: 4.0   memory length: 278530   epsilon: 0.646508620007674    steps: 242    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1356   score: 5.0   memory length: 278839   epsilon: 0.6458968000076872    steps: 309    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1357   score: 4.0   memory length: 279117   epsilon: 0.6453463600076992    steps: 278    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1358   score: 2.0   memory length: 279315   epsilon: 0.6449543200077077    steps: 198    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1359   score: 4.0   memory length: 279613   epsilon: 0.6443642800077205    steps: 298    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1360   score: 5.0   memory length: 279960   epsilon: 0.6436772200077354    steps: 347    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1361   score: 5.0   memory length: 280270   epsilon: 0.6430634200077487    steps: 310    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1362   score: 6.0   memory length: 280646   epsilon: 0.6423189400077649    steps: 376    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1363   score: 4.0   memory length: 280942   epsilon: 0.6417328600077776    steps: 296    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1364   score: 5.0   memory length: 281249   epsilon: 0.6411250000077908    steps: 307    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1365   score: 6.0   memory length: 281585   epsilon: 0.6404597200078053    steps: 336    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1366   score: 4.0   memory length: 281859   epsilon: 0.639917200007817    steps: 274    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1367   score: 0.0   memory length: 281981   epsilon: 0.6396756400078223    steps: 122    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1368   score: 3.0   memory length: 282206   epsilon: 0.639230140007832    steps: 225    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1369   score: 4.0   memory length: 282468   epsilon: 0.6387113800078432    steps: 262    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1370   score: 4.0   memory length: 282728   epsilon: 0.6381965800078544    steps: 260    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1371   score: 3.0   memory length: 282941   epsilon: 0.6377748400078636    steps: 213    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1372   score: 3.0   memory length: 283169   epsilon: 0.6373234000078734    steps: 228    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1373   score: 2.0   memory length: 283351   epsilon: 0.6369630400078812    steps: 182    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1374   score: 0.0   memory length: 283473   epsilon: 0.6367214800078864    steps: 122    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1375   score: 2.0   memory length: 283673   epsilon: 0.636325480007895    steps: 200    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1376   score: 10.0   memory length: 284020   epsilon: 0.6356384200079099    steps: 347    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1377   score: 6.0   memory length: 284408   epsilon: 0.6348701800079266    steps: 388    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1378   score: 1.0   memory length: 284577   epsilon: 0.6345355600079339    steps: 169    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1379   score: 4.0   memory length: 284836   epsilon: 0.634022740007945    steps: 259    lr: 4e-05     evaluation reward: 3.83\n",
      "episode: 1380   score: 5.0   memory length: 285117   epsilon: 0.6334663600079571    steps: 281    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1381   score: 5.0   memory length: 285444   epsilon: 0.6328189000079711    steps: 327    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1382   score: 7.0   memory length: 285842   epsilon: 0.6320308600079882    steps: 398    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1383   score: 2.0   memory length: 286022   epsilon: 0.631674460007996    steps: 180    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1384   score: 7.0   memory length: 286430   epsilon: 0.6308666200080135    steps: 408    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1385   score: 3.0   memory length: 286677   epsilon: 0.6303775600080241    steps: 247    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1386   score: 2.0   memory length: 286874   epsilon: 0.6299875000080326    steps: 197    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1387   score: 11.0   memory length: 287347   epsilon: 0.6290509600080529    steps: 473    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1388   score: 2.0   memory length: 287545   epsilon: 0.6286589200080615    steps: 198    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1389   score: 4.0   memory length: 287820   epsilon: 0.6281144200080733    steps: 275    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1390   score: 2.0   memory length: 288020   epsilon: 0.6277184200080819    steps: 200    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1391   score: 11.0   memory length: 288547   epsilon: 0.6266749600081045    steps: 527    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1392   score: 7.0   memory length: 288918   epsilon: 0.6259403800081205    steps: 371    lr: 4e-05     evaluation reward: 4.04\n",
      "episode: 1393   score: 3.0   memory length: 289168   epsilon: 0.6254453800081312    steps: 250    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1394   score: 1.0   memory length: 289319   epsilon: 0.6251464000081377    steps: 151    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1395   score: 6.0   memory length: 289711   epsilon: 0.6243702400081546    steps: 392    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1396   score: 1.0   memory length: 289862   epsilon: 0.624071260008161    steps: 151    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1397   score: 3.0   memory length: 290126   epsilon: 0.6235485400081724    steps: 264    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1398   score: 2.0   memory length: 290308   epsilon: 0.6231881800081802    steps: 182    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1399   score: 5.0   memory length: 290672   epsilon: 0.6224674600081959    steps: 364    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1400   score: 7.0   memory length: 291059   epsilon: 0.6217012000082125    steps: 387    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1401   score: 1.0   memory length: 291209   epsilon: 0.6214042000082189    steps: 150    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1402   score: 6.0   memory length: 291576   epsilon: 0.6206775400082347    steps: 367    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1403   score: 4.0   memory length: 291873   epsilon: 0.6200894800082475    steps: 297    lr: 4e-05     evaluation reward: 4.04\n",
      "episode: 1404   score: 8.0   memory length: 292301   epsilon: 0.6192420400082659    steps: 428    lr: 4e-05     evaluation reward: 4.07\n",
      "episode: 1405   score: 2.0   memory length: 292501   epsilon: 0.6188460400082745    steps: 200    lr: 4e-05     evaluation reward: 4.09\n",
      "episode: 1406   score: 6.0   memory length: 292872   epsilon: 0.6181114600082904    steps: 371    lr: 4e-05     evaluation reward: 4.14\n",
      "episode: 1407   score: 1.0   memory length: 293042   epsilon: 0.6177748600082977    steps: 170    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1408   score: 5.0   memory length: 293387   epsilon: 0.6170917600083126    steps: 345    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1409   score: 5.0   memory length: 293727   epsilon: 0.6164185600083272    steps: 340    lr: 4e-05     evaluation reward: 4.2\n",
      "episode: 1410   score: 2.0   memory length: 293908   epsilon: 0.616060180008335    steps: 181    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1411   score: 3.0   memory length: 294117   epsilon: 0.6156463600083439    steps: 209    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1412   score: 9.0   memory length: 294424   epsilon: 0.6150385000083571    steps: 307    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1413   score: 3.0   memory length: 294687   epsilon: 0.6145177600083684    steps: 263    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1414   score: 2.0   memory length: 294887   epsilon: 0.614121760008377    steps: 200    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1415   score: 6.0   memory length: 295265   epsilon: 0.6133733200083933    steps: 378    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1416   score: 5.0   memory length: 295558   epsilon: 0.6127931800084059    steps: 293    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1417   score: 3.0   memory length: 295771   epsilon: 0.612371440008415    steps: 213    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1418   score: 5.0   memory length: 296077   epsilon: 0.6117655600084282    steps: 306    lr: 4e-05     evaluation reward: 4.2\n",
      "episode: 1419   score: 8.0   memory length: 296474   epsilon: 0.6109795000084453    steps: 397    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1420   score: 4.0   memory length: 296770   epsilon: 0.610393420008458    steps: 296    lr: 4e-05     evaluation reward: 4.2\n",
      "episode: 1421   score: 3.0   memory length: 296980   epsilon: 0.609977620008467    steps: 210    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1422   score: 3.0   memory length: 297226   epsilon: 0.6094905400084776    steps: 246    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1423   score: 4.0   memory length: 297487   epsilon: 0.6089737600084888    steps: 261    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1424   score: 4.0   memory length: 297747   epsilon: 0.6084589600085    steps: 260    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1425   score: 9.0   memory length: 298211   epsilon: 0.6075402400085199    steps: 464    lr: 4e-05     evaluation reward: 4.25\n",
      "episode: 1426   score: 2.0   memory length: 298428   epsilon: 0.6071105800085292    steps: 217    lr: 4e-05     evaluation reward: 4.2\n",
      "episode: 1427   score: 5.0   memory length: 298749   epsilon: 0.606475000008543    steps: 321    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1428   score: 4.0   memory length: 299040   epsilon: 0.6058988200085555    steps: 291    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1429   score: 2.0   memory length: 299220   epsilon: 0.6055424200085633    steps: 180    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1430   score: 5.0   memory length: 299509   epsilon: 0.6049702000085757    steps: 289    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1431   score: 4.0   memory length: 299768   epsilon: 0.6044573800085868    steps: 259    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1432   score: 4.0   memory length: 300026   epsilon: 0.6039465400085979    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1433   score: 10.0   memory length: 300497   epsilon: 0.6030139600086182    steps: 471    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1434   score: 2.0   memory length: 300694   epsilon: 0.6026239000086266    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1435   score: 2.0   memory length: 300914   epsilon: 0.6021883000086361    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1436   score: 4.0   memory length: 301155   epsilon: 0.6017111200086465    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1437   score: 2.0   memory length: 301352   epsilon: 0.6013210600086549    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1438   score: 2.0   memory length: 301534   epsilon: 0.6009607000086628    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1439   score: 8.0   memory length: 301916   epsilon: 0.6002043400086792    steps: 382    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1440   score: 7.0   memory length: 302342   epsilon: 0.5993608600086975    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1441   score: 3.0   memory length: 302586   epsilon: 0.598877740008708    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1442   score: 9.0   memory length: 302914   epsilon: 0.5982283000087221    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1443   score: 8.0   memory length: 303338   epsilon: 0.5973887800087403    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1444   score: 7.0   memory length: 303759   epsilon: 0.5965552000087584    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 4.35\n",
      "episode: 1445   score: 1.0   memory length: 303910   epsilon: 0.5962562200087649    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.36\n",
      "episode: 1446   score: 5.0   memory length: 304236   epsilon: 0.5956107400087789    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1447   score: 5.0   memory length: 304582   epsilon: 0.5949256600087938    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1448   score: 6.0   memory length: 304938   epsilon: 0.5942207800088091    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.36\n",
      "episode: 1449   score: 7.0   memory length: 305345   epsilon: 0.5934149200088266    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1450   score: 4.0   memory length: 305619   epsilon: 0.5928724000088383    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.41\n",
      "episode: 1451   score: 5.0   memory length: 305928   epsilon: 0.5922605800088516    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1452   score: 7.0   memory length: 306374   epsilon: 0.5913775000088708    steps: 446    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1453   score: 5.0   memory length: 306658   epsilon: 0.590815180008883    steps: 284    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1454   score: 5.0   memory length: 306969   epsilon: 0.5901994000088964    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 4.43\n",
      "episode: 1455   score: 5.0   memory length: 307280   epsilon: 0.5895836200089097    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 4.44\n",
      "episode: 1456   score: 7.0   memory length: 307666   epsilon: 0.5888193400089263    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 4.46\n",
      "episode: 1457   score: 10.0   memory length: 308175   epsilon: 0.5878115200089482    steps: 509    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
      "episode: 1458   score: 9.0   memory length: 308676   epsilon: 0.5868195400089697    steps: 501    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
      "episode: 1459   score: 6.0   memory length: 308990   epsilon: 0.5861978200089832    steps: 314    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1460   score: 5.0   memory length: 309333   epsilon: 0.585518680008998    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1461   score: 6.0   memory length: 309711   epsilon: 0.5847702400090142    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1462   score: 6.0   memory length: 310085   epsilon: 0.5840297200090303    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1463   score: 5.0   memory length: 310376   epsilon: 0.5834535400090428    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1464   score: 7.0   memory length: 310753   epsilon: 0.582707080009059    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1465   score: 4.0   memory length: 311011   epsilon: 0.5821962400090701    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1466   score: 6.0   memory length: 311373   epsilon: 0.5814794800090857    steps: 362    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1467   score: 2.0   memory length: 311593   epsilon: 0.5810438800090951    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1468   score: 7.0   memory length: 312000   epsilon: 0.5802380200091126    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
      "episode: 1469   score: 7.0   memory length: 312300   epsilon: 0.5796440200091255    steps: 300    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1470   score: 4.0   memory length: 312541   epsilon: 0.5791668400091359    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1471   score: 3.0   memory length: 312750   epsilon: 0.5787530200091449    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1472   score: 4.0   memory length: 313008   epsilon: 0.578242180009156    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1473   score: 7.0   memory length: 313357   epsilon: 0.577551160009171    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1474   score: 1.0   memory length: 313528   epsilon: 0.5772125800091783    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1475   score: 10.0   memory length: 314077   epsilon: 0.5761255600092019    steps: 549    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
      "episode: 1476   score: 2.0   memory length: 314256   epsilon: 0.5757711400092096    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1477   score: 4.0   memory length: 314535   epsilon: 0.5752187200092216    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.79\n",
      "episode: 1478   score: 6.0   memory length: 314903   epsilon: 0.5744900800092374    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1479   score: 8.0   memory length: 315377   epsilon: 0.5735515600092578    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 4.88\n",
      "episode: 1480   score: 4.0   memory length: 315654   epsilon: 0.5730031000092697    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1481   score: 5.0   memory length: 315943   epsilon: 0.5724308800092821    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1482   score: 3.0   memory length: 316170   epsilon: 0.5719814200092919    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.83\n",
      "episode: 1483   score: 3.0   memory length: 316399   epsilon: 0.5715280000093017    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1484   score: 5.0   memory length: 316708   epsilon: 0.570916180009315    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
      "episode: 1485   score: 6.0   memory length: 317073   epsilon: 0.5701934800093307    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
      "episode: 1486   score: 3.0   memory length: 317306   epsilon: 0.5697321400093407    steps: 233    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1487   score: 6.0   memory length: 317614   epsilon: 0.5691223000093539    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1488   score: 4.0   memory length: 317911   epsilon: 0.5685342400093667    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 4.83\n",
      "episode: 1489   score: 3.0   memory length: 318121   epsilon: 0.5681184400093757    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
      "episode: 1490   score: 9.0   memory length: 318577   epsilon: 0.5672155600093953    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
      "episode: 1491   score: 7.0   memory length: 318987   epsilon: 0.566403760009413    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
      "episode: 1492   score: 3.0   memory length: 319216   epsilon: 0.5659503400094228    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1493   score: 4.0   memory length: 319454   epsilon: 0.565479100009433    steps: 238    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
      "episode: 1494   score: 5.0   memory length: 319764   epsilon: 0.5648653000094463    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1495   score: 4.0   memory length: 320060   epsilon: 0.5642792200094591    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1496   score: 1.0   memory length: 320211   epsilon: 0.5639802400094656    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1497   score: 5.0   memory length: 320517   epsilon: 0.5633743600094787    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1498   score: 2.0   memory length: 320717   epsilon: 0.5629783600094873    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1499   score: 7.0   memory length: 321137   epsilon: 0.5621467600095054    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 4.88\n",
      "episode: 1500   score: 5.0   memory length: 321426   epsilon: 0.5615745400095178    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1501   score: 4.0   memory length: 321700   epsilon: 0.5610320200095296    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
      "episode: 1502   score: 5.0   memory length: 322025   epsilon: 0.5603885200095435    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.88\n",
      "episode: 1503   score: 1.0   memory length: 322176   epsilon: 0.56008954000955    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
      "episode: 1504   score: 3.0   memory length: 322386   epsilon: 0.559673740009559    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1505   score: 5.0   memory length: 322712   epsilon: 0.5590282600095731    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 4.83\n",
      "episode: 1506   score: 5.0   memory length: 323021   epsilon: 0.5584164400095863    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
      "episode: 1507   score: 4.0   memory length: 323339   epsilon: 0.5577868000096    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
      "episode: 1508   score: 6.0   memory length: 323666   epsilon: 0.5571393400096141    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1509   score: 3.0   memory length: 323877   epsilon: 0.5567215600096231    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1510   score: 4.0   memory length: 324135   epsilon: 0.5562107200096342    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.86\n",
      "episode: 1511   score: 4.0   memory length: 324409   epsilon: 0.555668200009646    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1512   score: 4.0   memory length: 324665   epsilon: 0.555161320009657    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
      "episode: 1513   score: 2.0   memory length: 324863   epsilon: 0.5547692800096655    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1514   score: 13.0   memory length: 325529   epsilon: 0.5534506000096941    steps: 666    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
      "episode: 1515   score: 9.0   memory length: 325881   epsilon: 0.5527536400097093    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
      "episode: 1516   score: 7.0   memory length: 326261   epsilon: 0.5520012400097256    steps: 380    lr: 1.6000000000000003e-05     evaluation reward: 4.97\n",
      "episode: 1517   score: 5.0   memory length: 326569   epsilon: 0.5513914000097389    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1518   score: 5.0   memory length: 326893   epsilon: 0.5507498800097528    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1519   score: 2.0   memory length: 327093   epsilon: 0.5503538800097614    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1520   score: 2.0   memory length: 327312   epsilon: 0.5499202600097708    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1521   score: 4.0   memory length: 327553   epsilon: 0.5494430800097811    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
      "episode: 1522   score: 2.0   memory length: 327773   epsilon: 0.5490074800097906    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1523   score: 5.0   memory length: 328079   epsilon: 0.5484016000098038    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
      "episode: 1524   score: 5.0   memory length: 328385   epsilon: 0.5477957200098169    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1525   score: 4.0   memory length: 328658   epsilon: 0.5472551800098286    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 4.88\n",
      "episode: 1526   score: 5.0   memory length: 328962   epsilon: 0.5466532600098417    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1527   score: 8.0   memory length: 329428   epsilon: 0.5457305800098617    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
      "episode: 1528   score: 3.0   memory length: 329638   epsilon: 0.5453147800098708    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1529   score: 1.0   memory length: 329789   epsilon: 0.5450158000098773    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
      "episode: 1530   score: 6.0   memory length: 330206   epsilon: 0.5441901400098952    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1531   score: 3.0   memory length: 330454   epsilon: 0.5436991000099058    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
      "episode: 1532   score: 5.0   memory length: 330757   epsilon: 0.5430991600099189    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1533   score: 4.0   memory length: 331000   epsilon: 0.5426180200099293    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1534   score: 4.0   memory length: 331240   epsilon: 0.5421428200099396    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
      "episode: 1535   score: 0.0   memory length: 331362   epsilon: 0.5419012600099449    steps: 122    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1536   score: 4.0   memory length: 331656   epsilon: 0.5413191400099575    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
      "episode: 1537   score: 6.0   memory length: 332015   epsilon: 0.5406083200099729    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1538   score: 9.0   memory length: 332500   epsilon: 0.5396480200099938    steps: 485    lr: 1.6000000000000003e-05     evaluation reward: 4.98\n",
      "episode: 1539   score: 11.0   memory length: 333058   epsilon: 0.5385431800100178    steps: 558    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1540   score: 6.0   memory length: 333408   epsilon: 0.5378501800100328    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
      "episode: 1541   score: 4.0   memory length: 333687   epsilon: 0.5372977600100448    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1542   score: 7.0   memory length: 334055   epsilon: 0.5365691200100606    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1543   score: 12.0   memory length: 334613   epsilon: 0.5354642800100846    steps: 558    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1544   score: 10.0   memory length: 334986   epsilon: 0.5347257400101006    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1545   score: 2.0   memory length: 335187   epsilon: 0.5343277600101093    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
      "episode: 1546   score: 3.0   memory length: 335396   epsilon: 0.5339139400101183    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
      "episode: 1547   score: 7.0   memory length: 335795   epsilon: 0.5331239200101354    steps: 399    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
      "episode: 1548   score: 5.0   memory length: 336123   epsilon: 0.5324744800101495    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1549   score: 9.0   memory length: 336556   epsilon: 0.5316171400101681    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1550   score: 7.0   memory length: 336883   epsilon: 0.5309696800101822    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1551   score: 5.0   memory length: 337204   epsilon: 0.530334100010196    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1552   score: 3.0   memory length: 337448   epsilon: 0.5298509800102065    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
      "episode: 1553   score: 4.0   memory length: 337746   epsilon: 0.5292609400102193    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1554   score: 4.0   memory length: 338004   epsilon: 0.5287501000102304    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
      "episode: 1555   score: 4.0   memory length: 338277   epsilon: 0.5282095600102421    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1556   score: 6.0   memory length: 338582   epsilon: 0.5276056600102552    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1557   score: 6.0   memory length: 338939   epsilon: 0.5268988000102706    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1558   score: 3.0   memory length: 339170   epsilon: 0.5264414200102805    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1559   score: 7.0   memory length: 339554   epsilon: 0.525681100010297    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
      "episode: 1560   score: 6.0   memory length: 339933   epsilon: 0.5249306800103133    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
      "episode: 1561   score: 2.0   memory length: 340113   epsilon: 0.524574280010321    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1562   score: 8.0   memory length: 340577   epsilon: 0.523655560010341    steps: 464    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1563   score: 7.0   memory length: 340956   epsilon: 0.5229051400103573    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
      "episode: 1564   score: 4.0   memory length: 341232   epsilon: 0.5223586600103691    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
      "episode: 1565   score: 3.0   memory length: 341443   epsilon: 0.5219408800103782    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1566   score: 5.0   memory length: 341767   epsilon: 0.5212993600103921    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.9\n",
      "episode: 1567   score: 5.0   memory length: 342054   epsilon: 0.5207311000104045    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1568   score: 10.0   memory length: 342598   epsilon: 0.5196539800104278    steps: 544    lr: 1.6000000000000003e-05     evaluation reward: 4.96\n",
      "episode: 1569   score: 6.0   memory length: 342935   epsilon: 0.5189867200104423    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
      "episode: 1570   score: 3.0   memory length: 343167   epsilon: 0.5185273600104523    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
      "episode: 1571   score: 11.0   memory length: 343631   epsilon: 0.5176086400104722    steps: 464    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1572   score: 5.0   memory length: 343976   epsilon: 0.5169255400104871    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1573   score: 6.0   memory length: 344331   epsilon: 0.5162226400105023    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1574   score: 4.0   memory length: 344591   epsilon: 0.5157078400105135    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
      "episode: 1575   score: 8.0   memory length: 345037   epsilon: 0.5148247600105327    steps: 446    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1576   score: 4.0   memory length: 345296   epsilon: 0.5143119400105438    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
      "episode: 1577   score: 4.0   memory length: 345542   epsilon: 0.5138248600105544    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
      "episode: 1578   score: 9.0   memory length: 345956   epsilon: 0.5130051400105722    steps: 414    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1579   score: 4.0   memory length: 346232   epsilon: 0.512458660010584    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1580   score: 9.0   memory length: 346752   epsilon: 0.5114290600106064    steps: 520    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1581   score: 5.0   memory length: 347038   epsilon: 0.5108627800106187    steps: 286    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1582   score: 3.0   memory length: 347266   epsilon: 0.5104113400106285    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1583   score: 3.0   memory length: 347493   epsilon: 0.5099618800106382    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1584   score: 5.0   memory length: 347818   epsilon: 0.5093183800106522    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1585   score: 7.0   memory length: 348225   epsilon: 0.5085125200106697    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 5.1\n",
      "episode: 1586   score: 5.0   memory length: 348533   epsilon: 0.507902680010683    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
      "episode: 1587   score: 2.0   memory length: 348731   epsilon: 0.5075106400106915    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1588   score: 5.0   memory length: 349053   epsilon: 0.5068730800107053    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1589   score: 8.0   memory length: 349435   epsilon: 0.5061167200107217    steps: 382    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
      "episode: 1590   score: 9.0   memory length: 349937   epsilon: 0.5051227600107433    steps: 502    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
      "episode: 1591   score: 8.0   memory length: 350382   epsilon: 0.5042416600107624    steps: 445    lr: 1.6000000000000003e-05     evaluation reward: 5.15\n",
      "episode: 1592   score: 8.0   memory length: 350778   epsilon: 0.5034575800107794    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1593   score: 5.0   memory length: 351084   epsilon: 0.5028517000107926    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1594   score: 12.0   memory length: 351556   epsilon: 0.5019171400108129    steps: 472    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1595   score: 5.0   memory length: 351901   epsilon: 0.5012340400108277    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
      "episode: 1596   score: 4.0   memory length: 352177   epsilon: 0.5006875600108396    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1597   score: 5.0   memory length: 352500   epsilon: 0.5000480200108535    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1598   score: 6.0   memory length: 352837   epsilon: 0.4993807600108506    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1599   score: 8.0   memory length: 353239   epsilon: 0.49858480001084554    steps: 402    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1600   score: 9.0   memory length: 353619   epsilon: 0.4978324000108408    steps: 380    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1601   score: 10.0   memory length: 354147   epsilon: 0.49678696001083417    steps: 528    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1602   score: 5.0   memory length: 354439   epsilon: 0.4962088000108305    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1603   score: 3.0   memory length: 354649   epsilon: 0.4957930000108279    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1604   score: 5.0   memory length: 354973   epsilon: 0.4951514800108238    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1605   score: 7.0   memory length: 355377   epsilon: 0.49435156001081876    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1606   score: 6.0   memory length: 355715   epsilon: 0.4936823200108145    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1607   score: 2.0   memory length: 355931   epsilon: 0.4932546400108118    steps: 216    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1608   score: 5.0   memory length: 356239   epsilon: 0.49264480001080796    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1609   score: 4.0   memory length: 356497   epsilon: 0.4921339600108047    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1610   score: 4.0   memory length: 356778   epsilon: 0.4915775800108012    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1611   score: 4.0   memory length: 357053   epsilon: 0.49103308001079776    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1612   score: 6.0   memory length: 357373   epsilon: 0.49039948001079375    steps: 320    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1613   score: 3.0   memory length: 357583   epsilon: 0.4899836800107911    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1614   score: 6.0   memory length: 357904   epsilon: 0.4893481000107871    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1615   score: 3.0   memory length: 358135   epsilon: 0.4888907200107842    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1616   score: 4.0   memory length: 358411   epsilon: 0.48834424001078075    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1617   score: 8.0   memory length: 358864   epsilon: 0.4874473000107751    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1618   score: 6.0   memory length: 359239   epsilon: 0.4867048000107704    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1619   score: 7.0   memory length: 359640   epsilon: 0.48591082001076535    steps: 401    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1620   score: 2.0   memory length: 359839   epsilon: 0.48551680001076286    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1621   score: 4.0   memory length: 360111   epsilon: 0.48497824001075945    steps: 272    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1622   score: 2.0   memory length: 360293   epsilon: 0.4846178800107572    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1623   score: 5.0   memory length: 360600   epsilon: 0.48401002001075333    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1624   score: 4.0   memory length: 360861   epsilon: 0.48349324001075006    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1625   score: 7.0   memory length: 361244   epsilon: 0.48273490001074526    steps: 383    lr: 1.6000000000000003e-05     evaluation reward: 5.5\n",
      "episode: 1626   score: 3.0   memory length: 361473   epsilon: 0.4822814800107424    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1627   score: 7.0   memory length: 361878   epsilon: 0.4814795800107373    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1628   score: 5.0   memory length: 362203   epsilon: 0.48083608001073325    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1629   score: 1.0   memory length: 362354   epsilon: 0.48053710001073136    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1630   score: 9.0   memory length: 362697   epsilon: 0.47985796001072706    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1631   score: 8.0   memory length: 363114   epsilon: 0.47903230001072183    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1632   score: 4.0   memory length: 363352   epsilon: 0.47856106001071885    steps: 238    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1633   score: 4.0   memory length: 363590   epsilon: 0.47808982001071587    steps: 238    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1634   score: 6.0   memory length: 363945   epsilon: 0.4773869200107114    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1635   score: 10.0   memory length: 364460   epsilon: 0.476367220010705    steps: 515    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
      "episode: 1636   score: 1.0   memory length: 364611   epsilon: 0.4760682400107031    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
      "episode: 1637   score: 7.0   memory length: 365030   epsilon: 0.47523862001069783    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
      "episode: 1638   score: 7.0   memory length: 365454   epsilon: 0.4743991000106925    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
      "episode: 1639   score: 4.0   memory length: 365696   epsilon: 0.4739199400106895    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1640   score: 4.0   memory length: 365937   epsilon: 0.47344276001068647    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1641   score: 7.0   memory length: 366341   epsilon: 0.4726428400106814    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1642   score: 6.0   memory length: 366737   epsilon: 0.47185876001067645    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1643   score: 4.0   memory length: 366999   epsilon: 0.47134000001067317    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1644   score: 8.0   memory length: 367478   epsilon: 0.47039158001066717    steps: 479    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1645   score: 6.0   memory length: 367854   epsilon: 0.46964710001066245    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1646   score: 0.0   memory length: 367977   epsilon: 0.4694035600106609    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1647   score: 5.0   memory length: 368284   epsilon: 0.46879570001065707    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1648   score: 4.0   memory length: 368578   epsilon: 0.4682135800106534    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1649   score: 9.0   memory length: 369040   epsilon: 0.4672988200106476    steps: 462    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1650   score: 8.0   memory length: 369445   epsilon: 0.4664969200106425    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1651   score: 5.0   memory length: 369776   epsilon: 0.4658415400106384    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1652   score: 10.0   memory length: 370250   epsilon: 0.46490302001063244    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1653   score: 7.0   memory length: 370637   epsilon: 0.4641367600106276    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1654   score: 6.0   memory length: 371001   epsilon: 0.46341604001062303    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1655   score: 2.0   memory length: 371180   epsilon: 0.4630616200106208    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1656   score: 6.0   memory length: 371527   epsilon: 0.46237456001061644    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1657   score: 4.0   memory length: 371782   epsilon: 0.46186966001061325    steps: 255    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1658   score: 3.0   memory length: 372012   epsilon: 0.46141426001061037    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1659   score: 8.0   memory length: 372448   epsilon: 0.4605509800106049    steps: 436    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1660   score: 6.0   memory length: 372786   epsilon: 0.45988174001060067    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1661   score: 3.0   memory length: 373016   epsilon: 0.4594263400105978    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1662   score: 4.0   memory length: 373258   epsilon: 0.45894718001059476    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1663   score: 4.0   memory length: 373517   epsilon: 0.4584343600105915    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1664   score: 8.0   memory length: 373903   epsilon: 0.4576700800105867    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1665   score: 3.0   memory length: 374155   epsilon: 0.4571711200105835    steps: 252    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1666   score: 10.0   memory length: 374654   epsilon: 0.45618310001057727    steps: 499    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1667   score: 3.0   memory length: 374864   epsilon: 0.45576730001057464    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1668   score: 10.0   memory length: 375217   epsilon: 0.4550683600105702    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1669   score: 10.0   memory length: 375728   epsilon: 0.4540565800105638    steps: 511    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
      "episode: 1670   score: 3.0   memory length: 375979   epsilon: 0.45355960001056067    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
      "episode: 1671   score: 11.0   memory length: 376366   epsilon: 0.4527933400105558    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
      "episode: 1672   score: 6.0   memory length: 376703   epsilon: 0.4521260800105516    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
      "episode: 1673   score: 8.0   memory length: 377137   epsilon: 0.45126676001054616    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1674   score: 9.0   memory length: 377590   epsilon: 0.4503698200105405    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
      "episode: 1675   score: 8.0   memory length: 378017   epsilon: 0.44952436001053514    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
      "episode: 1676   score: 9.0   memory length: 378467   epsilon: 0.4486333600105295    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
      "episode: 1677   score: 12.0   memory length: 378945   epsilon: 0.4476869200105235    steps: 478    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1678   score: 10.0   memory length: 379472   epsilon: 0.4466434600105169    steps: 527    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1679   score: 6.0   memory length: 379807   epsilon: 0.4459801600105127    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1680   score: 10.0   memory length: 380300   epsilon: 0.44500402001050654    steps: 493    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
      "episode: 1681   score: 4.0   memory length: 380541   epsilon: 0.4445268400105035    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1682   score: 5.0   memory length: 380830   epsilon: 0.4439546200104999    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1683   score: 1.0   memory length: 380980   epsilon: 0.443657620010498    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1684   score: 9.0   memory length: 381444   epsilon: 0.4427389000104922    steps: 464    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1685   score: 9.0   memory length: 381875   epsilon: 0.4418855200104868    steps: 431    lr: 1.6000000000000003e-05     evaluation reward: 5.9\n",
      "episode: 1686   score: 10.0   memory length: 382407   epsilon: 0.44083216001048015    steps: 532    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1687   score: 8.0   memory length: 382841   epsilon: 0.4399728400104747    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1688   score: 10.0   memory length: 383359   epsilon: 0.4389472000104682    steps: 518    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
      "episode: 1689   score: 8.0   memory length: 383828   epsilon: 0.43801858001046234    steps: 469    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
      "episode: 1690   score: 6.0   memory length: 384184   epsilon: 0.4373137000104579    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 6.03\n",
      "episode: 1691   score: 5.0   memory length: 384533   epsilon: 0.4366226800104535    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 6.0\n",
      "episode: 1692   score: 8.0   memory length: 384929   epsilon: 0.43583860001044855    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 6.0\n",
      "episode: 1693   score: 5.0   memory length: 385242   epsilon: 0.43521886001044463    steps: 313    lr: 1.6000000000000003e-05     evaluation reward: 6.0\n",
      "episode: 1694   score: 6.0   memory length: 385578   epsilon: 0.4345535800104404    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 5.94\n",
      "episode: 1695   score: 8.0   memory length: 386007   epsilon: 0.43370416001043505    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 5.97\n",
      "episode: 1696   score: 5.0   memory length: 386299   epsilon: 0.4331260000104314    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.98\n",
      "episode: 1697   score: 6.0   memory length: 386651   epsilon: 0.432429040010427    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 5.99\n",
      "episode: 1698   score: 14.0   memory length: 387044   epsilon: 0.43165090001042206    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 6.07\n",
      "episode: 1699   score: 3.0   memory length: 387257   epsilon: 0.4312291600104194    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1700   score: 3.0   memory length: 387483   epsilon: 0.43078168001041656    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 5.96\n",
      "episode: 1701   score: 8.0   memory length: 387858   epsilon: 0.43003918001041186    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.94\n",
      "episode: 1702   score: 6.0   memory length: 388194   epsilon: 0.42937390001040765    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1703   score: 7.0   memory length: 388566   epsilon: 0.428637340010403    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 5.99\n",
      "episode: 1704   score: 8.0   memory length: 388983   epsilon: 0.42781168001039777    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1705   score: 5.0   memory length: 389258   epsilon: 0.4272671800103943    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 6.0\n",
      "episode: 1706   score: 6.0   memory length: 389610   epsilon: 0.4265702200103899    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 6.0\n",
      "episode: 1707   score: 6.0   memory length: 389927   epsilon: 0.42594256001038594    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 6.04\n",
      "episode: 1708   score: 7.0   memory length: 390294   epsilon: 0.42521590001038134    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
      "episode: 1709   score: 11.0   memory length: 390741   epsilon: 0.42433084001037574    steps: 447    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
      "episode: 1710   score: 5.0   memory length: 391032   epsilon: 0.4237546600103721    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 6.14\n",
      "episode: 1711   score: 3.0   memory length: 391279   epsilon: 0.423265600010369    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
      "episode: 1712   score: 6.0   memory length: 391614   epsilon: 0.4226023000103648    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
      "episode: 1713   score: 9.0   memory length: 392104   epsilon: 0.42163210001035867    steps: 490    lr: 1.6000000000000003e-05     evaluation reward: 6.19\n",
      "episode: 1714   score: 8.0   memory length: 392530   epsilon: 0.42078862001035333    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 6.21\n",
      "episode: 1715   score: 9.0   memory length: 392993   epsilon: 0.41987188001034753    steps: 463    lr: 1.6000000000000003e-05     evaluation reward: 6.27\n",
      "episode: 1716   score: 5.0   memory length: 393298   epsilon: 0.4192679800103437    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1717   score: 4.0   memory length: 393593   epsilon: 0.41868388001034    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1718   score: 6.0   memory length: 393984   epsilon: 0.4179097000103351    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1719   score: 5.0   memory length: 394290   epsilon: 0.4173038200103313    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 6.22\n",
      "episode: 1720   score: 8.0   memory length: 394716   epsilon: 0.41646034001032595    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1721   score: 8.0   memory length: 395121   epsilon: 0.4156584400103209    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 6.32\n",
      "episode: 1722   score: 3.0   memory length: 395372   epsilon: 0.41516146001031773    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
      "episode: 1723   score: 2.0   memory length: 395575   epsilon: 0.4147595200103152    steps: 203    lr: 1.6000000000000003e-05     evaluation reward: 6.3\n",
      "episode: 1724   score: 7.0   memory length: 395979   epsilon: 0.4139596000103101    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
      "episode: 1725   score: 5.0   memory length: 396286   epsilon: 0.4133517400103063    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1726   score: 5.0   memory length: 396597   epsilon: 0.4127359600103024    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
      "episode: 1727   score: 5.0   memory length: 396908   epsilon: 0.4121201800102985    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1728   score: 4.0   memory length: 397171   epsilon: 0.4115994400102952    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 6.3\n",
      "episode: 1729   score: 1.0   memory length: 397321   epsilon: 0.4113024400102933    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 6.3\n",
      "episode: 1730   score: 7.0   memory length: 397671   epsilon: 0.41060944001028893    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1731   score: 9.0   memory length: 398139   epsilon: 0.40968280001028307    steps: 468    lr: 1.6000000000000003e-05     evaluation reward: 6.29\n",
      "episode: 1732   score: 4.0   memory length: 398396   epsilon: 0.40917394001027985    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 6.29\n",
      "episode: 1733   score: 1.0   memory length: 398546   epsilon: 0.40887694001027797    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1734   score: 4.0   memory length: 398826   epsilon: 0.40832254001027446    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1735   score: 6.0   memory length: 399185   epsilon: 0.40761172001026996    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 6.2\n",
      "episode: 1736   score: 7.0   memory length: 399570   epsilon: 0.40684942001026514    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1737   score: 4.0   memory length: 399862   epsilon: 0.4062712600102615    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
      "episode: 1738   score: 12.0   memory length: 400339   epsilon: 0.4053268000102555    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 1739   score: 11.0   memory length: 400856   epsilon: 0.40430314001024903    steps: 517    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 1740   score: 9.0   memory length: 401311   epsilon: 0.40340224001024333    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1741   score: 4.0   memory length: 401604   epsilon: 0.40282210001023966    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 1742   score: 12.0   memory length: 402087   epsilon: 0.4018657600102336    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 1743   score: 3.0   memory length: 402316   epsilon: 0.40141234001023074    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 1744   score: 7.0   memory length: 402683   epsilon: 0.40068568001022614    steps: 367    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1745   score: 6.0   memory length: 403035   epsilon: 0.39998872001022173    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1746   score: 4.0   memory length: 403333   epsilon: 0.399398680010218    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 6.45\n",
      "episode: 1747   score: 8.0   memory length: 403747   epsilon: 0.3985789600102128    steps: 414    lr: 6.400000000000001e-06     evaluation reward: 6.48\n",
      "episode: 1748   score: 6.0   memory length: 404122   epsilon: 0.3978364600102081    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 6.5\n",
      "episode: 1749   score: 11.0   memory length: 404550   epsilon: 0.39698902001020275    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
      "episode: 1750   score: 5.0   memory length: 404897   epsilon: 0.3963019600101984    steps: 347    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 1751   score: 8.0   memory length: 405307   epsilon: 0.39549016001019327    steps: 410    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
      "episode: 1752   score: 8.0   memory length: 405766   epsilon: 0.3945813400101875    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 6.5\n",
      "episode: 1753   score: 3.0   memory length: 405991   epsilon: 0.3941358400101847    steps: 225    lr: 6.400000000000001e-06     evaluation reward: 6.46\n",
      "episode: 1754   score: 11.0   memory length: 406518   epsilon: 0.3930923800101781    steps: 527    lr: 6.400000000000001e-06     evaluation reward: 6.51\n",
      "episode: 1755   score: 13.0   memory length: 407146   epsilon: 0.39184894001017023    steps: 628    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 1756   score: 9.0   memory length: 407654   epsilon: 0.39084310001016387    steps: 508    lr: 6.400000000000001e-06     evaluation reward: 6.65\n",
      "episode: 1757   score: 12.0   memory length: 408240   epsilon: 0.3896828200101565    steps: 586    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1758   score: 16.0   memory length: 408752   epsilon: 0.3886690600101501    steps: 512    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1759   score: 4.0   memory length: 409011   epsilon: 0.38815624001014687    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1760   score: 7.0   memory length: 409375   epsilon: 0.3874355200101423    steps: 364    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1761   score: 7.0   memory length: 409743   epsilon: 0.3867068800101377    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 6.87\n",
      "episode: 1762   score: 7.0   memory length: 410132   epsilon: 0.3859366600101328    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1763   score: 8.0   memory length: 410586   epsilon: 0.38503774001012714    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
      "episode: 1764   score: 6.0   memory length: 410909   epsilon: 0.3843982000101231    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 6.92\n",
      "episode: 1765   score: 7.0   memory length: 411331   epsilon: 0.3835626400101178    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 6.96\n",
      "episode: 1766   score: 6.0   memory length: 411703   epsilon: 0.38282608001011315    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 6.92\n",
      "episode: 1767   score: 10.0   memory length: 412190   epsilon: 0.38186182001010704    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 6.99\n",
      "episode: 1768   score: 6.0   memory length: 412512   epsilon: 0.381224260010103    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 6.95\n",
      "episode: 1769   score: 5.0   memory length: 412820   epsilon: 0.38061442001009915    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1770   score: 7.0   memory length: 413234   epsilon: 0.37979470001009397    steps: 414    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
      "episode: 1771   score: 5.0   memory length: 413544   epsilon: 0.3791809000100901    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1772   score: 11.0   memory length: 414150   epsilon: 0.3779810200100825    steps: 606    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
      "episode: 1773   score: 6.0   memory length: 414548   epsilon: 0.3771929800100775    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
      "episode: 1774   score: 16.0   memory length: 415205   epsilon: 0.3758921200100693    steps: 657    lr: 6.400000000000001e-06     evaluation reward: 6.98\n",
      "episode: 1775   score: 10.0   memory length: 415690   epsilon: 0.3749318200100632    steps: 485    lr: 6.400000000000001e-06     evaluation reward: 7.0\n",
      "episode: 1776   score: 13.0   memory length: 416343   epsilon: 0.373638880010055    steps: 653    lr: 6.400000000000001e-06     evaluation reward: 7.04\n",
      "episode: 1777   score: 4.0   memory length: 416638   epsilon: 0.3730547800100513    steps: 295    lr: 6.400000000000001e-06     evaluation reward: 6.96\n",
      "episode: 1778   score: 10.0   memory length: 416993   epsilon: 0.3723518800100469    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 6.96\n",
      "episode: 1779   score: 9.0   memory length: 417464   epsilon: 0.371419300010041    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 6.99\n",
      "episode: 1780   score: 14.0   memory length: 418002   epsilon: 0.37035406001003424    steps: 538    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
      "episode: 1781   score: 7.0   memory length: 418370   epsilon: 0.3696254200100296    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
      "episode: 1782   score: 15.0   memory length: 418959   epsilon: 0.36845920001002225    steps: 589    lr: 6.400000000000001e-06     evaluation reward: 7.16\n",
      "episode: 1783   score: 8.0   memory length: 419374   epsilon: 0.36763750001001705    steps: 415    lr: 6.400000000000001e-06     evaluation reward: 7.23\n",
      "episode: 1784   score: 17.0   memory length: 419860   epsilon: 0.36667522001001096    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1785   score: 9.0   memory length: 420348   epsilon: 0.36570898001000485    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1786   score: 11.0   memory length: 420898   epsilon: 0.36461998000999796    steps: 550    lr: 6.400000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1787   score: 8.0   memory length: 421326   epsilon: 0.3637725400099926    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1788   score: 9.0   memory length: 421765   epsilon: 0.3629033200099871    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1789   score: 7.0   memory length: 422151   epsilon: 0.36213904000998226    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 7.3\n",
      "episode: 1790   score: 11.0   memory length: 422686   epsilon: 0.36107974000997556    steps: 535    lr: 6.400000000000001e-06     evaluation reward: 7.35\n",
      "episode: 1791   score: 10.0   memory length: 423178   epsilon: 0.3601055800099694    steps: 492    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1792   score: 6.0   memory length: 423534   epsilon: 0.35940070000996494    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1793   score: 6.0   memory length: 423889   epsilon: 0.3586978000099605    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1794   score: 5.0   memory length: 424180   epsilon: 0.35812162000995684    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1795   score: 7.0   memory length: 424568   epsilon: 0.357353380009952    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1796   score: 15.0   memory length: 425173   epsilon: 0.3561554800099444    steps: 605    lr: 6.400000000000001e-06     evaluation reward: 7.47\n",
      "episode: 1797   score: 7.0   memory length: 425580   epsilon: 0.3553496200099393    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1798   score: 6.0   memory length: 425937   epsilon: 0.35464276000993483    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1799   score: 7.0   memory length: 426316   epsilon: 0.3538923400099301    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 7.44\n",
      "episode: 1800   score: 8.0   memory length: 426762   epsilon: 0.3530092600099245    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 7.49\n",
      "episode: 1801   score: 7.0   memory length: 427153   epsilon: 0.3522350800099196    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1802   score: 8.0   memory length: 427627   epsilon: 0.35129656000991366    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 7.5\n",
      "episode: 1803   score: 8.0   memory length: 428063   epsilon: 0.3504332800099082    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 7.51\n",
      "episode: 1804   score: 6.0   memory length: 428383   epsilon: 0.3497996800099042    steps: 320    lr: 6.400000000000001e-06     evaluation reward: 7.49\n",
      "episode: 1805   score: 5.0   memory length: 428726   epsilon: 0.3491205400098999    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 7.49\n",
      "episode: 1806   score: 6.0   memory length: 429068   epsilon: 0.3484433800098956    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 7.49\n",
      "episode: 1807   score: 8.0   memory length: 429547   epsilon: 0.3474949600098896    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 7.51\n",
      "episode: 1808   score: 6.0   memory length: 429887   epsilon: 0.34682176000988535    steps: 340    lr: 6.400000000000001e-06     evaluation reward: 7.5\n",
      "episode: 1809   score: 6.0   memory length: 430228   epsilon: 0.3461465800098811    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 7.45\n",
      "episode: 1810   score: 7.0   memory length: 430634   epsilon: 0.345342700009876    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 7.47\n",
      "episode: 1811   score: 9.0   memory length: 431113   epsilon: 0.34439428000987    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 7.53\n",
      "episode: 1812   score: 6.0   memory length: 431468   epsilon: 0.34369138000986554    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 7.53\n",
      "episode: 1813   score: 3.0   memory length: 431696   epsilon: 0.3432399400098627    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 7.47\n",
      "episode: 1814   score: 4.0   memory length: 431936   epsilon: 0.3427647400098597    steps: 240    lr: 6.400000000000001e-06     evaluation reward: 7.43\n",
      "episode: 1815   score: 14.0   memory length: 432600   epsilon: 0.34145002000985136    steps: 664    lr: 6.400000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1816   score: 7.0   memory length: 432979   epsilon: 0.3406996000098466    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 7.5\n",
      "episode: 1817   score: 8.0   memory length: 433407   epsilon: 0.33985216000984125    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 7.54\n",
      "episode: 1818   score: 10.0   memory length: 433902   epsilon: 0.33887206000983505    steps: 495    lr: 6.400000000000001e-06     evaluation reward: 7.58\n",
      "episode: 1819   score: 12.0   memory length: 434498   epsilon: 0.3376919800098276    steps: 596    lr: 6.400000000000001e-06     evaluation reward: 7.65\n",
      "episode: 1820   score: 9.0   memory length: 434952   epsilon: 0.3367930600098219    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 7.66\n",
      "episode: 1821   score: 9.0   memory length: 435383   epsilon: 0.3359396800098165    steps: 431    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1822   score: 10.0   memory length: 435889   epsilon: 0.33493780000981016    steps: 506    lr: 6.400000000000001e-06     evaluation reward: 7.74\n",
      "episode: 1823   score: 2.0   memory length: 436091   epsilon: 0.33453784000980763    steps: 202    lr: 6.400000000000001e-06     evaluation reward: 7.74\n",
      "episode: 1824   score: 21.0   memory length: 436803   epsilon: 0.3331280800097987    steps: 712    lr: 6.400000000000001e-06     evaluation reward: 7.88\n",
      "episode: 1825   score: 9.0   memory length: 437243   epsilon: 0.3322568800097932    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1826   score: 5.0   memory length: 437552   epsilon: 0.3316450600097893    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1827   score: 12.0   memory length: 437965   epsilon: 0.33082732000978415    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 7.99\n",
      "episode: 1828   score: 10.0   memory length: 438442   epsilon: 0.3298828600097782    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
      "episode: 1829   score: 8.0   memory length: 438898   epsilon: 0.32897998000977247    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 8.12\n",
      "episode: 1830   score: 14.0   memory length: 439545   epsilon: 0.32769892000976436    steps: 647    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1831   score: 8.0   memory length: 439986   epsilon: 0.32682574000975884    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 8.18\n",
      "episode: 1832   score: 5.0   memory length: 440302   epsilon: 0.3262000600097549    steps: 316    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1833   score: 9.0   memory length: 440738   epsilon: 0.3253367800097494    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 8.27\n",
      "episode: 1834   score: 5.0   memory length: 441025   epsilon: 0.3247685200097458    steps: 287    lr: 6.400000000000001e-06     evaluation reward: 8.28\n",
      "episode: 1835   score: 7.0   memory length: 441417   epsilon: 0.3239923600097409    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 8.29\n",
      "episode: 1836   score: 8.0   memory length: 441852   epsilon: 0.32313106000973546    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 8.3\n",
      "episode: 1837   score: 9.0   memory length: 442349   epsilon: 0.32214700000972923    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 8.35\n",
      "episode: 1838   score: 3.0   memory length: 442561   epsilon: 0.3217272400097266    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1839   score: 11.0   memory length: 443086   epsilon: 0.32068774000972    steps: 525    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1840   score: 6.0   memory length: 443411   epsilon: 0.32004424000971593    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
      "episode: 1841   score: 8.0   memory length: 443863   epsilon: 0.31914928000971027    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 8.27\n",
      "episode: 1842   score: 7.0   memory length: 444272   epsilon: 0.31833946000970514    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 8.22\n",
      "episode: 1843   score: 10.0   memory length: 444803   epsilon: 0.3172880800096985    steps: 531    lr: 6.400000000000001e-06     evaluation reward: 8.29\n",
      "episode: 1844   score: 10.0   memory length: 445260   epsilon: 0.31638322000969277    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1845   score: 12.0   memory length: 445845   epsilon: 0.31522492000968544    steps: 585    lr: 6.400000000000001e-06     evaluation reward: 8.38\n",
      "episode: 1846   score: 9.0   memory length: 446309   epsilon: 0.3143062000096796    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1847   score: 14.0   memory length: 446852   epsilon: 0.3132310600096728    steps: 543    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1848   score: 7.0   memory length: 447280   epsilon: 0.31238362000966746    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.5\n",
      "episode: 1849   score: 9.0   memory length: 447743   epsilon: 0.31146688000966166    steps: 463    lr: 6.400000000000001e-06     evaluation reward: 8.48\n",
      "episode: 1850   score: 6.0   memory length: 448067   epsilon: 0.3108253600096576    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1851   score: 6.0   memory length: 448415   epsilon: 0.31013632000965324    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 8.47\n",
      "episode: 1852   score: 8.0   memory length: 448834   epsilon: 0.309306700009648    steps: 419    lr: 6.400000000000001e-06     evaluation reward: 8.47\n",
      "episode: 1853   score: 5.0   memory length: 449124   epsilon: 0.30873250000964436    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1854   score: 9.0   memory length: 449606   epsilon: 0.3077781400096383    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 8.47\n",
      "episode: 1855   score: 10.0   memory length: 450088   epsilon: 0.3068237800096323    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 8.44\n",
      "episode: 1856   score: 9.0   memory length: 450548   epsilon: 0.3059129800096265    steps: 460    lr: 6.400000000000001e-06     evaluation reward: 8.44\n",
      "episode: 1857   score: 6.0   memory length: 450886   epsilon: 0.3052437400096223    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 8.38\n",
      "episode: 1858   score: 5.0   memory length: 451214   epsilon: 0.3045943000096182    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 8.27\n",
      "episode: 1859   score: 5.0   memory length: 451507   epsilon: 0.3040141600096145    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 8.28\n",
      "episode: 1860   score: 10.0   memory length: 451991   epsilon: 0.30305584000960845    steps: 484    lr: 6.400000000000001e-06     evaluation reward: 8.31\n",
      "episode: 1861   score: 12.0   memory length: 452572   epsilon: 0.30190546000960117    steps: 581    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1862   score: 8.0   memory length: 452977   epsilon: 0.3011035600095961    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 8.37\n",
      "episode: 1863   score: 11.0   memory length: 453516   epsilon: 0.30003634000958934    steps: 539    lr: 6.400000000000001e-06     evaluation reward: 8.4\n",
      "episode: 1864   score: 9.0   memory length: 453980   epsilon: 0.29911762000958353    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1865   score: 8.0   memory length: 454417   epsilon: 0.29825236000957805    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 8.44\n",
      "episode: 1866   score: 8.0   memory length: 454816   epsilon: 0.29746234000957306    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 8.46\n",
      "episode: 1867   score: 6.0   memory length: 455193   epsilon: 0.29671588000956833    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 8.42\n",
      "episode: 1868   score: 5.0   memory length: 455501   epsilon: 0.2961060400095645    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 8.41\n",
      "episode: 1869   score: 9.0   memory length: 455993   epsilon: 0.2951318800095583    steps: 492    lr: 6.400000000000001e-06     evaluation reward: 8.45\n",
      "episode: 1870   score: 9.0   memory length: 456391   epsilon: 0.2943438400095533    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 8.47\n",
      "episode: 1871   score: 10.0   memory length: 456911   epsilon: 0.2933142400095468    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 8.52\n",
      "episode: 1872   score: 8.0   memory length: 457345   epsilon: 0.2924549200095414    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1873   score: 6.0   memory length: 457707   epsilon: 0.29173816000953684    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1874   score: 8.0   memory length: 458141   epsilon: 0.2908788400095314    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 8.41\n",
      "episode: 1875   score: 12.0   memory length: 458712   epsilon: 0.28974826000952425    steps: 571    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1876   score: 7.0   memory length: 459104   epsilon: 0.28897210000951934    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 8.37\n",
      "episode: 1877   score: 12.0   memory length: 459536   epsilon: 0.2881167400095139    steps: 432    lr: 6.400000000000001e-06     evaluation reward: 8.45\n",
      "episode: 1878   score: 6.0   memory length: 459870   epsilon: 0.28745542000950974    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 8.41\n",
      "episode: 1879   score: 10.0   memory length: 460263   epsilon: 0.2866772800095048    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 8.42\n",
      "episode: 1880   score: 9.0   memory length: 460768   epsilon: 0.2856773800094985    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 8.37\n",
      "episode: 1881   score: 6.0   memory length: 461124   epsilon: 0.28497250000949403    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1882   score: 11.0   memory length: 461645   epsilon: 0.2839409200094875    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1883   score: 8.0   memory length: 462039   epsilon: 0.28316080000948257    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1884   score: 7.0   memory length: 462468   epsilon: 0.2823113800094772    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 8.22\n",
      "episode: 1885   score: 6.0   memory length: 462827   epsilon: 0.2816005600094727    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1886   score: 13.0   memory length: 463395   epsilon: 0.2804759200094656    steps: 568    lr: 6.400000000000001e-06     evaluation reward: 8.21\n",
      "episode: 1887   score: 6.0   memory length: 463717   epsilon: 0.27983836000946155    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1888   score: 9.0   memory length: 464180   epsilon: 0.27892162000945575    steps: 463    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1889   score: 13.0   memory length: 464681   epsilon: 0.2779296400094495    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 8.25\n",
      "episode: 1890   score: 7.0   memory length: 465052   epsilon: 0.2771950600094448    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 8.21\n",
      "episode: 1891   score: 10.0   memory length: 465567   epsilon: 0.2761753600094384    steps: 515    lr: 6.400000000000001e-06     evaluation reward: 8.21\n",
      "episode: 1892   score: 8.0   memory length: 466020   epsilon: 0.2752784200094327    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
      "episode: 1893   score: 6.0   memory length: 466377   epsilon: 0.27457156000942823    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
      "episode: 1894   score: 10.0   memory length: 466864   epsilon: 0.2736073000094221    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 8.28\n",
      "episode: 1895   score: 11.0   memory length: 467424   epsilon: 0.2724985000094151    steps: 560    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1896   score: 7.0   memory length: 467781   epsilon: 0.27179164000941064    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 8.24\n",
      "episode: 1897   score: 9.0   memory length: 468251   epsilon: 0.27086104000940475    steps: 470    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1898   score: 12.0   memory length: 468836   epsilon: 0.2697027400093974    steps: 585    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1899   score: 7.0   memory length: 469213   epsilon: 0.2689562800093927    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1900   score: 12.0   memory length: 469785   epsilon: 0.26782372000938554    steps: 572    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1901   score: 4.0   memory length: 470080   epsilon: 0.26723962000938184    steps: 295    lr: 6.400000000000001e-06     evaluation reward: 8.33\n",
      "episode: 1902   score: 7.0   memory length: 470477   epsilon: 0.26645356000937687    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1903   score: 15.0   memory length: 471142   epsilon: 0.26513686000936854    steps: 665    lr: 6.400000000000001e-06     evaluation reward: 8.39\n",
      "episode: 1904   score: 15.0   memory length: 471673   epsilon: 0.2640854800093619    steps: 531    lr: 6.400000000000001e-06     evaluation reward: 8.48\n",
      "episode: 1905   score: 7.0   memory length: 472081   epsilon: 0.2632776400093568    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 8.5\n",
      "episode: 1906   score: 13.0   memory length: 472716   epsilon: 0.2620203400093488    steps: 635    lr: 6.400000000000001e-06     evaluation reward: 8.57\n",
      "episode: 1907   score: 10.0   memory length: 473086   epsilon: 0.2612877400093442    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 8.59\n",
      "episode: 1908   score: 16.0   memory length: 473698   epsilon: 0.2600759800093365    steps: 612    lr: 6.400000000000001e-06     evaluation reward: 8.69\n",
      "episode: 1909   score: 9.0   memory length: 474211   epsilon: 0.2590602400093301    steps: 513    lr: 6.400000000000001e-06     evaluation reward: 8.72\n",
      "episode: 1910   score: 12.0   memory length: 474768   epsilon: 0.2579573800093231    steps: 557    lr: 6.400000000000001e-06     evaluation reward: 8.77\n",
      "episode: 1911   score: 8.0   memory length: 475221   epsilon: 0.25706044000931744    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 8.76\n",
      "episode: 1912   score: 12.0   memory length: 475838   epsilon: 0.2558387800093097    steps: 617    lr: 6.400000000000001e-06     evaluation reward: 8.82\n",
      "episode: 1913   score: 6.0   memory length: 476214   epsilon: 0.255094300009305    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 8.85\n",
      "episode: 1914   score: 10.0   memory length: 476608   epsilon: 0.25431418000930006    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 8.91\n",
      "episode: 1915   score: 11.0   memory length: 477032   epsilon: 0.25347466000929475    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 8.88\n",
      "episode: 1916   score: 14.0   memory length: 477657   epsilon: 0.2522371600092869    steps: 625    lr: 6.400000000000001e-06     evaluation reward: 8.95\n",
      "episode: 1917   score: 18.0   memory length: 478341   epsilon: 0.25088284000927835    steps: 684    lr: 6.400000000000001e-06     evaluation reward: 9.05\n",
      "episode: 1918   score: 8.0   memory length: 478762   epsilon: 0.2500492600092731    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 9.03\n",
      "episode: 1919   score: 9.0   memory length: 479256   epsilon: 0.2490711400092669    steps: 494    lr: 6.400000000000001e-06     evaluation reward: 9.0\n",
      "episode: 1920   score: 8.0   memory length: 479716   epsilon: 0.24816034000926113    steps: 460    lr: 6.400000000000001e-06     evaluation reward: 8.99\n",
      "episode: 1921   score: 8.0   memory length: 480140   epsilon: 0.24732082000925582    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 8.98\n",
      "episode: 1922   score: 15.0   memory length: 480690   epsilon: 0.24623182000924893    steps: 550    lr: 6.400000000000001e-06     evaluation reward: 9.03\n",
      "episode: 1923   score: 9.0   memory length: 481137   epsilon: 0.24534676000924333    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 9.1\n",
      "episode: 1924   score: 7.0   memory length: 481565   epsilon: 0.24449932000923796    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.96\n",
      "episode: 1925   score: 10.0   memory length: 482094   epsilon: 0.24345190000923134    steps: 529    lr: 6.400000000000001e-06     evaluation reward: 8.97\n",
      "episode: 1926   score: 5.0   memory length: 482400   epsilon: 0.2428460200092275    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 8.97\n",
      "episode: 1927   score: 9.0   memory length: 482864   epsilon: 0.2419273000092217    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 8.94\n",
      "episode: 1928   score: 9.0   memory length: 483336   epsilon: 0.24099274000921578    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 8.93\n",
      "episode: 1929   score: 10.0   memory length: 483822   epsilon: 0.2400304600092097    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 8.95\n",
      "episode: 1930   score: 9.0   memory length: 484290   epsilon: 0.23910382000920383    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 8.9\n",
      "episode: 1931   score: 10.0   memory length: 484776   epsilon: 0.23814154000919774    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 8.92\n",
      "episode: 1932   score: 11.0   memory length: 485331   epsilon: 0.23704264000919079    steps: 555    lr: 6.400000000000001e-06     evaluation reward: 8.98\n",
      "episode: 1933   score: 6.0   memory length: 485667   epsilon: 0.23637736000918658    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 8.95\n",
      "episode: 1934   score: 5.0   memory length: 485998   epsilon: 0.23572198000918243    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 8.95\n",
      "episode: 1935   score: 8.0   memory length: 486435   epsilon: 0.23485672000917696    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 8.96\n",
      "episode: 1936   score: 8.0   memory length: 486860   epsilon: 0.23401522000917163    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 8.96\n",
      "episode: 1937   score: 8.0   memory length: 487265   epsilon: 0.23321332000916656    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 8.95\n",
      "episode: 1938   score: 9.0   memory length: 487718   epsilon: 0.23231638000916088    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 9.01\n",
      "episode: 1939   score: 4.0   memory length: 487978   epsilon: 0.23180158000915763    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 8.94\n",
      "episode: 1940   score: 6.0   memory length: 488316   epsilon: 0.2311323400091534    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 8.94\n",
      "episode: 1941   score: 12.0   memory length: 488952   epsilon: 0.22987306000914542    steps: 636    lr: 6.400000000000001e-06     evaluation reward: 8.98\n",
      "episode: 1942   score: 14.0   memory length: 489508   epsilon: 0.22877218000913846    steps: 556    lr: 6.400000000000001e-06     evaluation reward: 9.05\n",
      "episode: 1943   score: 10.0   memory length: 490041   epsilon: 0.22771684000913178    steps: 533    lr: 6.400000000000001e-06     evaluation reward: 9.05\n",
      "episode: 1944   score: 8.0   memory length: 490442   epsilon: 0.22692286000912676    steps: 401    lr: 6.400000000000001e-06     evaluation reward: 9.03\n",
      "episode: 1945   score: 11.0   memory length: 490998   epsilon: 0.2258219800091198    steps: 556    lr: 6.400000000000001e-06     evaluation reward: 9.02\n",
      "episode: 1946   score: 13.0   memory length: 491589   epsilon: 0.2246518000091124    steps: 591    lr: 6.400000000000001e-06     evaluation reward: 9.06\n",
      "episode: 1947   score: 6.0   memory length: 491892   epsilon: 0.2240518600091086    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 8.98\n",
      "episode: 1948   score: 10.0   memory length: 492393   epsilon: 0.22305988000910232    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 9.01\n",
      "episode: 1949   score: 14.0   memory length: 493008   epsilon: 0.22184218000909461    steps: 615    lr: 6.400000000000001e-06     evaluation reward: 9.06\n",
      "episode: 1950   score: 12.0   memory length: 493472   epsilon: 0.2209234600090888    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 9.12\n",
      "episode: 1951   score: 8.0   memory length: 493884   epsilon: 0.22010770000908364    steps: 412    lr: 6.400000000000001e-06     evaluation reward: 9.14\n",
      "episode: 1952   score: 10.0   memory length: 494383   epsilon: 0.2191196800090774    steps: 499    lr: 6.400000000000001e-06     evaluation reward: 9.16\n",
      "episode: 1953   score: 10.0   memory length: 494898   epsilon: 0.21809998000907094    steps: 515    lr: 6.400000000000001e-06     evaluation reward: 9.21\n",
      "episode: 1954   score: 13.0   memory length: 495510   epsilon: 0.21688822000906327    steps: 612    lr: 6.400000000000001e-06     evaluation reward: 9.25\n",
      "episode: 1955   score: 11.0   memory length: 496109   epsilon: 0.21570220000905577    steps: 599    lr: 6.400000000000001e-06     evaluation reward: 9.26\n",
      "episode: 1956   score: 5.0   memory length: 496396   epsilon: 0.21513394000905217    steps: 287    lr: 6.400000000000001e-06     evaluation reward: 9.22\n",
      "episode: 1957   score: 19.0   memory length: 496998   epsilon: 0.21394198000904463    steps: 602    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
      "episode: 1958   score: 7.0   memory length: 497426   epsilon: 0.21309454000903927    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
      "episode: 1959   score: 6.0   memory length: 497762   epsilon: 0.21242926000903506    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
      "episode: 1960   score: 9.0   memory length: 498219   epsilon: 0.21152440000902933    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
      "episode: 1961   score: 4.0   memory length: 498478   epsilon: 0.2110115800090261    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 9.29\n",
      "episode: 1962   score: 21.0   memory length: 499127   epsilon: 0.20972656000901796    steps: 649    lr: 6.400000000000001e-06     evaluation reward: 9.42\n",
      "episode: 1963   score: 10.0   memory length: 499624   epsilon: 0.20874250000901173    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 9.41\n",
      "episode: 1964   score: 8.0   memory length: 500064   epsilon: 0.20787130000900622    steps: 440    lr: 2.560000000000001e-06     evaluation reward: 9.4\n",
      "episode: 1965   score: 13.0   memory length: 500708   epsilon: 0.20659618000899815    steps: 644    lr: 2.560000000000001e-06     evaluation reward: 9.45\n",
      "episode: 1966   score: 6.0   memory length: 501018   epsilon: 0.20598238000899427    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 9.43\n",
      "episode: 1967   score: 7.0   memory length: 501407   epsilon: 0.2052121600089894    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 9.44\n",
      "episode: 1968   score: 10.0   memory length: 501928   epsilon: 0.20418058000898287    steps: 521    lr: 2.560000000000001e-06     evaluation reward: 9.49\n",
      "episode: 1969   score: 8.0   memory length: 502321   epsilon: 0.20340244000897795    steps: 393    lr: 2.560000000000001e-06     evaluation reward: 9.48\n",
      "episode: 1970   score: 7.0   memory length: 502657   epsilon: 0.20273716000897374    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 9.46\n",
      "episode: 1971   score: 8.0   memory length: 503013   epsilon: 0.20203228000896928    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 9.44\n",
      "episode: 1972   score: 7.0   memory length: 503413   epsilon: 0.20124028000896427    steps: 400    lr: 2.560000000000001e-06     evaluation reward: 9.43\n",
      "episode: 1973   score: 17.0   memory length: 504011   epsilon: 0.20005624000895678    steps: 598    lr: 2.560000000000001e-06     evaluation reward: 9.54\n",
      "episode: 1974   score: 14.0   memory length: 504561   epsilon: 0.19896724000894989    steps: 550    lr: 2.560000000000001e-06     evaluation reward: 9.6\n",
      "episode: 1975   score: 8.0   memory length: 505038   epsilon: 0.1980227800089439    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 9.56\n",
      "episode: 1976   score: 9.0   memory length: 505514   epsilon: 0.19708030000893795    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 9.58\n",
      "episode: 1977   score: 13.0   memory length: 506129   epsilon: 0.19586260000893024    steps: 615    lr: 2.560000000000001e-06     evaluation reward: 9.59\n",
      "episode: 1978   score: 7.0   memory length: 506487   epsilon: 0.19515376000892576    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 9.6\n",
      "episode: 1979   score: 8.0   memory length: 506912   epsilon: 0.19431226000892043    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 9.58\n",
      "episode: 1980   score: 12.0   memory length: 507483   epsilon: 0.19318168000891328    steps: 571    lr: 2.560000000000001e-06     evaluation reward: 9.61\n",
      "episode: 1981   score: 9.0   memory length: 507952   epsilon: 0.1922530600089074    steps: 469    lr: 2.560000000000001e-06     evaluation reward: 9.64\n",
      "episode: 1982   score: 7.0   memory length: 508332   epsilon: 0.19150066000890265    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 9.6\n",
      "episode: 1983   score: 6.0   memory length: 508673   epsilon: 0.19082548000889837    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 9.58\n",
      "episode: 1984   score: 12.0   memory length: 509128   epsilon: 0.18992458000889267    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 9.63\n",
      "episode: 1985   score: 9.0   memory length: 509550   epsilon: 0.1890890200088874    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 9.66\n",
      "episode: 1986   score: 10.0   memory length: 510075   epsilon: 0.1880495200088808    steps: 525    lr: 2.560000000000001e-06     evaluation reward: 9.63\n",
      "episode: 1987   score: 10.0   memory length: 510578   epsilon: 0.1870535800088745    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 9.67\n",
      "episode: 1988   score: 9.0   memory length: 511013   epsilon: 0.18619228000886906    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 9.67\n",
      "episode: 1989   score: 9.0   memory length: 511494   epsilon: 0.18523990000886303    steps: 481    lr: 2.560000000000001e-06     evaluation reward: 9.63\n",
      "episode: 1990   score: 9.0   memory length: 511952   epsilon: 0.1843330600088573    steps: 458    lr: 2.560000000000001e-06     evaluation reward: 9.65\n",
      "episode: 1991   score: 7.0   memory length: 512348   epsilon: 0.18354898000885234    steps: 396    lr: 2.560000000000001e-06     evaluation reward: 9.62\n",
      "episode: 1992   score: 11.0   memory length: 512882   epsilon: 0.18249166000884565    steps: 534    lr: 2.560000000000001e-06     evaluation reward: 9.65\n",
      "episode: 1993   score: 13.0   memory length: 513515   epsilon: 0.18123832000883772    steps: 633    lr: 2.560000000000001e-06     evaluation reward: 9.72\n",
      "episode: 1994   score: 10.0   memory length: 513996   epsilon: 0.1802859400088317    steps: 481    lr: 2.560000000000001e-06     evaluation reward: 9.72\n",
      "episode: 1995   score: 8.0   memory length: 514439   epsilon: 0.17940880000882614    steps: 443    lr: 2.560000000000001e-06     evaluation reward: 9.69\n",
      "episode: 1996   score: 6.0   memory length: 514793   epsilon: 0.1787078800088217    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 9.68\n",
      "episode: 1997   score: 8.0   memory length: 515239   epsilon: 0.17782480000881612    steps: 446    lr: 2.560000000000001e-06     evaluation reward: 9.67\n",
      "episode: 1998   score: 15.0   memory length: 515920   epsilon: 0.1764764200088076    steps: 681    lr: 2.560000000000001e-06     evaluation reward: 9.7\n",
      "episode: 1999   score: 13.0   memory length: 516510   epsilon: 0.1753082200088002    steps: 590    lr: 2.560000000000001e-06     evaluation reward: 9.76\n",
      "episode: 2000   score: 11.0   memory length: 517002   epsilon: 0.17433406000879403    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
      "episode: 2001   score: 11.0   memory length: 517431   epsilon: 0.17348464000878866    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 9.82\n",
      "episode: 2002   score: 7.0   memory length: 517835   epsilon: 0.1726847200087836    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 9.82\n",
      "episode: 2003   score: 5.0   memory length: 518176   epsilon: 0.17200954000877933    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 9.72\n",
      "episode: 2004   score: 23.0   memory length: 518802   epsilon: 0.17077006000877148    steps: 626    lr: 2.560000000000001e-06     evaluation reward: 9.8\n",
      "episode: 2005   score: 12.0   memory length: 519418   epsilon: 0.16955038000876377    steps: 616    lr: 2.560000000000001e-06     evaluation reward: 9.85\n",
      "episode: 2006   score: 8.0   memory length: 519895   epsilon: 0.1686059200087578    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 9.8\n",
      "episode: 2007   score: 10.0   memory length: 520403   epsilon: 0.16760008000875143    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 9.8\n",
      "episode: 2008   score: 8.0   memory length: 520815   epsilon: 0.16678432000874627    steps: 412    lr: 2.560000000000001e-06     evaluation reward: 9.72\n",
      "episode: 2009   score: 15.0   memory length: 521387   epsilon: 0.1656517600087391    steps: 572    lr: 2.560000000000001e-06     evaluation reward: 9.78\n",
      "episode: 2010   score: 5.0   memory length: 521729   epsilon: 0.16497460000873482    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 9.71\n",
      "episode: 2011   score: 11.0   memory length: 522259   epsilon: 0.16392520000872818    steps: 530    lr: 2.560000000000001e-06     evaluation reward: 9.74\n",
      "episode: 2012   score: 11.0   memory length: 522787   epsilon: 0.16287976000872156    steps: 528    lr: 2.560000000000001e-06     evaluation reward: 9.73\n",
      "episode: 2013   score: 8.0   memory length: 523265   epsilon: 0.16193332000871558    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
      "episode: 2014   score: 15.0   memory length: 523705   epsilon: 0.16106212000871006    steps: 440    lr: 2.560000000000001e-06     evaluation reward: 9.8\n",
      "episode: 2015   score: 9.0   memory length: 524189   epsilon: 0.160103800008704    steps: 484    lr: 2.560000000000001e-06     evaluation reward: 9.78\n",
      "episode: 2016   score: 7.0   memory length: 524566   epsilon: 0.15935734000869928    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 9.71\n",
      "episode: 2017   score: 10.0   memory length: 525039   epsilon: 0.15842080000869335    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 9.63\n",
      "episode: 2018   score: 7.0   memory length: 525427   epsilon: 0.1576525600086885    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 9.62\n",
      "episode: 2019   score: 10.0   memory length: 525945   epsilon: 0.156626920008682    steps: 518    lr: 2.560000000000001e-06     evaluation reward: 9.63\n",
      "episode: 2020   score: 6.0   memory length: 526286   epsilon: 0.15595174000867773    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 9.61\n",
      "episode: 2021   score: 13.0   memory length: 526725   epsilon: 0.15508252000867223    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 9.66\n",
      "episode: 2022   score: 10.0   memory length: 527239   epsilon: 0.1540648000086658    steps: 514    lr: 2.560000000000001e-06     evaluation reward: 9.61\n",
      "episode: 2023   score: 7.0   memory length: 527645   epsilon: 0.1532609200086607    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 9.59\n",
      "episode: 2024   score: 7.0   memory length: 528027   epsilon: 0.15250456000865592    steps: 382    lr: 2.560000000000001e-06     evaluation reward: 9.59\n",
      "episode: 2025   score: 16.0   memory length: 528751   epsilon: 0.15107104000864685    steps: 724    lr: 2.560000000000001e-06     evaluation reward: 9.65\n",
      "episode: 2026   score: 7.0   memory length: 529110   epsilon: 0.15036022000864235    steps: 359    lr: 2.560000000000001e-06     evaluation reward: 9.67\n",
      "episode: 2027   score: 12.0   memory length: 529648   epsilon: 0.1492949800086356    steps: 538    lr: 2.560000000000001e-06     evaluation reward: 9.7\n",
      "episode: 2028   score: 11.0   memory length: 530242   epsilon: 0.14811886000862817    steps: 594    lr: 2.560000000000001e-06     evaluation reward: 9.72\n",
      "episode: 2029   score: 12.0   memory length: 530800   epsilon: 0.14701402000862118    steps: 558    lr: 2.560000000000001e-06     evaluation reward: 9.74\n",
      "episode: 2030   score: 10.0   memory length: 531286   epsilon: 0.1460517400086151    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
      "episode: 2031   score: 11.0   memory length: 531790   epsilon: 0.14505382000860878    steps: 504    lr: 2.560000000000001e-06     evaluation reward: 9.76\n",
      "episode: 2032   score: 11.0   memory length: 532366   epsilon: 0.14391334000860156    steps: 576    lr: 2.560000000000001e-06     evaluation reward: 9.76\n",
      "episode: 2033   score: 7.0   memory length: 532724   epsilon: 0.14320450000859708    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 9.77\n",
      "episode: 2034   score: 11.0   memory length: 533225   epsilon: 0.1422125200085908    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 9.83\n",
      "episode: 2035   score: 11.0   memory length: 533741   epsilon: 0.14119084000858434    steps: 516    lr: 2.560000000000001e-06     evaluation reward: 9.86\n",
      "episode: 2036   score: 14.0   memory length: 534396   epsilon: 0.13989394000857613    steps: 655    lr: 2.560000000000001e-06     evaluation reward: 9.92\n",
      "episode: 2037   score: 9.0   memory length: 534830   epsilon: 0.1390346200085707    steps: 434    lr: 2.560000000000001e-06     evaluation reward: 9.93\n",
      "episode: 2038   score: 10.0   memory length: 535349   epsilon: 0.1380070000085642    steps: 519    lr: 2.560000000000001e-06     evaluation reward: 9.94\n",
      "episode: 2039   score: 9.0   memory length: 535844   epsilon: 0.137026900008558    steps: 495    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
      "episode: 2040   score: 12.0   memory length: 536396   epsilon: 0.13593394000855108    steps: 552    lr: 2.560000000000001e-06     evaluation reward: 10.05\n",
      "episode: 2041   score: 19.0   memory length: 537070   epsilon: 0.13459942000854264    steps: 674    lr: 2.560000000000001e-06     evaluation reward: 10.12\n",
      "episode: 2042   score: 9.0   memory length: 537477   epsilon: 0.13379356000853754    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 10.07\n",
      "episode: 2043   score: 14.0   memory length: 538122   epsilon: 0.13251646000852946    steps: 645    lr: 2.560000000000001e-06     evaluation reward: 10.11\n",
      "episode: 2044   score: 10.0   memory length: 538663   epsilon: 0.13144528000852268    steps: 541    lr: 2.560000000000001e-06     evaluation reward: 10.13\n",
      "episode: 2045   score: 11.0   memory length: 539213   epsilon: 0.1303562800085158    steps: 550    lr: 2.560000000000001e-06     evaluation reward: 10.13\n",
      "episode: 2046   score: 5.0   memory length: 539493   epsilon: 0.12980188000851228    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 10.05\n",
      "episode: 2047   score: 10.0   memory length: 539971   epsilon: 0.1288554400085063    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 10.09\n",
      "episode: 2048   score: 5.0   memory length: 540296   epsilon: 0.12821194000850222    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 10.04\n",
      "episode: 2049   score: 11.0   memory length: 540815   epsilon: 0.12718432000849572    steps: 519    lr: 2.560000000000001e-06     evaluation reward: 10.01\n",
      "episode: 2050   score: 10.0   memory length: 541316   epsilon: 0.12619234000848945    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
      "episode: 2051   score: 11.0   memory length: 541841   epsilon: 0.12515284000848287    steps: 525    lr: 2.560000000000001e-06     evaluation reward: 10.02\n",
      "episode: 2052   score: 10.0   memory length: 542351   epsilon: 0.12414304000848249    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 10.02\n",
      "episode: 2053   score: 7.0   memory length: 542724   epsilon: 0.12340450000848299    steps: 373    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
      "episode: 2054   score: 10.0   memory length: 543229   epsilon: 0.12240460000848367    steps: 505    lr: 2.560000000000001e-06     evaluation reward: 9.96\n",
      "episode: 2055   score: 9.0   memory length: 543644   epsilon: 0.12158290000848423    steps: 415    lr: 2.560000000000001e-06     evaluation reward: 9.94\n",
      "episode: 2056   score: 12.0   memory length: 544171   epsilon: 0.12053944000848495    steps: 527    lr: 2.560000000000001e-06     evaluation reward: 10.01\n",
      "episode: 2057   score: 7.0   memory length: 544541   epsilon: 0.11980684000848545    steps: 370    lr: 2.560000000000001e-06     evaluation reward: 9.89\n",
      "episode: 2058   score: 11.0   memory length: 545050   epsilon: 0.11879902000848613    steps: 509    lr: 2.560000000000001e-06     evaluation reward: 9.93\n",
      "episode: 2059   score: 7.0   memory length: 545416   epsilon: 0.11807434000848663    steps: 366    lr: 2.560000000000001e-06     evaluation reward: 9.94\n",
      "episode: 2060   score: 7.0   memory length: 545763   epsilon: 0.1173872800084871    steps: 347    lr: 2.560000000000001e-06     evaluation reward: 9.92\n",
      "episode: 2061   score: 25.0   memory length: 546443   epsilon: 0.11604088000848801    steps: 680    lr: 2.560000000000001e-06     evaluation reward: 10.13\n",
      "episode: 2062   score: 7.0   memory length: 546854   epsilon: 0.11522710000848857    steps: 411    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
      "episode: 2063   score: 10.0   memory length: 547326   epsilon: 0.11429254000848921    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
      "episode: 2064   score: 15.0   memory length: 547721   epsilon: 0.11351044000848974    steps: 395    lr: 2.560000000000001e-06     evaluation reward: 10.06\n",
      "episode: 2065   score: 11.0   memory length: 548303   epsilon: 0.11235808000849053    steps: 582    lr: 2.560000000000001e-06     evaluation reward: 10.04\n",
      "episode: 2066   score: 15.0   memory length: 548917   epsilon: 0.11114236000849136    steps: 614    lr: 2.560000000000001e-06     evaluation reward: 10.13\n",
      "episode: 2067   score: 12.0   memory length: 549519   epsilon: 0.10995040000849217    steps: 602    lr: 2.560000000000001e-06     evaluation reward: 10.18\n",
      "episode: 2068   score: 7.0   memory length: 549906   epsilon: 0.10918414000849269    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 10.15\n",
      "episode: 2069   score: 11.0   memory length: 550426   epsilon: 0.1081545400084934    steps: 520    lr: 2.560000000000001e-06     evaluation reward: 10.18\n",
      "episode: 2070   score: 17.0   memory length: 551023   epsilon: 0.1069724800084942    steps: 597    lr: 2.560000000000001e-06     evaluation reward: 10.28\n",
      "episode: 2071   score: 11.0   memory length: 551526   epsilon: 0.10597654000849488    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 10.31\n",
      "episode: 2072   score: 15.0   memory length: 552088   epsilon: 0.10486378000849564    steps: 562    lr: 2.560000000000001e-06     evaluation reward: 10.39\n",
      "episode: 2073   score: 12.0   memory length: 552670   epsilon: 0.10371142000849642    steps: 582    lr: 2.560000000000001e-06     evaluation reward: 10.34\n",
      "episode: 2074   score: 7.0   memory length: 553054   epsilon: 0.10295110000849694    steps: 384    lr: 2.560000000000001e-06     evaluation reward: 10.27\n",
      "episode: 2075   score: 13.0   memory length: 553644   epsilon: 0.10178290000849774    steps: 590    lr: 2.560000000000001e-06     evaluation reward: 10.32\n",
      "episode: 2076   score: 18.0   memory length: 554127   epsilon: 0.10082656000849839    steps: 483    lr: 2.560000000000001e-06     evaluation reward: 10.41\n",
      "episode: 2077   score: 18.0   memory length: 554823   epsilon: 0.09944848000849933    steps: 696    lr: 2.560000000000001e-06     evaluation reward: 10.46\n",
      "episode: 2078   score: 13.0   memory length: 555458   epsilon: 0.09819118000850019    steps: 635    lr: 2.560000000000001e-06     evaluation reward: 10.52\n",
      "episode: 2079   score: 9.0   memory length: 555953   epsilon: 0.09721108000850086    steps: 495    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
      "episode: 2080   score: 10.0   memory length: 556453   epsilon: 0.09622108000850153    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 10.51\n",
      "episode: 2081   score: 7.0   memory length: 556859   epsilon: 0.09541720000850208    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 10.49\n",
      "episode: 2082   score: 14.0   memory length: 557360   epsilon: 0.09442522000850276    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 10.56\n",
      "episode: 2083   score: 18.0   memory length: 558000   epsilon: 0.09315802000850362    steps: 640    lr: 2.560000000000001e-06     evaluation reward: 10.68\n",
      "episode: 2084   score: 7.0   memory length: 558361   epsilon: 0.09244324000850411    steps: 361    lr: 2.560000000000001e-06     evaluation reward: 10.63\n",
      "episode: 2085   score: 5.0   memory length: 558634   epsilon: 0.09190270000850448    steps: 273    lr: 2.560000000000001e-06     evaluation reward: 10.59\n",
      "episode: 2086   score: 7.0   memory length: 559004   epsilon: 0.09117010000850498    steps: 370    lr: 2.560000000000001e-06     evaluation reward: 10.56\n",
      "episode: 2087   score: 9.0   memory length: 559482   epsilon: 0.09022366000850562    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 10.55\n",
      "episode: 2088   score: 8.0   memory length: 559874   epsilon: 0.08944750000850615    steps: 392    lr: 2.560000000000001e-06     evaluation reward: 10.54\n",
      "episode: 2089   score: 12.0   memory length: 560499   epsilon: 0.088210000008507    steps: 625    lr: 2.560000000000001e-06     evaluation reward: 10.57\n",
      "episode: 2090   score: 9.0   memory length: 560970   epsilon: 0.08727742000850763    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 10.57\n",
      "episode: 2091   score: 10.0   memory length: 561422   epsilon: 0.08638246000850824    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 10.6\n",
      "episode: 2092   score: 14.0   memory length: 561913   epsilon: 0.08541028000850891    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 10.63\n",
      "episode: 2093   score: 10.0   memory length: 562396   epsilon: 0.08445394000850956    steps: 483    lr: 2.560000000000001e-06     evaluation reward: 10.6\n",
      "episode: 2094   score: 5.0   memory length: 562702   epsilon: 0.08384806000850997    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 10.55\n",
      "episode: 2095   score: 9.0   memory length: 563159   epsilon: 0.08294320000851059    steps: 457    lr: 2.560000000000001e-06     evaluation reward: 10.56\n",
      "episode: 2096   score: 8.0   memory length: 563611   epsilon: 0.0820482400085112    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 10.58\n",
      "episode: 2097   score: 8.0   memory length: 564030   epsilon: 0.08121862000851177    steps: 419    lr: 2.560000000000001e-06     evaluation reward: 10.58\n",
      "episode: 2098   score: 12.0   memory length: 564622   epsilon: 0.08004646000851257    steps: 592    lr: 2.560000000000001e-06     evaluation reward: 10.55\n",
      "episode: 2099   score: 10.0   memory length: 565127   epsilon: 0.07904656000851325    steps: 505    lr: 2.560000000000001e-06     evaluation reward: 10.52\n",
      "episode: 2100   score: 10.0   memory length: 565623   epsilon: 0.07806448000851392    steps: 496    lr: 2.560000000000001e-06     evaluation reward: 10.51\n",
      "episode: 2101   score: 7.0   memory length: 566008   epsilon: 0.07730218000851444    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 10.47\n",
      "episode: 2102   score: 11.0   memory length: 566548   epsilon: 0.07623298000851517    steps: 540    lr: 2.560000000000001e-06     evaluation reward: 10.51\n",
      "episode: 2103   score: 10.0   memory length: 567001   epsilon: 0.07533604000851578    steps: 453    lr: 2.560000000000001e-06     evaluation reward: 10.56\n",
      "episode: 2104   score: 7.0   memory length: 567342   epsilon: 0.07466086000851624    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 10.4\n",
      "episode: 2105   score: 8.0   memory length: 567817   epsilon: 0.07372036000851688    steps: 475    lr: 2.560000000000001e-06     evaluation reward: 10.36\n",
      "episode: 2106   score: 10.0   memory length: 568294   epsilon: 0.07277590000851752    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 10.38\n",
      "episode: 2107   score: 6.0   memory length: 568618   epsilon: 0.07213438000851796    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 10.34\n",
      "episode: 2108   score: 12.0   memory length: 569242   epsilon: 0.0708988600085188    steps: 624    lr: 2.560000000000001e-06     evaluation reward: 10.38\n",
      "episode: 2109   score: 9.0   memory length: 569730   epsilon: 0.06993262000851946    steps: 488    lr: 2.560000000000001e-06     evaluation reward: 10.32\n",
      "episode: 2110   score: 8.0   memory length: 570174   epsilon: 0.06905350000852006    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 10.35\n",
      "episode: 2111   score: 16.0   memory length: 570764   epsilon: 0.06788530000852086    steps: 590    lr: 2.560000000000001e-06     evaluation reward: 10.4\n",
      "episode: 2112   score: 8.0   memory length: 571147   epsilon: 0.06712696000852138    steps: 383    lr: 2.560000000000001e-06     evaluation reward: 10.37\n",
      "episode: 2113   score: 4.0   memory length: 571408   epsilon: 0.06661018000852173    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 10.33\n",
      "episode: 2114   score: 25.0   memory length: 572193   epsilon: 0.06505588000852279    steps: 785    lr: 2.560000000000001e-06     evaluation reward: 10.43\n",
      "episode: 2115   score: 10.0   memory length: 572677   epsilon: 0.06409756000852344    steps: 484    lr: 2.560000000000001e-06     evaluation reward: 10.44\n",
      "episode: 2116   score: 16.0   memory length: 573244   epsilon: 0.06297490000852421    steps: 567    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
      "episode: 2117   score: 8.0   memory length: 573655   epsilon: 0.062161120008524764    steps: 411    lr: 2.560000000000001e-06     evaluation reward: 10.51\n",
      "episode: 2118   score: 8.0   memory length: 574111   epsilon: 0.06125824000852538    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 10.52\n",
      "episode: 2119   score: 8.0   memory length: 574509   epsilon: 0.06047020000852592    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 10.5\n",
      "episode: 2120   score: 14.0   memory length: 575112   epsilon: 0.05927626000852673    steps: 603    lr: 2.560000000000001e-06     evaluation reward: 10.58\n",
      "episode: 2121   score: 10.0   memory length: 575589   epsilon: 0.058331800008527376    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 10.55\n",
      "episode: 2122   score: 9.0   memory length: 576017   epsilon: 0.057484360008527954    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 10.54\n",
      "episode: 2123   score: 6.0   memory length: 576360   epsilon: 0.05680522000852842    steps: 343    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
      "episode: 2124   score: 7.0   memory length: 576765   epsilon: 0.056003320008528965    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
      "episode: 2125   score: 8.0   memory length: 577150   epsilon: 0.055241020008529484    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 10.45\n",
      "episode: 2126   score: 8.0   memory length: 577588   epsilon: 0.054373780008530076    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 10.46\n",
      "episode: 2127   score: 9.0   memory length: 577998   epsilon: 0.05356198000853063    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 10.43\n",
      "episode: 2128   score: 11.0   memory length: 578517   epsilon: 0.05253436000853133    steps: 519    lr: 2.560000000000001e-06     evaluation reward: 10.43\n",
      "episode: 2129   score: 10.0   memory length: 578896   epsilon: 0.05178394000853184    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 10.41\n",
      "episode: 2130   score: 18.0   memory length: 579528   epsilon: 0.050532580008532696    steps: 632    lr: 2.560000000000001e-06     evaluation reward: 10.49\n",
      "episode: 2131   score: 14.0   memory length: 580159   epsilon: 0.04928320000853355    steps: 631    lr: 2.560000000000001e-06     evaluation reward: 10.52\n",
      "episode: 2132   score: 12.0   memory length: 580746   epsilon: 0.04812094000853434    steps: 587    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
      "episode: 2133   score: 12.0   memory length: 581310   epsilon: 0.0470042200085351    steps: 564    lr: 2.560000000000001e-06     evaluation reward: 10.58\n",
      "episode: 2134   score: 26.0   memory length: 581996   epsilon: 0.04564594000853603    steps: 686    lr: 2.560000000000001e-06     evaluation reward: 10.73\n",
      "episode: 2135   score: 11.0   memory length: 582545   epsilon: 0.04455892000853677    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 10.73\n",
      "episode: 2136   score: 9.0   memory length: 583010   epsilon: 0.0436382200085374    steps: 465    lr: 2.560000000000001e-06     evaluation reward: 10.68\n",
      "episode: 2137   score: 9.0   memory length: 583447   epsilon: 0.04277296000853799    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 10.68\n",
      "episode: 2138   score: 7.0   memory length: 583821   epsilon: 0.042032440008538494    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 10.65\n",
      "episode: 2139   score: 11.0   memory length: 584359   epsilon: 0.04096720000853922    steps: 538    lr: 2.560000000000001e-06     evaluation reward: 10.67\n",
      "episode: 2140   score: 12.0   memory length: 584833   epsilon: 0.04002868000853986    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 10.67\n",
      "episode: 2141   score: 8.0   memory length: 585251   epsilon: 0.039201040008540425    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 10.56\n",
      "episode: 2142   score: 7.0   memory length: 585592   epsilon: 0.038525860008540885    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 10.54\n",
      "episode: 2143   score: 19.0   memory length: 586301   epsilon: 0.03712204000854184    steps: 709    lr: 2.560000000000001e-06     evaluation reward: 10.59\n",
      "episode: 2144   score: 10.0   memory length: 586779   epsilon: 0.03617560000854249    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 10.59\n",
      "episode: 2145   score: 15.0   memory length: 587291   epsilon: 0.03516184000854318    steps: 512    lr: 2.560000000000001e-06     evaluation reward: 10.63\n",
      "episode: 2146   score: 16.0   memory length: 588022   epsilon: 0.03371446000854417    steps: 731    lr: 2.560000000000001e-06     evaluation reward: 10.74\n",
      "episode: 2147   score: 5.0   memory length: 588353   epsilon: 0.033059080008544614    steps: 331    lr: 2.560000000000001e-06     evaluation reward: 10.69\n",
      "episode: 2148   score: 18.0   memory length: 589009   epsilon: 0.0317602000085455    steps: 656    lr: 2.560000000000001e-06     evaluation reward: 10.82\n",
      "episode: 2149   score: 14.0   memory length: 589511   epsilon: 0.030766240008546178    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 10.85\n",
      "episode: 2150   score: 10.0   memory length: 589965   epsilon: 0.02986732000854679    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 10.85\n",
      "episode: 2151   score: 9.0   memory length: 590436   epsilon: 0.028934740008547427    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 10.83\n",
      "episode: 2152   score: 10.0   memory length: 590944   epsilon: 0.027928900008548113    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 10.83\n",
      "episode: 2153   score: 8.0   memory length: 591373   epsilon: 0.027079480008548693    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 10.84\n",
      "episode: 2154   score: 5.0   memory length: 591686   epsilon: 0.026459740008549115    steps: 313    lr: 2.560000000000001e-06     evaluation reward: 10.79\n",
      "episode: 2155   score: 8.0   memory length: 592096   epsilon: 0.02564794000854967    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 10.78\n",
      "episode: 2156   score: 18.0   memory length: 592738   epsilon: 0.024376780008550536    steps: 642    lr: 2.560000000000001e-06     evaluation reward: 10.84\n",
      "episode: 2157   score: 9.0   memory length: 593166   epsilon: 0.023529340008551114    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 10.86\n",
      "episode: 2158   score: 11.0   memory length: 593714   epsilon: 0.022444300008551854    steps: 548    lr: 2.560000000000001e-06     evaluation reward: 10.86\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m pylab\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRewards\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     53\u001b[0m pylab\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisodes vs Reward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mpylab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./save_graph/breakout_dqn.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# save graph for training visualization\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# every episode, plot the play time\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  memory length:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  epsilon:\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39mepsilon, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   steps:\u001b[39m\u001b[38;5;124m\"\u001b[39m, step,\n\u001b[1;32m     59\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   lr:\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    evaluation reward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(evaluation_reward))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py:1135\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m res \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[0;32m-> 1135\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py:1893\u001b[0m, in \u001b[0;36mFigureCanvasBase.draw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_idle_drawing:\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idle_draw_cntx():\n\u001b[0;32m-> 1893\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py:388\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    387\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py:3154\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3154\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3158\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py:3070\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3068\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3070\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3073\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/lines.py:801\u001b[0m, in \u001b[0;36mLine2D.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    798\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_foreground(lc_rgba, isRGBA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    800\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_dashes(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dash_pattern)\n\u001b[0;32m--> 801\u001b[0m         \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m         gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markersize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py:132\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
